# process_video.py
import argparse
import os
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq
import re
import json
import warnings
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æŠ‘åˆ¶ PaddleOCR/PaddleX æ¨¡å‹åŠ è½½æ—¥å¿—ï¼ˆå¿…é¡»åœ¨ import å‰è®¾ç½®ï¼‰
os.environ['PADDLEX_DISABLE_PRINT'] = '1'
os.environ['DISABLE_MODEL_SOURCE_CHECK'] = 'True'
warnings.filterwarnings('ignore')
logging.getLogger('ppocr').setLevel(logging.ERROR)
logging.getLogger('paddle').setLevel(logging.ERROR)
logging.getLogger('paddlex').setLevel(logging.ERROR)

from ocr.ocr_utils import init_ocr, ocr_folder_to_text

# å¯¼å…¥å¤šè¿›ç¨‹OCRï¼ˆç”¨äºæå‡CPUåˆ©ç”¨ç‡ï¼‰
try:
    from ocr.ocr_parallel import ocr_folder_parallel
    PARALLEL_OCR_AVAILABLE = True
except ImportError:
    PARALLEL_OCR_AVAILABLE = False
    print("âš ï¸  å¤šè¿›ç¨‹OCRæ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼")

# å¯¼å…¥æ•°æ®åº“æ¨¡å—
from db import VideoRepository
from db.models import Video, Artifact, Topic, TimelineEntry, SourceType, ArtifactType, ProcessingStatus

# å¯é€‰ï¼šæ”¯æŒä» URL ç›´æ¥ä¸‹è½½
try:
    from core.video_downloader import VideoDownloader
    DOWNLOADER_AVAILABLE = True
except ImportError:
    DOWNLOADER_AVAILABLE = False

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ========== è·¯å¾„/ç›®å½•å¤„ç† ==========
def ensure_dir(path: Path):
    path.mkdir(parents=True, exist_ok=True)


# ========== ffmpeg: éŸ³é¢‘ & æŠ½å¸§ ==========

# Groq Whisper API é™åˆ¶
MAX_AUDIO_SIZE_MB = 20
MAX_AUDIO_SIZE_BYTES = MAX_AUDIO_SIZE_MB * 1024 * 1024

def get_video_duration(video_path: Path) -> float:
    """
    ä½¿ç”¨ ffprobe è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ã€‚
    
    Returns:
        float: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å› 0
    """
    try:
        cmd = [
            "ffprobe",
            "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            str(video_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        duration = float(result.stdout.strip())
        return duration
    except (subprocess.CalledProcessError, ValueError) as e:
        print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•è·å–è§†é¢‘æ—¶é•¿: {e}")
        return 0


def extract_audio(video_path: Path, audio_path: Path):
    """
    ç”¨ ffmpeg ä»è§†é¢‘é‡Œåˆ†ç¦»éŸ³é¢‘ï¼Œè¾“å‡ºä¸ºå‹ç¼©çš„ wavã€‚
    ä½¿ç”¨ä»¥ä¸‹å‚æ•°å‹ç¼©éŸ³é¢‘ï¼š
      - ac 1: å•å£°é“
      - ar 16000: é‡‡æ ·ç‡ 16kHz
      - sample_fmt s16: 16-bit PCM
    """
    ensure_dir(audio_path.parent)
    cmd = [
        "ffmpeg",
        "-y",
        "-i", str(video_path),
        "-vn",                    # no video
        "-acodec", "pcm_s16le",   # 16-bit PCM
        "-ar", "16000",           # é‡‡æ ·ç‡ 16kHz
        "-ac", "1",               # å•å£°é“
        str(audio_path),
    ]
    subprocess.run(cmd, check=True, capture_output=True)


def get_audio_duration(audio_path: Path) -> float:
    """
    ä½¿ç”¨ ffprobe è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ã€‚
    """
    try:
        cmd = [
            "ffprobe",
            "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            str(audio_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout.strip())
    except (subprocess.CalledProcessError, ValueError):
        return 0


def split_audio(audio_path: Path, max_size_mb: float = MAX_AUDIO_SIZE_MB) -> list:
    """
    å¦‚æœéŸ³é¢‘æ–‡ä»¶è¶…è¿‡æŒ‡å®šå¤§å°ï¼Œæ‹†åˆ†æˆå¤šä¸ªç‰‡æ®µã€‚
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        max_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
    
    Returns:
        list: [(chunk_path, start_time), ...] æ¯ä¸ªç‰‡æ®µçš„è·¯å¾„å’Œèµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    """
    file_size = audio_path.stat().st_size
    max_size_bytes = max_size_mb * 1024 * 1024
    
    if file_size <= max_size_bytes:
        return [(audio_path, 0.0)]
    
    # è®¡ç®—éœ€è¦æ‹†åˆ†çš„æ®µæ•°
    num_chunks = int(file_size / max_size_bytes) + 1
    duration = get_audio_duration(audio_path)
    
    if duration <= 0:
        print(f"   âš ï¸  æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿ï¼Œå°è¯•ç›´æ¥ä¸Šä¼ ")
        return [(audio_path, 0.0)]
    
    chunk_duration = duration / num_chunks
    
    print(f"   ğŸ“Š éŸ³é¢‘æ–‡ä»¶: {file_size / 1024 / 1024:.1f}MB > {max_size_mb}MB")
    print(f"   âœ‚ï¸  æ‹†åˆ†ä¸º {num_chunks} æ®µ (æ¯æ®µçº¦ {chunk_duration:.0f}ç§’)")
    
    chunks = []
    chunk_dir = audio_path.parent / "audio_chunks"
    ensure_dir(chunk_dir)
    
    for i in range(num_chunks):
        start_time = i * chunk_duration
        chunk_path = chunk_dir / f"chunk_{i:03d}.wav"
        
        cmd = [
            "ffmpeg",
            "-y",
            "-i", str(audio_path),
            "-ss", str(start_time),
            "-t", str(chunk_duration),
            "-acodec", "pcm_s16le",
            "-ar", "16000",
            "-ac", "1",
            str(chunk_path),
        ]
        subprocess.run(cmd, check=True, capture_output=True)
        chunks.append((chunk_path, start_time))
        print(f"   âœ… ç‰‡æ®µ {i+1}/{num_chunks}: {chunk_path.name}")
    
    return chunks


def extract_frames(video_path: Path, frames_dir: Path, fps: int = 1):
    """
    ç”¨ ffmpeg æŠ½å¸§ï¼šé»˜è®¤ 1 fpsï¼ˆæ¯ç§’ä¸€å¸§ï¼‰ã€‚
    å¸§ç¼–å·ä» 1 å¼€å§‹ï¼Œframe_00001.png å¯¹åº”ç¬¬ 0-1 ç§’ã€‚
    """
    ensure_dir(frames_dir)
    out_pattern = frames_dir / "frame_%05d.png"
    cmd = [
        "ffmpeg",
        "-y",
        "-loglevel", "error",  # åªæ˜¾ç¤ºé”™è¯¯
        "-i", str(video_path),
        "-vf", f"fps={fps}",
        str(out_pattern),
    ]
    subprocess.run(cmd, check=True)


def match_audio_with_frames(transcript_data: dict, frames_dir: Path, fps: int = 1) -> list:
    """
    éŸ³ç”»åŒ¹é…ï¼šå°†éŸ³é¢‘è½¬å†™ç‰‡æ®µä¸è§†é¢‘å¸§å…³è”ã€‚
    
    Args:
        transcript_data: åŒ…å« segments çš„è½¬å†™æ•°æ®
        frames_dir: è§†é¢‘å¸§ç›®å½•
        fps: æŠ½å¸§é¢‘ç‡ï¼ˆæ¯ç§’å¸§æ•°ï¼‰
    
    Returns:
        list: [{'second': 0, 'frame': 'frame_00001.png', 'text': 'å¯¹åº”çš„æ–‡æœ¬'}, ...]
    """
    import glob
    
    # è·å–æ‰€æœ‰å¸§æ–‡ä»¶
    frame_files = sorted(glob.glob(str(frames_dir / "frame_*.png")))
    frame_count = len(frame_files)
    
    # ä¸ºæ¯ä¸€ç§’å»ºç«‹æ–‡æœ¬ç´¢å¼•
    timeline = []
    
    for i in range(frame_count):
        second = i  # å¸§ç¼–å·ä»1å¼€å§‹ï¼Œå¯¹åº”ç¬¬ i ç§’
        frame_name = f"frame_{i+1:05d}.png"
        
        # æŸ¥æ‰¾è¿™ä¸€ç§’å¯¹åº”çš„æ–‡æœ¬
        texts_in_second = []
        if 'segments' in transcript_data:
            for seg in transcript_data['segments']:
                seg_start = int(seg['start'])
                seg_end = int(seg['end'])
                # å¦‚æœç‰‡æ®µè¦†ç›–å½“å‰ç§’
                if seg_start <= second < seg_end:
                    texts_in_second.append(seg['text'].strip())
        
        timeline.append({
            'second': second,
            'frame': frame_name,
            'text': ' '.join(texts_in_second) if texts_in_second else ''
        })
    
    return timeline


# ========== Groq API é›†æˆ ==========
def _transcribe_single_audio(client, model: str, audio_path: Path) -> dict:
    """
    è½¬å†™å•ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰ã€‚
    """
    with open(audio_path, "rb") as audio_file:
        transcription = client.audio.transcriptions.create(
            file=(audio_path.name, audio_file.read()),
            model=model,
            response_format="verbose_json",
            timestamp_granularities=["segment"]
        )
    
    result = {
        'text': transcription.text,
        'segments': []
    }
    
    if hasattr(transcription, 'segments') and transcription.segments:
        for seg in transcription.segments:
            result['segments'].append({
                'start': seg.get('start', 0),
                'end': seg.get('end', 0),
                'text': seg.get('text', '')
            })
    
    return result


def transcribe_audio_with_groq(audio_path: Path) -> dict:
    """
    ä½¿ç”¨ Groq çš„ Whisper æ¨¡å‹è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—ï¼Œè¿”å›å¸¦æ—¶é—´æˆ³çš„æ•°æ®ã€‚
    å¦‚æœéŸ³é¢‘æ–‡ä»¶è¶…è¿‡ 20MBï¼Œè‡ªåŠ¨æ‹†åˆ†æˆå¤šæ®µåˆ†åˆ«è¯†åˆ«ï¼Œç„¶åæ‹¼æ¥ç»“æœã€‚
    
    Returns:
        dict: {
            'text': 'å®Œæ•´æ–‡æœ¬',
            'segments': [{'start': 0.0, 'end': 2.5, 'text': 'ç‰‡æ®µæ–‡æœ¬'}, ...]
        }
    """
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        print("  âš ï¸  GROQ_API_KEY æœªè®¾ç½®ï¼Œä½¿ç”¨å ä½ç¬¦")
        return {
            'text': f"[FAKE TRANSCRIPT for {audio_path.name}] è¯·åœ¨ .env ä¸­è®¾ç½® GROQ_API_KEY",
            'segments': []
        }
    
    try:
        client = Groq(api_key=api_key)
        model = os.getenv("GROQ_ASR_MODEL", "whisper-large-v3-turbo")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå†³å®šæ˜¯å¦éœ€è¦æ‹†åˆ†
        file_size = audio_path.stat().st_size
        
        if file_size <= MAX_AUDIO_SIZE_BYTES:
            # æ–‡ä»¶è¶³å¤Ÿå°ï¼Œç›´æ¥è½¬å†™
            return _transcribe_single_audio(client, model, audio_path)
        
        # æ–‡ä»¶è¿‡å¤§ï¼Œéœ€è¦æ‹†åˆ†
        chunks = split_audio(audio_path)
        
        if len(chunks) == 1:
            # æ‹†åˆ†å¤±è´¥æˆ–ä¸éœ€è¦æ‹†åˆ†ï¼Œå°è¯•ç›´æ¥ä¸Šä¼ 
            return _transcribe_single_audio(client, model, audio_path)
        
        # åˆ†æ®µè½¬å†™å¹¶åˆå¹¶ç»“æœ
        all_text = []
        all_segments = []
        
        for i, (chunk_path, time_offset) in enumerate(chunks):
            print(f"   ğŸ¤ è½¬å†™ç‰‡æ®µ {i+1}/{len(chunks)}...")
            try:
                chunk_result = _transcribe_single_audio(client, model, chunk_path)
                
                # æ·»åŠ æ–‡æœ¬
                if chunk_result.get('text'):
                    all_text.append(chunk_result['text'])
                
                # æ·»åŠ ç‰‡æ®µï¼ˆè°ƒæ•´æ—¶é—´åç§»ï¼‰
                for seg in chunk_result.get('segments', []):
                    all_segments.append({
                        'start': seg['start'] + time_offset,
                        'end': seg['end'] + time_offset,
                        'text': seg['text']
                    })
                    
            except Exception as chunk_err:
                print(f"   âš ï¸  ç‰‡æ®µ {i+1} è½¬å†™å¤±è´¥: {chunk_err}")
                all_text.append(f"[ç‰‡æ®µ{i+1}è½¬å†™å¤±è´¥]")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        chunk_dir = audio_path.parent / "audio_chunks"
        if chunk_dir.exists():
            import shutil
            shutil.rmtree(chunk_dir)
        
        print(f"   âœ… åˆå¹¶ {len(chunks)} ä¸ªç‰‡æ®µçš„è½¬å†™ç»“æœ")
        
        return {
            'text': ' '.join(all_text),
            'segments': all_segments
        }
        
    except Exception as e:
        print(f"  âœ— Groq è½¬å†™å¤±è´¥: {e}")
        return {
            'text': f"[è½¬å†™å¤±è´¥: {str(e)}]",
            'segments': []
        }


def summarize_with_gpt_oss_120b(full_text: str) -> str:
    """
    ä½¿ç”¨ Groq çš„ LLM è¿›è¡Œæ–‡æœ¬æ€»ç»“ã€‚
    """
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        print("  âš ï¸  GROQ_API_KEY æœªè®¾ç½®ï¼Œè¿”å›åŸæ–‡")
        return f"[FAKE SUMMARY - è¯·åœ¨ .env ä¸­è®¾ç½® GROQ_API_KEY]\n\n{full_text}"
    
    try:
        client = Groq(api_key=api_key)
        model = os.getenv("GROQ_LLM_MODEL", "openai/gpt-oss-120b")
        # å¢åŠ  token é™åˆ¶ä»¥æ”¯æŒæ›´é•¿çš„è¾“å‡º
        max_tokens = int(os.getenv("GROQ_MAX_TOKENS", "8192"))  # ä» 4096 æå‡åˆ° 8192
        temperature = float(os.getenv("GROQ_TEMPERATURE", "0.7"))
        
        prompt = f"""

è¯·å°†ä»¥ä¸‹â€œå¸¦æ—¶é—´æˆ³çš„éŸ³é¢‘è½¬å†™ + OCR æ–‡æœ¬â€æ•´ç†æˆä¸€ä»½**ç»“æ„åŒ– Markdown çŸ¥è¯†æ¡£æ¡ˆå’Œå†…å®¹æ¦‚è¦**ã€‚

ä½ éœ€è¦ï¼š
1. **ä½¿ç”¨ Markdown** è¾“å‡ºï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€å¼•ç”¨ã€è¡¨æ ¼ç­‰ï¼‰
2. æŒ‰æ—¶é—´é¡ºåºæ¢³ç†ä¸»è¦ç‰‡æ®µï¼Œå¹¶ä¸ºå…³é”®å†…å®¹æ ‡æ³¨å¯¹åº”æ—¶é—´æˆ³
3. åˆå¹¶éŸ³é¢‘ä¸ OCR å†…å®¹ï¼š  
   - å¦‚æœ OCR æ–‡å­—ä¸å®Œæ•´ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡**æ¨æ–­åˆç†å«ä¹‰**  
   - å¦‚æœæŸäº›å±å¹•æ–‡å­—é‡è¦ï¼ˆå¦‚ PPTã€ç•Œé¢æŒ‰é’®ã€å‚æ•°ã€ä»£ç ï¼‰ï¼Œè¯·å•ç‹¬æå–å¹¶è§£é‡Š
4. è‡ªåŠ¨è¯†åˆ«â€œä¸»é¢˜/ç« èŠ‚â€å¹¶ç»“æ„åŒ–æ€»ç»“ï¼šæ¦‚å¿µã€æ­¥éª¤ã€åœºæ™¯ã€ç»“è®º
5. æå–é‡è¦æ•°æ®ï¼šæ•°å­—ã€é˜ˆå€¼ã€è§„åˆ™ã€å¼•ç”¨ã€å‘½ä»¤ã€æ—¥æœŸç­‰
6. ç”Ÿæˆæ ‡ç­¾å’Œæ‘˜è¦ï¼š
   - **æ ‡ç­¾ï¼ˆtagsï¼‰**ï¼š3-6ä¸ªé«˜åº¦æ¦‚æ‹¬çš„ä¸»é¢˜æ ‡ç­¾ï¼Œå¦‚"æƒ…æ„Ÿ"ã€"å‘Šç™½"ã€"äººç”Ÿæ„ä¹‰"ã€"ç§‘æŠ€"ã€"æ•™è‚²"ç­‰ã€‚é¿å…ä½¿ç”¨"è¯­éŸ³è½¬å†™"ã€"OCRæ¨æ–­"ç­‰æŠ€æœ¯æ€§æè¿°è¯ã€‚æ ‡ç­¾åº”ç®€çŸ­ï¼ˆ1-4ä¸ªå­—ï¼‰ï¼Œæ¦‚æ‹¬æ€§å¼ºï¼Œä¾¿äºæ•°æ®åº“æœç´¢ã€‚
   - **æ‘˜è¦**ï¼šä¸è¶…è¿‡50ä¸ªå­—çš„ç³»ç»Ÿæ€§å†…å®¹æ¦‚æ‹¬ï¼Œæç‚¼æ ¸å¿ƒä¸»é¢˜å’Œè¦ç‚¹ã€‚
7. ç¨å¾®è¯¦ç»†ä¸€äº›ï¼Œä½†ä¸è¦å†™åºŸè¯ï¼ˆé‡ç‚¹æ˜¯**å¯å›æº¯ã€å¯æœç´¢ã€å¯ç†è§£**ï¼‰

æ¨èç»“æ„ï¼š
## æ‘˜è¦
ï¼ˆä¸è¶…è¿‡50å­—çš„æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬ï¼‰

## ä¸»è¦å†…å®¹æ¦‚æ‹¬
## ä¸»é¢˜æ€»ç»“ï¼ˆè‡ªåŠ¨ç”Ÿæˆä¸»é¢˜åï¼‰
## è¯¦ç»†è¯´æ˜ï¼ˆåˆå¹¶éŸ³é¢‘ä¸ OCRï¼‰
## å…³é”®ä¿¡æ¯ï¼ˆæ•°å­—ã€è§„åˆ™ã€å‚æ•°ï¼‰
## OCR ä¿¡æ¯ä¸æ¨æ–­ï¼ˆåˆ—å‡ºé‡è¦å±å¹•æ–‡å­—å¹¶è§£é‡Šï¼‰
## æ—¶é—´çº¿ï¼ˆå…³é”®ç‰‡æ®µ + æ—¶é—´æˆ³ï¼‰
## å…³é”®å¥ï¼ˆå«æ—¶é—´æˆ³ï¼‰
## æ ‡ç­¾
æ ¼å¼ï¼šæ ‡ç­¾: æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3

ä»¥ä¸‹æ˜¯å†…å®¹ï¼š
{full_text[:40000]}  



"""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çŸ¥è¯†æ¡£æ¡ˆç”Ÿæˆå™¨ã€‚

                    è¾“å…¥æ¥è‡ªåŒä¸€æ®µè§†é¢‘ï¼ŒåŒ…æ‹¬ï¼š
                    - å¸¦æ—¶é—´æˆ³çš„éŸ³é¢‘è½¬å†™
                    - å¸¦æ—¶é—´æˆ³æˆ–å¸§åºåˆ—çš„ OCR æ–‡æœ¬

                    ä½ çš„èŒè´£æ˜¯ï¼š
                    - èåˆéŸ³é¢‘ä¸ OCR å†…å®¹
                    - åˆ©ç”¨æ—¶é—´æˆ³é‡å»ºç»“æ„ä¸é¡ºåº
                    - æ ¹æ®å†…å®¹è‡ªåŠ¨è¯†åˆ«ä¸»é¢˜ä¸é‡ç‚¹
                    - æ¨æ–­çº æ­£ OCR å¯èƒ½çš„é”™è¯¯
                    - ç”Ÿæˆæ¸…æ™°ã€å¯é•¿æœŸä¿å­˜ã€é€‚åˆæ£€ç´¢çš„ Markdown çŸ¥è¯†æ¡£æ¡ˆ"""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"  âœ— Groq æ€»ç»“å¤±è´¥: {e}")
        return f"[æ€»ç»“å¤±è´¥: {str(e)}]\n\nåŸå§‹å†…å®¹:\n{full_text}"


def generate_detailed_content(full_text: str) -> str:
    """
    ç”Ÿæˆè¯¦ç»†çš„å†…å®¹æ¦‚æ‹¬ï¼ŒåŒ…å«æ›´å¤šç»†èŠ‚ã€‚
    ä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶ï¼ˆ12000ï¼‰ä»¥äº§å‡ºæ›´å®Œæ•´çš„å†…å®¹ã€‚
    """
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        print("  âš ï¸  GROQ_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡è¯¦ç»†å†…å®¹ç”Ÿæˆ")
        return ""
    
    try:
        client = Groq(api_key=api_key)
        model = os.getenv("GROQ_LLM_MODEL", "openai/gpt-oss-120b")
        # è¯¦ç»†å†…å®¹ä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶
        max_tokens = int(os.getenv("GROQ_DETAIL_MAX_TOKENS", "12000"))
        temperature = float(os.getenv("GROQ_TEMPERATURE", "0.7"))
        
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹è§†é¢‘çš„éŸ³é¢‘è½¬å†™å’ŒOCRæ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä»½**è¯¦ç»†çš„å†…å®¹æ¦‚æ‹¬**ã€‚

è¦æ±‚ï¼š
1. **é€æ®µè¯¦ç»†å±•å¼€**ï¼šæŒ‰è§†é¢‘çš„æ—¶é—´é¡ºåºï¼Œè¯¦ç»†æè¿°æ¯ä¸ªä¸»è¦éƒ¨åˆ†çš„å†…å®¹
2. **ä¿ç•™å…³é”®ç»†èŠ‚**ï¼š
   - å…·ä½“çš„æ•°å­—ã€æ•°æ®ã€å‚æ•°
   - äººåã€åœ°åã€ä¸“ä¸šæœ¯è¯­
   - å…·ä½“çš„æ“ä½œæ­¥éª¤ã€æµç¨‹
   - å¼•ç”¨çš„åŸè¯ã€å…³é”®å¥å­
   - ä»£ç ç‰‡æ®µã€å‘½ä»¤ã€å…¬å¼
3. **æ—¶é—´æˆ³æ ‡æ³¨**ï¼šä¸ºé‡è¦å†…å®¹æ ‡æ³¨å¯¹åº”çš„æ—¶é—´ç‚¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
4. **å®Œæ•´æ€§ä¼˜å…ˆ**ï¼šå®å¯å†…å®¹å¤šä¸€äº›ï¼Œä¹Ÿä¸è¦é—æ¼é‡è¦ä¿¡æ¯
5. **ç»“æ„æ¸…æ™°**ï¼šä½¿ç”¨å±‚çº§æ ‡é¢˜å’Œåˆ—è¡¨ç»„ç»‡å†…å®¹

è¾“å‡ºæ ¼å¼ï¼š
## è¯¦ç»†å†…å®¹æ¦‚æ‹¬

### ç¬¬ä¸€éƒ¨åˆ†ï¼š[ä¸»é¢˜åç§°]
ï¼ˆè¯¦ç»†æè¿°è¿™éƒ¨åˆ†çš„å†…å®¹...ï¼‰

### ç¬¬äºŒéƒ¨åˆ†ï¼š[ä¸»é¢˜åç§°]
ï¼ˆè¯¦ç»†æè¿°è¿™éƒ¨åˆ†çš„å†…å®¹...ï¼‰

### å…³é”®ä¿¡æ¯æ±‡æ€»
- é‡è¦æ•°æ®ï¼š...
- å…³é”®æœ¯è¯­ï¼š...
- æ“ä½œæ­¥éª¤ï¼š...

### åŸæ–‡å…³é”®å¥æ‘˜å½•
> "åŸå¥1..." â€”â€” [æ—¶é—´æˆ³]
> "åŸå¥2..." â€”â€” [æ—¶é—´æˆ³]

ä»¥ä¸‹æ˜¯åŸå§‹å†…å®¹ï¼š
{full_text[:50000]}
"""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ•´ç†åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
                    - ä»è§†é¢‘è½¬å†™å’ŒOCRæ–‡æœ¬ä¸­æå–æ‰€æœ‰é‡è¦ä¿¡æ¯
                    - ç”Ÿæˆè¯¦å°½ã€å®Œæ•´çš„å†…å®¹æ¦‚æ‹¬
                    - ä¿ç•™åŸå§‹å†…å®¹ä¸­çš„å…³é”®ç»†èŠ‚å’Œæ•°æ®
                    - ä½¿ç”¨æ¸…æ™°çš„ç»“æ„ç»„ç»‡ä¿¡æ¯
                    - ç¡®ä¿å†…å®¹å¯ä»¥ä½œä¸ºè§†é¢‘å†…å®¹çš„å®Œæ•´å‚è€ƒ"""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"  âš ï¸  è¯¦ç»†å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
        return ""


def merge_summary_with_details(summary: str, detailed_content: str) -> str:
    """
    å°†è¯¦ç»†å†…å®¹æ¦‚æ‹¬è¿½åŠ åˆ°æŠ¥å‘Šæœ«å°¾ã€‚
    ä¿æŒåŸæœ‰æŠ¥å‘Šå†…å®¹ä¸å˜ã€‚
    """
    if not detailed_content:
        return summary
    
    # ç›´æ¥è¿½åŠ åˆ°æœ«å°¾
    return summary + f"\n\n---\n\n## ğŸ“– è¯¦ç»†å†…å®¹æ¦‚æ‹¬ï¼ˆå®Œæ•´ç‰ˆï¼‰\n\n{detailed_content}\n"


def generate_timeline_report(timeline: list, output_path: Path):
    """
    ç”ŸæˆéŸ³ç”»æ—¶é—´è½´å¯¹ç…§æŠ¥å‘Š
    
    Args:
        timeline: éŸ³ç”»åŒ¹é…çš„æ—¶é—´è½´æ•°æ®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    report = []
    report.append("# ğŸ¬ éŸ³ç”»æ—¶é—´è½´å¯¹ç…§\n")
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  \n")
    report.append(f"**æ€»æ—¶é•¿**: {len(timeline)} ç§’  \n")
    report.append("\n---\n")
    
    report.append("## ğŸ“Š é€ç§’å¯¹ç…§è¡¨\n")
    
    for item in timeline:
        second = item['second']
        frame = item['frame']
        text = item['text']
        
        # æ ¼å¼åŒ–æ—¶é—´
        minutes = second // 60
        seconds = second % 60
        time_str = f"{minutes:02d}:{seconds:02d}"
        
        report.append(f"### [{time_str}] ç¬¬ {second} ç§’\n")
        report.append(f"**ç”»é¢**: `{frame}`  \n")
        if text:
            report.append(f"**éŸ³é¢‘**: {text}\n")
        else:
            report.append(f"**éŸ³é¢‘**: *(æ— è¯­éŸ³)*\n")
        report.append("\n")
    
    output_path.write_text('\n'.join(report), encoding='utf-8')


def generate_formatted_report(
    video_name: str,
    timestamp: str,
    transcript_text: str,
    ocr_text: str,
    summary: str,
    with_frames: bool,
    session_dir: Path,
    timeline: list = None
) -> str:
    """
    ç”Ÿæˆæ ¼å¼åŒ–çš„æŠ¥å‘Šï¼ŒåŒ…å«å…ƒä¿¡æ¯ã€AIæ€»ç»“å’ŒåŸå§‹æ•°æ®
    """
    # ç»Ÿè®¡ä¿¡æ¯
    transcript_chars = len(transcript_text)
    transcript_lines = transcript_text.count('\n')
    ocr_chars = len(ocr_text) if ocr_text else 0
    ocr_lines = ocr_text.count('\n') if ocr_text else 0
    
    # æ ¼å¼åŒ–æ—¶é—´
    dt = datetime.strptime(timestamp, "%Y%m%d_%H%M%S")
    formatted_time = dt.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
    
    # ä½¿ç”¨ Markdown æ ¼å¼
    report = []
    report.append("# ğŸ“¹ è§†é¢‘åˆ†ææŠ¥å‘Š\n")
    report.append(f"**ğŸ“ è§†é¢‘åç§°**: {video_name}  ")
    report.append(f"**ğŸ•’ å¤„ç†æ—¶é—´**: {formatted_time}  ")
    report.append(f"**ğŸ“ è¾“å‡ºç›®å½•**: `{session_dir.name}`  ")
    report.append(f"**ğŸ”§ å¤„ç†æ¨¡å¼**: {'å®Œæ•´æ¨¡å¼ (OCR + éŸ³é¢‘)' if with_frames else 'éŸ³é¢‘æ¨¡å¼'}  ")
    report.append("\n---\n")
    report.append("## ğŸ“Š æ•°æ®ç»Ÿè®¡\n")
    report.append(f"- **è¯­éŸ³è¯†åˆ«**: {transcript_chars} å­—ç¬¦, {transcript_lines} è¡Œ")
    if with_frames:
        report.append(f"- **OCRè¯†åˆ«**: {ocr_chars} å­—ç¬¦, {ocr_lines} è¡Œ")
    report.append("\n---\n")
    
    # AI æ€»ç»“ï¼ˆå·²ç»æ˜¯ markdown æ ¼å¼ï¼‰
    report.append("## ğŸ¤– AI æ™ºèƒ½æ€»ç»“\n")
    report.append(summary)
    report.append("\n---\n")
    
    # åŸå§‹æ•°æ®å¼•ç”¨
    report.append("## ğŸ“‚ åŸå§‹æ•°æ®æ–‡ä»¶\n")
    report.append(f"- ğŸ“„ [è¯­éŸ³è¯†åˆ«åŸæ–‡](transcript_raw.md) ({transcript_chars} å­—ç¬¦)")
    if with_frames:
        report.append(f"- ğŸ“„ [OCRè¯†åˆ«åŸæ–‡](ocr_raw.md) ({ocr_chars} å­—ç¬¦)")
        report.append(f"- ğŸ“ è§†é¢‘å¸§å›¾ç‰‡: `frames/` ç›®å½•")
        if timeline:
            report.append(f"- ğŸ¬ [éŸ³ç”»æ—¶é—´è½´å¯¹ç…§](timeline.md) (é€ç§’åŒ¹é…)")
    report.append(f"- ğŸ”Š éŸ³é¢‘æ–‡ä»¶: `{video_name}.wav`")
    report.append("\n> ğŸ’¡ **æç¤º**: ç‚¹å‡»é“¾æ¥æŸ¥çœ‹åŸå§‹æ•°æ®æ–‡ä»¶è·å–å®Œæ•´çš„è¯†åˆ«å†…å®¹\n")
    report.append("---\n")
    report.append(f"*ğŸ“Œ æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {formatted_time}*")
    
    return "\n".join(report)


def extract_summary_from_report(summary: str) -> str:
    """ä»AIæŠ¥å‘Šä¸­æå–æ‘˜è¦ï¼ˆä¸è¶…è¿‡50å­—ï¼‰"""
    # æŸ¥æ‰¾æ‘˜è¦éƒ¨åˆ†
    summary_patterns = [
        r'##\s*æ‘˜è¦\s*\n+(.+?)(?:\n\n|\n##)',  # ## æ‘˜è¦ åçš„å†…å®¹
        r'æ‘˜è¦[ï¼š:]\s*(.+?)(?:\n\n|\n##)',     # æ‘˜è¦: åçš„å†…å®¹
    ]
    
    for pattern in summary_patterns:
        matches = re.findall(pattern, summary, re.DOTALL | re.MULTILINE)
        if matches:
            extracted = matches[0].strip()
            # ç§»é™¤Markdownæ ¼å¼
            extracted = re.sub(r'\*\*|\*|`|#|\[|\]|\(.*?\)', '', extracted)
            # é™åˆ¶é•¿åº¦ä¸º50å­—
            if len(extracted) > 50:
                extracted = extracted[:50]
            return extracted
    
    # å¦‚æœæ²¡æ‰¾åˆ°æ‘˜è¦ç« èŠ‚ï¼Œå°è¯•æå–ç¬¬ä¸€æ®µéæ ‡é¢˜å†…å®¹
    lines = summary.split('\n')
    for line in lines:
        line = line.strip()
        if line and not line.startswith('#') and not line.startswith('*') and len(line) > 10:
            # ç§»é™¤Markdownæ ¼å¼
            line = re.sub(r'\*\*|\*|`|#|\[|\]|\(.*?\)', '', line)
            if len(line) > 50:
                return line[:50]
            return line
    
    return "æš‚æ— æ‘˜è¦"


def extract_tags_from_summary(summary: str) -> list:
    """ä»AIæ€»ç»“ä¸­æå–æ ‡ç­¾"""
    tags = []
    
    # æŸ¥æ‰¾æ ‡ç­¾è¡Œï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    tag_patterns = [
        r'##\s*æ ‡ç­¾\s*\n+(.+?)(?:\n\n|\n##)',  # ## æ ‡ç­¾ åçš„å†…å®¹
        r'æ ‡ç­¾[ï¼š:]\s*(.+)',
        r'Tags[ï¼š:]\s*(.+)',
        r'å…³é”®è¯[ï¼š:]\s*(.+)',
        r'Keywords[ï¼š:]\s*(.+)',
    ]
    
    for pattern in tag_patterns:
        matches = re.findall(pattern, summary, re.IGNORECASE | re.MULTILINE | re.DOTALL)
        for match in matches:
            # ç§»é™¤Markdownæ ¼å¼ï¼ˆç²—ä½“ã€æ–œä½“ç­‰ï¼‰
            clean_match = re.sub(r'\*\*|\*|`|#', '', match)
            # ç§»é™¤å¼•å·
            clean_match = re.sub(r'["""\'\'"]', '', clean_match)
            # ç§»é™¤æ¢è¡Œ
            clean_match = clean_match.replace('\n', ' ')
            # åˆ†å‰²æ ‡ç­¾ï¼ˆæ”¯æŒé€—å·ã€é¡¿å·ã€ç©ºæ ¼ã€åˆ†å·ç­‰åˆ†éš”ç¬¦ï¼‰
            tag_list = re.split(r'[,ï¼Œã€\s;ï¼›]+', clean_match.strip())
            tags.extend([t.strip() for t in tag_list if t.strip()])
    
    # å»é‡å¹¶è¿‡æ»¤
    seen = set()
    unique_tags = []
    for tag in tags:
        # æ¸…ç†æ¯ä¸ªæ ‡ç­¾
        tag = re.sub(r'[^\w\u4e00-\u9fa5\-]', '', tag)  # åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡ã€è¿å­—ç¬¦
        tag_lower = tag.lower()
        if tag_lower not in seen and len(tag) > 1 and len(tag) < 20:
            seen.add(tag_lower)
            unique_tags.append(tag)
    
    return unique_tags[:10]  # æœ€å¤šè¿”å›10ä¸ªæ ‡ç­¾


def extract_topics_from_summary(summary: str, video_duration: float = 0) -> list:
    """ä»AIæ€»ç»“ä¸­æå–ä¸»é¢˜ç« èŠ‚"""
    topics = []
    
    # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜ï¼ˆ## å¼€å¤´ï¼‰
    lines = summary.split('\n')
    current_topic = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
        if line.startswith('##') and not line.startswith('###'):
            title = line.lstrip('#').strip()
            
            # è¿‡æ»¤æ‰ä¸€äº›éç« èŠ‚çš„æ ‡é¢˜
            skip_titles = ['AI æ™ºèƒ½æ€»ç»“', 'æ•°æ®ç»Ÿè®¡', 'åŸå§‹æ•°æ®', 'æ€»ç»“', 'æ ‡ç­¾', 'Tags', 'å…³é”®è¯']
            if any(skip in title for skip in skip_titles):
                continue
            
            # æå–æ—¶é—´èŒƒå›´ï¼ˆå¦‚æœæœ‰ï¼‰
            time_match = re.search(r'\[?(\d{1,2}):(\d{2})\s*-\s*(\d{1,2}):(\d{2})\]?', line)
            
            if time_match:
                start_min, start_sec, end_min, end_sec = map(int, time_match.groups())
                start_time = start_min * 60 + start_sec
                end_time = end_min * 60 + end_sec
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®æ—¶é—´ï¼ŒæŒ‰é¡ºåºåˆ†é…
                start_time = (len(topics) * video_duration / 5) if video_duration > 0 else 0
                end_time = min(start_time + video_duration / 5, video_duration) if video_duration > 0 else 0
            
            # æ”¶é›†æè¿°ï¼ˆä¸‹é¢å‡ è¡Œéæ ‡é¢˜å†…å®¹ï¼‰
            description_lines = []
            for j in range(i + 1, min(i + 5, len(lines))):
                desc_line = lines[j].strip()
                if desc_line and not desc_line.startswith('#'):
                    description_lines.append(desc_line)
                elif desc_line.startswith('##'):
                    break
            
            description = ' '.join(description_lines)[:200]
            
            topics.append({
                'title': title[:100],
                'start_time': start_time,
                'end_time': end_time,
                'description': description,
                'keywords': []  # å¯ä»¥åç»­ä»æè¿°ä¸­æå–
            })
    
    return topics[:20]  # æœ€å¤šè¿”å›20ä¸ªä¸»é¢˜


def save_to_database(
    video_path: Path,
    video_name: str,
    session_dir: Path,
    transcript_text: str,
    ocr_text: str,
    summary: str,
    transcript_data: dict,
    timeline: list = None,
    with_frames: bool = False,
    video_duration: float = 0,
    source_url: str = None,
    platform_title: str = None,
) -> int:
    """
    å°†å¤„ç†ç»“æœä¿å­˜åˆ°æ•°æ®åº“
    
    Returns:
        int: è§†é¢‘ID
    """
    try:
        repo = VideoRepository()
        
        # 1. åˆ›å»ºè§†é¢‘è®°å½•
        print("\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        content_hash = repo.calculate_content_hash(str(video_path))
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = repo.get_video_by_hash(content_hash)
        if existing:
            print(f"   âš ï¸  è§†é¢‘å·²å­˜åœ¨ (ID: {existing.id})ï¼Œæ›´æ–°äº§ç‰©...")
            video_id = existing.id
            # æ›´æ–°è§†é¢‘å…ƒæ•°æ®ï¼ˆæ—¶é•¿ã€æ ‡é¢˜ç­‰ï¼‰
            repo.update_video_metadata(
                video_id=video_id,
                duration_seconds=video_duration,
                title=platform_title or video_name,
                platform_title=platform_title
            )
        else:
            # åˆ¤æ–­æ¥æºç±»å‹
            if source_url:
                if 'bilibili.com' in source_url:
                    source_type = SourceType.BILIBILI
                elif 'youtube.com' in source_url or 'youtu.be' in source_url:
                    source_type = SourceType.YOUTUBE
                else:
                    source_type = SourceType.URL
            else:
                source_type = SourceType.LOCAL
            
            video = Video(
                content_hash=content_hash,
                video_id=None,
                source_type=source_type,
                source_url=source_url,
                platform_title=platform_title or video_name,
                title=platform_title or video_name,
                duration_seconds=video_duration,
                file_path=str(video_path),
                file_size_bytes=video_path.stat().st_size,
                processing_config={
                    'with_frames': with_frames,
                    'output_dir': str(session_dir)
                },
                status=ProcessingStatus.COMPLETED
            )
            
            video_id = repo.create_video(video)
            print(f"   âœ… åˆ›å»ºè§†é¢‘è®°å½• (ID: {video_id})")
        
        # 2. ä¿å­˜äº§ç‰©
        # 2.1 è¯­éŸ³è½¬å†™
        if transcript_text.strip():
            transcript_artifact = Artifact(
                video_id=video_id,
                artifact_type=ArtifactType.TRANSCRIPT,
                content_text=transcript_text,
                content_json=transcript_data,
                file_path=str(session_dir / "transcript_raw.md"),
                model_name="groq-whisper-large-v3",
                char_count=len(transcript_text)
            )
            repo.save_artifact(transcript_artifact)
            print(f"   âœ… ä¿å­˜è¯­éŸ³è½¬å†™ ({len(transcript_text)} å­—ç¬¦)")
        
        # 2.2 OCRè¯†åˆ«
        if with_frames and ocr_text.strip():
            ocr_artifact = Artifact(
                video_id=video_id,
                artifact_type=ArtifactType.OCR,
                content_text=ocr_text,
                file_path=str(session_dir / "ocr_raw.md"),
                model_name="paddleocr-v4",
                char_count=len(ocr_text)
            )
            repo.save_artifact(ocr_artifact)
            print(f"   âœ… ä¿å­˜OCRè¯†åˆ« ({len(ocr_text)} å­—ç¬¦)")
        
        # 2.3 AIæŠ¥å‘Š
        if summary.strip():
            report_artifact = Artifact(
                video_id=video_id,
                artifact_type=ArtifactType.REPORT,
                content_text=summary,
                file_path=str(session_dir / "report.md"),
                model_name="groq-llama3-120b",
                char_count=len(summary)
            )
            repo.save_artifact(report_artifact)
            print(f"   âœ… ä¿å­˜AIæŠ¥å‘Š ({len(summary)} å­—ç¬¦)")
        
        # 3. æå–å¹¶ä¿å­˜æ ‡ç­¾
        tags = extract_tags_from_summary(summary)
        if tags:
            repo.save_tags(video_id, tags, source='auto', confidence=0.8)
            print(f"   âœ… ä¿å­˜æ ‡ç­¾: {', '.join(tags)}")
        
        # 4. æå–å¹¶ä¿å­˜ä¸»é¢˜
        topics = extract_topics_from_summary(summary, video_duration)
        if topics:
            topic_objects = []
            for t in topics:
                topic = Topic(
                    video_id=video_id,
                    title=t['title'],
                    start_time=t['start_time'],
                    end_time=t['end_time'],
                    summary=t['description'],
                    keywords=t['keywords']
                )
                topic_objects.append(topic)
            
            repo.save_topics(video_id, topic_objects)
            print(f"   âœ… ä¿å­˜ä¸»é¢˜: {len(topics)} ä¸ªç« èŠ‚")
        
        # 5. ä¿å­˜æ—¶é—´çº¿
        if timeline and len(timeline) > 0:
            timeline_entries = []
            for entry in timeline[:100]:  # é™åˆ¶æ•°é‡
                if entry.get('text'):
                    tl = TimelineEntry(
                        video_id=video_id,
                        timestamp_seconds=entry['second'],
                        transcript_text=entry['text'][:500]
                    )
                    timeline_entries.append(tl)
            
            if timeline_entries:
                repo.save_timeline(video_id, timeline_entries)
                print(f"   âœ… ä¿å­˜æ—¶é—´çº¿: {len(timeline_entries)} ä¸ªæ¡ç›®")
        
        # 6. æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•
        print("   ğŸ” æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•...")
        repo.update_fts_index(video_id)
        
        print(f"   âœ… æ•°æ®åº“ä¿å­˜å®Œæˆï¼(è§†é¢‘ID: {video_id})")
        print(f"   ğŸ’¡ å¯ä»¥ä½¿ç”¨ `make db-show ID={video_id}` æŸ¥çœ‹è¯¦æƒ…")
        print(f"   ğŸ’¡ å¯ä»¥ä½¿ç”¨ `make search Q=\"å…³é”®è¯\"` æ¥æœç´¢")
        
        return video_id
        
    except Exception as e:
        print(f"   âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


# ========== ä¸»æ§åˆ¶æµç¨‹ ==========
def process_video(
    video_path: Path,
    output_dir: Path,
    with_frames: bool = False,
    ocr_lang: str = "ch",
    ocr_det_model: str = "mobile",
    ocr_rec_model: str = "mobile",
    use_gpu: bool = False,
    source_url: str = None,
    platform_title: str = None,
):
    ensure_dir(output_dir)

    # 1. åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    video_name = video_path.stem
    session_dir = output_dir / f"{video_name}_{timestamp}"
    ensure_dir(session_dir)
    
    # 2. å„ç±»æ–‡ä»¶è·¯å¾„
    audio_path = session_dir / f"{video_name}.wav"
    frames_dir = session_dir / "frames"
    ocr_raw_path = session_dir / "ocr_raw.md"
    transcript_raw_path = session_dir / "transcript_raw.md"
    report_path = session_dir / "report.md"
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {session_dir}")
    print(f"   æ—¶é—´æˆ³: {timestamp}\n")

    # è·å–è§†é¢‘æ—¶é•¿
    print(">> è·å–è§†é¢‘æ—¶é•¿...")
    video_duration = get_video_duration(video_path)
    print(f"   â±ï¸  è§†é¢‘æ—¶é•¿: {video_duration:.2f} ç§’ ({int(video_duration // 60)}:{int(video_duration % 60):02d})")

    ocr_text = ""
    transcript_text = ""
    
    # 2. å¦‚æœæ˜¯OCRæ¨¡å¼ï¼Œå…ˆå¤„ç†è§†é¢‘å¸§å’ŒOCR
    if with_frames:
        print("\n" + "="*60)
        print("ğŸ“¹ ç¬¬ä¸€æ­¥ï¼šå¤„ç†è§†é¢‘å¸§ OCR")
        print("="*60)
        
        print(">> æŠ½å¸§ä¸­...")
        extract_frames(video_path, frames_dir, fps=1)

        print("\n>> OCR å¤„ç†ä¸­...")
        
        # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ä»¥æå‡CPUåˆ©ç”¨ç‡
        if PARALLEL_OCR_AVAILABLE:
            import os
            # ä»ç¯å¢ƒå˜é‡è¯»å–å·¥ä½œè¿›ç¨‹æ•°ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨CPUæ ¸å¿ƒæ•°/2
            ocr_workers_env = os.environ.get('OCR_WORKERS', '').strip()
            if ocr_workers_env and ocr_workers_env.lower() != 'auto':
                try:
                    num_workers = max(1, int(ocr_workers_env))
                except ValueError:
                    num_workers = max(1, os.cpu_count() // 2)
            else:
                num_workers = max(1, os.cpu_count() // 2)
            
            ocr_text = ocr_folder_parallel(
                str(frames_dir),
                min_score=0.3,
                num_workers=num_workers,
                use_preprocessing=True,
                hybrid_mode=True,
            )
        else:
            # é™çº§åˆ°å•è¿›ç¨‹æ¨¡å¼
            print(f">> åˆå§‹åŒ– OCR (det={ocr_det_model}, rec={ocr_rec_model})...")
            ocr = init_ocr(
                lang=ocr_lang,
                use_gpu=use_gpu,
                det_model=ocr_det_model,
                rec_model=ocr_rec_model
            )
            ocr_text = ocr_folder_to_text(
                ocr, 
                str(frames_dir), 
                min_score=0.3,
                debug=False,
                use_preprocessing=True,
                roi_bottom_only=True,
                hybrid_mode=True,
            )
        
        if ocr_text.strip():
            char_count = len(ocr_text)
            line_count = ocr_text.count('\n')
            print(f"\nâœ… OCR å®Œæˆï¼è¯†åˆ« {char_count} å­—ç¬¦ï¼Œ{line_count} è¡Œ")
            
            # ä¿å­˜OCRåŸå§‹ç»“æœï¼ˆMarkdown æ ¼å¼ï¼‰
            print(f"   ğŸ’¾ ä¿å­˜OCRåŸå§‹ç»“æœ: {ocr_raw_path.name}")
            ocr_markdown = f"# ğŸ” OCR è¯†åˆ«åŸå§‹æ•°æ®\n\n"
            ocr_markdown += f"**è¯†åˆ«æ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  \n"
            ocr_markdown += f"**æ€»å­—ç¬¦æ•°**: {char_count}  \n"
            ocr_markdown += f"**æ€»è¡Œæ•°**: {line_count}  \n"
            ocr_markdown += f"**å¤„ç†æ¨¡å¼**: æ··åˆæ¨¡å¼ï¼ˆå­—å¹•åŒº + å…¨ç”»é¢ï¼‰\n\n"
            ocr_markdown += "---\n\n"
            ocr_markdown += "## ğŸ“ è¯†åˆ«å†…å®¹\n\n"
            ocr_markdown += "```\n"
            ocr_markdown += ocr_text
            ocr_markdown += "\n```\n"
            ocr_raw_path.write_text(ocr_markdown, encoding="utf-8")
        else:
            print("âš ï¸  è­¦å‘Šï¼šOCR æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡å­—ï¼ˆå¯èƒ½è§†é¢‘ä¸­æ²¡æœ‰æ–‡å­—å†…å®¹ï¼‰")
        
        print("\n" + "="*60)
        print("ğŸ¤ ç¬¬äºŒæ­¥ï¼šå¤„ç†éŸ³é¢‘è½¬å†™")
        print("="*60)
    
    # 3. å¤„ç†éŸ³é¢‘ï¼ˆOCRæ¨¡å¼åœ¨OCRä¹‹åï¼Œæ™®é€šæ¨¡å¼ç›´æ¥å¤„ç†ï¼‰
    print(">> æå–éŸ³é¢‘ä¸­...")
    extract_audio(video_path, audio_path)

    # 4. Groq è¯­éŸ³è½¬æ–‡å­—ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    print(">> è°ƒç”¨ Groq è¯­éŸ³è½¬å†™ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰...")
    transcript_data = transcribe_audio_with_groq(audio_path)
    transcript_text = transcript_data.get('text', '')
    
    # ä¿å­˜è¯­éŸ³è¯†åˆ«åŸå§‹ç»“æœï¼ˆMarkdown æ ¼å¼ï¼ŒåŒ…å«æ—¶é—´æˆ³ï¼‰
    if transcript_text.strip():
        print(f"   ğŸ’¾ ä¿å­˜è¯­éŸ³è¯†åˆ«åŸå§‹ç»“æœ: {transcript_raw_path.name}")
        transcript_markdown = f"# ğŸ¤ è¯­éŸ³è¯†åˆ«åŸå§‹æ•°æ®\n\n"
        transcript_markdown += f"**è¯†åˆ«æ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  \n"
        transcript_markdown += f"**æ€»å­—ç¬¦æ•°**: {len(transcript_text)}  \n"
        transcript_markdown += f"**è¯†åˆ«æ¨¡å‹**: Groq Whisper  \n"
        transcript_markdown += f"**ç‰‡æ®µæ•°é‡**: {len(transcript_data.get('segments', []))}  \n\n"
        transcript_markdown += "---\n\n"
        transcript_markdown += "## ğŸ“ å®Œæ•´è½¬å†™\n\n"
        transcript_markdown += transcript_text + "\n\n"
        
        # æ·»åŠ å¸¦æ—¶é—´æˆ³çš„ç‰‡æ®µ
        if transcript_data.get('segments'):
            transcript_markdown += "---\n\n"
            transcript_markdown += "## â±ï¸ æ—¶é—´æˆ³ç‰‡æ®µ\n\n"
            for seg in transcript_data['segments']:
                start_time = f"{int(seg['start']//60):02d}:{int(seg['start']%60):02d}"
                end_time = f"{int(seg['end']//60):02d}:{int(seg['end']%60):02d}"
                transcript_markdown += f"**[{start_time} - {end_time}]** {seg['text']}\n\n"
        
        transcript_raw_path.write_text(transcript_markdown, encoding="utf-8")

    # 4.5 ç”ŸæˆéŸ³ç”»åŒ¹é…æ—¶é—´è½´
    timeline = None
    if with_frames and transcript_data.get('segments'):
        print(">> ç”ŸæˆéŸ³ç”»æ—¶é—´è½´åŒ¹é…...")
        timeline = match_audio_with_frames(transcript_data, frames_dir, fps=1)
        timeline_path = session_dir / "timeline.md"
        generate_timeline_report(timeline, timeline_path)
        print(f"   ğŸ’¾ ä¿å­˜éŸ³ç”»æ—¶é—´è½´: {timeline_path.name}")

    # 5. åˆå¹¶æ–‡æœ¬ï¼šéŸ³é¢‘æ–‡å­— + OCR ç»“æœ
    combined_text_parts = [f"=== Audio Transcript ===\n{transcript_text}\n"]
    if with_frames:
        combined_text_parts.append(f"\n\n=== OCR from Frames ===\n{ocr_text}\n")

    combined_text = "\n".join(combined_text_parts)

    # 6. ç¬¬ä¸€æ¬¡AIè°ƒç”¨ï¼šç”Ÿæˆç»“æ„åŒ–æ‘˜è¦æŠ¥å‘Š
    print("\n>> ç¬¬ä¸€æ¬¡AIè°ƒç”¨ï¼šç”Ÿæˆç»“æ„åŒ–æ‘˜è¦...")
    summary = summarize_with_gpt_oss_120b(combined_text)
    
    # 7. ç¬¬äºŒæ¬¡AIè°ƒç”¨ï¼šç”Ÿæˆè¯¦ç»†å†…å®¹æ¦‚æ‹¬ï¼ˆä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„å®Œæ•´æ–‡æœ¬ï¼‰
    print(">> ç¬¬äºŒæ¬¡AIè°ƒç”¨ï¼šç”Ÿæˆè¯¦ç»†å†…å®¹æ¦‚æ‹¬...")
    # æ„å»ºå¸¦æ—¶é—´æˆ³çš„è½¬å†™æ–‡æœ¬
    timestamped_text_parts = ["=== Audio Transcript with Timestamps ===\n"]
    if transcript_data.get('segments'):
        for seg in transcript_data['segments']:
            start_time = f"{int(seg['start']//60):02d}:{int(seg['start']%60):02d}"
            end_time = f"{int(seg['end']//60):02d}:{int(seg['end']%60):02d}"
            timestamped_text_parts.append(f"[{start_time} - {end_time}] {seg['text']}")
    else:
        timestamped_text_parts.append(transcript_text)
    
    if with_frames:
        timestamped_text_parts.append(f"\n\n=== OCR from Frames ===\n{ocr_text}\n")
    
    timestamped_combined_text = "\n".join(timestamped_text_parts)
    detailed_content = generate_detailed_content(timestamped_combined_text)
    
    # 8. åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†å†…å®¹
    if detailed_content:
        print(">> åˆå¹¶æ‘˜è¦ä¸è¯¦ç»†å†…å®¹...")
        summary = merge_summary_with_details(summary, detailed_content)
        print(f"   âœ… è¯¦ç»†å†…å®¹å·²æ·»åŠ  ({len(detailed_content)} å­—ç¬¦)")

    # 9. ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Š
    report_content = generate_formatted_report(
        video_name=video_name,
        timestamp=timestamp,
        transcript_text=transcript_text,
        ocr_text=ocr_text,
        summary=summary,
        with_frames=with_frames,
        session_dir=session_dir,
        timeline=timeline
    )
    
    report_path.write_text(report_content, encoding="utf-8")
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    print(f"ğŸ“ å®Œæ•´è¾“å‡ºç›®å½•: {session_dir}")
    
    # 10. ä¿å­˜åˆ°æ•°æ®åº“
    save_to_database(
        video_path=video_path,
        video_name=video_name,
        session_dir=session_dir,
        transcript_text=transcript_text,
        ocr_text=ocr_text,
        summary=summary,
        transcript_data=transcript_data,
        timeline=timeline,
        with_frames=with_frames,
        video_duration=video_duration,
        source_url=source_url,
        platform_title=platform_title,
    )



# ========== CLI ==========
def main():
    parser = argparse.ArgumentParser(
        description="Video â†’ Text Report pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # å¤„ç†æœ¬åœ°è§†é¢‘æ–‡ä»¶
  python process_video.py video.mp4
  python process_video.py video.mp4 --with-frames
  
  # ä»URLä¸‹è½½å¹¶å¤„ç†ï¼ˆå¦‚æœå®‰è£…äº† video_downloaderï¼‰
  python process_video.py "https://www.youtube.com/watch?v=xxxxx"
  python process_video.py "https://www.bilibili.com/video/BVxxxxx" --with-frames
        """
    )
    parser.add_argument("video", type=str, help="è¾“å…¥è§†é¢‘è·¯å¾„æˆ–URL")
    parser.add_argument(
        "--with-frames",
        action="store_true",
        help="æ˜¯å¦å¯ç”¨æŠ½å¸§ + OCR åˆ†æ”¯",
    )
    parser.add_argument(
        "--out-dir",
        type=str,
        default="output",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ./outputï¼‰",
    )
    parser.add_argument(
        "--ocr-lang",
        type=str,
        default="ch",
        help="PaddleOCR è¯­è¨€ï¼ˆé»˜è®¤: chï¼‰",
    )
    parser.add_argument(
        "--ocr-det-model",
        type=str,
        default="server",  # æ”¹ä¸º server ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ
        choices=["server", "mobile"],
        help="OCR æ£€æµ‹æ¨¡å‹ç±»å‹ï¼ˆé»˜è®¤: serverï¼Œå¤æ‚èƒŒæ™¯å»ºè®®ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--ocr-rec-model",
        type=str,
        default="server",  # æ”¹ä¸º server ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ
        choices=["server", "mobile"],
        help="OCR è¯†åˆ«æ¨¡å‹ç±»å‹ï¼ˆé»˜è®¤: serverï¼Œæå‡å‡†ç¡®åº¦ï¼‰",
    )
    parser.add_argument(
        "--use-gpu",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨ GPU åŠ é€Ÿ",
    )
    parser.add_argument(
        "--download-dir",
        type=str,
        default="videos",
        help="è§†é¢‘ä¸‹è½½ç›®å½•ï¼ˆé»˜è®¤: videos/ï¼‰",
    )

    args = parser.parse_args()

    # æ£€æµ‹è¾“å…¥æ˜¯URLè¿˜æ˜¯æ–‡ä»¶è·¯å¾„
    input_str = args.video
    is_url = input_str.startswith("http://") or input_str.startswith("https://")
    
    source_url = None
    platform_title = None
    
    if is_url:
        # å¦‚æœæ˜¯URLï¼Œå°è¯•ä¸‹è½½
        if not DOWNLOADER_AVAILABLE:
            print("âŒ é”™è¯¯ï¼šæ£€æµ‹åˆ°URLä½†æœªå®‰è£… video_downloader æ¨¡å—")
            print("   è¯·å…ˆå®‰è£…ä¾èµ–: pip install yt-dlp")
            exit(1)
        
        print(f"ğŸ“¥ æ£€æµ‹åˆ°URLï¼Œå¼€å§‹ä¸‹è½½...")
        downloader = VideoDownloader(download_dir=args.download_dir)
        
        try:
            file_info = downloader.download_video(input_str)
            video_path = file_info.file_path
            source_url = input_str
            platform_title = getattr(file_info, 'title', None)
            print(f"âœ… ä¸‹è½½å®Œæˆ: {video_path}")
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            exit(1)
    else:
        # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
        video_path = Path(input_str).resolve()
        if not video_path.exists():
            print(f"âŒ é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            exit(1)

    output_dir = Path(args.out_dir).resolve()

    process_video(
        video_path=video_path,
        output_dir=output_dir,
        with_frames=args.with_frames,
        ocr_lang=args.ocr_lang,
        ocr_det_model=args.ocr_det_model,
        ocr_rec_model=args.ocr_rec_model,
        use_gpu=args.use_gpu,
        source_url=source_url,
        platform_title=platform_title,
    )


if __name__ == "__main__":
    main()
