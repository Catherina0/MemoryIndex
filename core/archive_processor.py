#!/usr/bin/env python3
"""
ç½‘é¡µå½’æ¡£å¤„ç†ä¸æ•°æ®åº“é›†æˆ
ç±»ä¼¼ db_integration.py çš„æ¶æ„ï¼Œç”¨äºç½‘é¡µå†…å®¹
"""
import sys
import json
import hashlib
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from db import VideoRepository, SearchRepository
from db.models import (
    Video, Artifact, Topic, TimelineEntry,
    SourceType, ProcessingStatus, ArtifactType
)


def _generate_folder_name_with_llm_for_archive(
    archive_result: Dict[str, Any],
    original_folder: Path
) -> Optional[str]:
    """
    \u4f7f\u7528 llama-3.1-8b-instant \u6a21\u578b\u6839\u636e\u5f52\u6863\u5185\u5bb9\u751f\u6210\u7b80\u6d01\u7684\u6587\u4ef6\u5939\u540d\u79f0
    
    Args:
        archive_result: \u5f52\u6863\u7ed3\u679c\u5b57\u5178
        original_folder: \u539f\u59cb\u6587\u4ef6\u5939\u8def\u5f84
    
    Returns:
        \u751f\u6210\u7684\u6587\u4ef6\u5939\u540d\u79f0\uff08\u4e0d\u5305\u542b\u65f6\u95f4\u6233\uff09
    """
    import os
    try:
        from groq import Groq
    except ImportError:
        print("  \u26a0\ufe0f  Groq SDK \u672a\u5b89\u88c5\uff0c\u4f7f\u7528\u9ed8\u8ba4\u6587\u4ef6\u5939\u540d")
        return None
    
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        print("  \u26a0\ufe0f  GROQ_API_KEY \u672a\u8bbe\u7f6e\uff0c\u4f7f\u7528\u9ed8\u8ba4\u6587\u4ef6\u5939\u540d")
        return None
    
    try:
        # \u8bfb\u53d6\u5f52\u6863\u7684 README.md \u5185\u5bb9
        readme_path = Path(archive_result.get('markdown_path', ''))
        if not readme_path.exists():
            # \u5c1d\u8bd5\u67e5\u627e output_path \u4e0b\u7684 README.md
            output_path = Path(archive_result.get('output_path', ''))
            readme_path = output_path / 'README.md'
            if not readme_path.exists():
                print("  \u26a0\ufe0f  \u672a\u627e\u5230 README.md\uff0c\u4f7f\u7528\u9ed8\u8ba4\u6587\u4ef6\u5939\u540d")
                return None
        
        markdown_content = readme_path.read_text(encoding='utf-8')
        
        client = Groq(api_key=api_key)
        
        # \u63d0\u53d6\u5185\u5bb9\u6458\u8981
        content_lines = markdown_content.split('\\n')
        content_start = 0
        
        # \u8df3\u8fc7 YAML frontmatter
        if content_lines and content_lines[0].strip() == '---':
            for i, line in enumerate(content_lines[1:], 1):
                if line.strip() == '---':
                    content_start = i + 1
                    break
        
        # \u83b7\u53d6\u5b9e\u9645\u5185\u5bb9
        actual_content = '\\n'.join(content_lines[content_start:])
        # \u79fb\u9664\u56fe\u7247\u94fe\u63a5
        import re
        actual_content = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', actual_content)
        # \u9650\u5236\u957f\u5ea6\u5230\u524d800\u5b57\u7b26
        content_summary = actual_content[:800].strip()
        
        if not content_summary or len(content_summary) < 20:
            print("  \u26a0\ufe0f  \u5185\u5bb9\u592a\u77ed\uff0c\u4f7f\u7528\u9ed8\u8ba4\u6587\u4ef6\u5939\u540d")
            return None
        
        title = archive_result.get('title', '\u672a\u547d\u540d')
        platform = archive_result.get('platform', 'web')
        url = archive_result.get('url', '')
        
        prompt = f"""\u6839\u636e\u4ee5\u4e0b\u7f51\u9875\u5185\u5bb9\uff0c\u751f\u6210\u4e00\u4e2a\u7b80\u6d01\u3001\u63cf\u8ff0\u6027\u7684\u6587\u4ef6\u5939\u540d\u79f0\u3002

\u7f51\u9875\u6807\u9898\uff1a{title}
\u5e73\u53f0\uff1a{platform}
URL\uff1a{url}

\u5185\u5bb9\u6458\u8981\uff1a
{content_summary}

\u8981\u6c42\uff1a
1. \u6587\u4ef6\u5939\u540d\u79f0\u5e94\u8be5\u7b80\u6d01\u660e\u4e86\uff0c\u80fd\u591f\u53cd\u6620\u5185\u5bb9\u7684\u6838\u5fc3\u4e3b\u9898
2. \u4f7f\u7528\u4e0b\u5212\u7ebf(_)\u5206\u9694\u5355\u8bcd\uff0c\u4e0d\u8981\u4f7f\u7528\u7a7a\u683c\u6216\u7279\u6b8a\u5b57\u7b26
3. \u957f\u5ea6\u4e0d\u8d85\u8fc730\u4e2a\u5b57\u7b26\uff08\u4e2d\u6587\u63092\u4e2a\u5b57\u7b26\u8ba1\u7b97\uff09
4. \u53ea\u8fd4\u56de\u6587\u4ef6\u5939\u540d\u79f0\uff0c\u4e0d\u8981\u6709\u4efb\u4f55\u89e3\u91ca\u6216\u6807\u70b9\u7b26\u53f7
5. \u4f7f\u7528\u4e2d\u6587\u6216\u82f1\u6587\u5747\u53ef\uff0c\u4f46\u8981\u786e\u4fdd\u6587\u4ef6\u7cfb\u7edf\u517c\u5bb9
6. \u4e0d\u9700\u8981\u5305\u542b\u5e73\u53f0\u540d\u79f0

\u793a\u4f8b\u683c\u5f0f\uff1a
- \u673a\u5668\u5b66\u4e60\u5165\u95e8\u6307\u5357
- Python\u6570\u636e\u5206\u6790\u6280\u5de7
- \u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u5206\u7c7b

\u8bf7\u76f4\u63a5\u8fd4\u56de\u6587\u4ef6\u5939\u540d\u79f0\uff1a"""

        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "\u4f60\u662f\u4e00\u4e2a\u6587\u4ef6\u547d\u540d\u52a9\u624b\uff0c\u64c5\u957f\u6839\u636e\u7f51\u9875\u5185\u5bb9\u751f\u6210\u7b80\u6d01\u3001\u63cf\u8ff0\u6027\u7684\u6587\u4ef6\u5939\u540d\u79f0\u3002"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=50,
            temperature=0.3,
        )
        
        folder_name = response.choices[0].message.content.strip()
        
        # æ¸…ç†æ–‡ä»¶å¤¹åç§°
        folder_name = re.sub(r'["\'\n\r\t]', '', folder_name)
        folder_name = re.sub(r'[/\\]', '_', folder_name)
        folder_name = re.sub(r'[<>:"|?*]', '', folder_name)
        
        # é™åˆ¶é•¿åº¦
        if len(folder_name) > 50:
            folder_name = folder_name[:50]
        
        # \u5982\u679c\u751f\u6210\u5931\u8d25\u6216\u4e3a\u7a7a\uff0c\u8fd4\u56de None
        if not folder_name or len(folder_name) < 3:
            print("  \u26a0\ufe0f  LLM \u751f\u6210\u7684\u6587\u4ef6\u5939\u540d\u65e0\u6548")
            return None
        
        print(f"  \u2705 LLM \u751f\u6210\u7684\u6587\u4ef6\u5939\u540d: {folder_name}")
        return folder_name
        
    except Exception as e:
        print(f"  \u26a0\ufe0f  LLM \u6587\u4ef6\u5939\u547d\u540d\u5931\u8d25: {e}")
        return None


class ArchiveProcessor:
    """ç½‘é¡µå½’æ¡£å¤„ç†ä¸æ•°æ®åº“é›†æˆ"""
    
    def __init__(self, db_path: Optional[str] = None):
        self.repo = VideoRepository(db_path)
    
    def process_and_save(
        self,
        url: str,
        output_dir: Path,
        archive_result: Dict[str, Any],
        source_type: str = 'web_archive',
        with_ocr: bool = False,
        processing_config: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        å¤„ç†ç½‘é¡µå½’æ¡£å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            url: ç½‘é¡µURL
            output_dir: è¾“å‡ºç›®å½•
            archive_result: å½’æ¡£ç»“æœå­—å…¸ï¼ˆæ¥è‡ª UniversalArchiver.archive()ï¼‰
            source_type: æ¥æºç±»å‹ï¼ˆå¦‚ zhihu, xiaohongshuç­‰ï¼‰
            with_ocr: æ˜¯å¦è¿›è¡ŒOCRè¯†åˆ«
            processing_config: å¤„ç†é…ç½®
        
        Returns:
            int: video_idï¼ˆæ•°æ®åº“ä¸»é”®ï¼Œå®é™…ä¸ºé€šç”¨å†…å®¹IDï¼‰
        """
        if not archive_result.get('success'):
            raise ValueError(f"å½’æ¡£å¤±è´¥: {archive_result.get('error')}")
        
        # 1. è®¡ç®—å†…å®¹hashï¼ˆåŸºäºURL+å†…å®¹ï¼‰
        content_for_hash = f"{url}_{archive_result.get('content', '')[:1000]}"
        content_hash = hashlib.sha256(content_for_hash.encode()).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = self.repo.get_video_by_hash(content_hash)
        if existing:
            print(f"âš ï¸  ç½‘é¡µå·²å­˜åœ¨ï¼ˆID: {existing.id}ï¼‰ï¼Œè·³è¿‡å¤„ç†")
            return existing.id
        
        # 2. ç¡®å®šsource_typeæšä¸¾
        source_type_map = {
            'zhihu': SourceType.ZHIHU,
            'xiaohongshu': SourceType.XIAOHONGSHU,
            'bilibili': SourceType.BILIBILI,
            'twitter': SourceType.TWITTER,
            'reddit': SourceType.REDDIT,
            'web': SourceType.WEB_ARCHIVE,
        }
        source_enum = source_type_map.get(
            archive_result.get('platform', source_type).lower(),
            SourceType.WEB_ARCHIVE
        )
        
        # 3. åˆ›å»ºè®°å½•ï¼ˆä½¿ç”¨Videoè¡¨ï¼Œä½†å®é™…æ˜¯ç½‘é¡µå†…å®¹ï¼‰
        video = Video(
            content_hash=content_hash,
            video_id=url,  # ä½¿ç”¨URLä½œä¸ºå”¯ä¸€æ ‡è¯†
            source_type=source_enum,
            source_url=url,
            title=archive_result.get('title', 'æœªå‘½åç½‘é¡µ'),
            platform_title=archive_result.get('title'),
            file_path=str(archive_result.get('output_path', '')),
            file_size_bytes=archive_result.get('content_length', 0),
            processing_config=processing_config or {
                'archive_mode': 'web',
                'with_ocr': with_ocr
            },
            status=ProcessingStatus.PROCESSING
        )
        
        try:
            db_id = self.repo.create_video(video)
            print(f"âœ… åˆ›å»ºå½’æ¡£è®°å½•: ID={db_id}")
            
            # 4. ä¿å­˜åŸå§‹å†…å®¹
            content_artifact = Artifact(
                video_id=db_id,
                artifact_type=ArtifactType.TRANSCRIPT,  # å¤ç”¨transcriptç±»å‹å­˜å‚¨ç½‘é¡µå†…å®¹
                content_text=archive_result.get('content', ''),
                content_json={
                    'url': url,
                    'title': archive_result.get('title'),
                    'platform': archive_result.get('platform'),
                    'content_length': archive_result.get('content_length'),
                    'archive_time': datetime.now().isoformat()
                },
                file_path=str(archive_result.get('output_path', '')),
                model_name='crawl4ai' if 'crawl4ai' in str(archive_result) else 'drissionpage'
            )
            self.repo.save_artifact(content_artifact)
            print("âœ… ä¿å­˜å½’æ¡£å†…å®¹")
            
            # 5. å¦‚æœæœ‰OCRï¼Œå¤„ç†å¹¶ä¿å­˜
            if with_ocr and archive_result.get('output_path'):
                ocr_result = self._process_ocr_for_archive(
                    archive_result.get('output_path'),
                    output_dir
                )
                if ocr_result:
                    ocr_artifact = Artifact(
                        video_id=db_id,
                        artifact_type=ArtifactType.OCR,
                        content_text=self._extract_plain_text(ocr_result),
                        content_json=ocr_result,
                        file_path=str(output_dir / 'archive_ocr.json'),
                        model_name=ocr_result.get('engine', 'vision_ocr')
                    )
                    self.repo.save_artifact(ocr_artifact)
                    print("âœ… ä¿å­˜OCRç»“æœ")
            
            # 6. ç”ŸæˆAIæŠ¥å‘Šï¼ˆå¦‚æœé…ç½®äº†GROQ_API_KEYï¼‰
            # è¯»å–å½’æ¡£çš„Markdownå†…å®¹ï¼ˆä½¿ç”¨å®é™…çš„output_dirï¼Œä¸æ˜¯archive_resultä¸­çš„æ—§è·¯å¾„ï¼‰
            archived_content = self._read_archived_content(str(output_dir))
            
            # å¦‚æœæœ‰OCRç»“æœï¼Œåˆå¹¶åˆ°å†…å®¹ä¸­
            if with_ocr and ocr_result:
                archived_content += f"\n\n## OCRè¯†åˆ«æ–‡å­—\n\n{ocr_result['combined_text']}"
            
            print(f"  ğŸ“ å†…å®¹é•¿åº¦: {len(archived_content)} å­—ç¬¦")
            
            report_data = self._generate_report_for_archive(
                archived_content,
                output_dir,
                with_ocr
            )
            if report_data:
                report_artifact = Artifact(
                    video_id=db_id,
                    artifact_type=ArtifactType.REPORT,
                    content_text=report_data.get('content', ''),
                    content_json=report_data,
                    file_path=str(output_dir / 'report.md'),
                    model_name=report_data.get('model', 'llama-3.3-70b')
                )
                self.repo.save_artifact(report_artifact)
                print("âœ… ä¿å­˜AIæŠ¥å‘Š")
                
                # 7. æå–å¹¶ä¿å­˜æ ‡ç­¾
                tags = self._extract_tags(report_data)
                if tags:
                    self.repo.save_tags(db_id, tags, source='auto')
                    print(f"âœ… ä¿å­˜æ ‡ç­¾: {', '.join(tags)}")
                
                # 8. æå–å¹¶ä¿å­˜ä¸»é¢˜
                topics = self._extract_topics(report_data)
                if topics:
                    self.repo.save_topics(db_id, topics)
                    print(f"âœ… ä¿å­˜ {len(topics)} ä¸ªä¸»é¢˜")
            
            # 9. æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•
            self.repo.update_fts_index(db_id)
            print("âœ… æ›´æ–°æœç´¢ç´¢å¼•")
            
            # 10. æ ‡è®°å¤„ç†å®Œæˆ
            self.repo.update_video_status(db_id, ProcessingStatus.COMPLETED)
            print(f"ğŸ‰ å½’æ¡£å¤„ç†å®Œæˆ: ID={db_id}")
            
            return db_id
            
        except Exception as e:
            # æ ‡è®°å¤±è´¥
            if 'db_id' in locals():
                self.repo.update_video_status(
                    db_id,
                    ProcessingStatus.FAILED,
                    str(e)
                )
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _process_ocr_for_archive(
        self,
        markdown_path: str,
        output_dir: Path
    ) -> Optional[Dict]:
        """
        å¯¹å½’æ¡£çš„å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
        æ‰«æoutput_dir/imagesç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡å¹¶è¿›è¡ŒOCR
        """
        try:
            from ocr.ocr_vision import init_vision_ocr, ocr_image_vision
        except ImportError:
            print("  âš ï¸  OCRæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡OCRè¯†åˆ«")
            return None
        
        # æŸ¥æ‰¾imagesç›®å½•
        images_dir = None
        
        # å°è¯•åœ¨output_dirä¸­æŸ¥æ‰¾imagesç›®å½•
        for item in output_dir.iterdir():
            if item.is_dir():
                images_subdir = item / 'images'
                if images_subdir.exists() and images_subdir.is_dir():
                    images_dir = images_subdir
                    break
        
        if not images_dir or not images_dir.exists():
            print("  â„¹ï¸  æœªæ‰¾åˆ°imagesç›®å½•ï¼Œè·³è¿‡OCRè¯†åˆ«")
            return None
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = list(images_dir.glob('*.jpg')) + \
                     list(images_dir.glob('*.jpeg')) + \
                     list(images_dir.glob('*.png')) + \
                     list(images_dir.glob('*.webp'))
        
        if not image_files:
            print("  â„¹ï¸  imagesç›®å½•ä¸ºç©ºï¼Œè·³è¿‡OCRè¯†åˆ«")
            return None
        
        print(f"  ğŸ” å‘ç° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹OCRè¯†åˆ«...")
        
        # åˆå§‹åŒ–Vision OCR
        try:
            ocr_instance = init_vision_ocr()
        except Exception as e:
            print(f"  âš ï¸  Vision OCRåˆå§‹åŒ–å¤±è´¥: {e}")
            return None
        
        # å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡ŒOCR
        ocr_results = []
        for i, img_path in enumerate(image_files, 1):
            try:
                print(f"    å¤„ç†å›¾ç‰‡ {i}/{len(image_files)}: {img_path.name}")
                text = ocr_image_vision(ocr_instance, str(img_path))
                if text and text.strip():
                    ocr_results.append({
                        'image': img_path.name,
                        'text': text.strip(),
                        'length': len(text.strip())
                    })
                    print(f"      âœ“ è¯†åˆ«æ–‡å­— {len(text.strip())} å­—ç¬¦")
                else:
                    print(f"      - æœªè¯†åˆ«åˆ°æ–‡å­—")
            except Exception as e:
                print(f"      âœ— OCRå¤±è´¥: {e}")
        
        if not ocr_results:
            print("  â„¹ï¸  æ‰€æœ‰å›¾ç‰‡å‡æœªè¯†åˆ«åˆ°æ–‡å­—")
            return None
        
        print(f"  âœ… OCRå®Œæˆï¼š{len(ocr_results)} å¼ å›¾ç‰‡è¯†åˆ«å‡ºæ–‡å­—")
        
        return {
            'engine': 'vision_ocr',
            'total_images': len(image_files),
            'recognized_images': len(ocr_results),
            'results': ocr_results,
            'combined_text': '\n\n'.join([f"[{r['image']}]\n{r['text']}" for r in ocr_results])
        }
    
    def _generate_report_for_archive(
        self,
        content: str,
        output_dir: Path,
        with_ocr: bool = False
    ) -> Optional[Dict]:
        """
        ä½¿ç”¨AIç”Ÿæˆç½‘é¡µå†…å®¹æŠ¥å‘Š
        è°ƒç”¨ä¸è§†é¢‘å¤„ç†ç›¸åŒçš„LLM
        """
        import os
        try:
            from groq import Groq
        except ImportError:
            print("  âš ï¸  Groq SDK æœªå®‰è£…ï¼Œè·³è¿‡AIæŠ¥å‘Šç”Ÿæˆ")
            return None
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            print("  âš ï¸  GROQ_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡AIæŠ¥å‘Šç”Ÿæˆ")
            return None
        
        try:
            client = Groq(api_key=api_key)
            model = os.getenv("GROQ_LLM_MODEL", "openai/gpt-oss-120b")
            max_tokens = int(os.getenv("GROQ_MAX_TOKENS", "8192"))
            temperature = float(os.getenv("GROQ_TEMPERATURE", "0.7"))
            
            prompt = f"""
è¯·å°†ä»¥ä¸‹ç½‘é¡µå†…å®¹æ•´ç†æˆä¸€ä»½**ç»“æ„åŒ– Markdown çŸ¥è¯†æ¡£æ¡ˆ**ã€‚

**âš ï¸ é‡è¦ï¼šè¯†åˆ«é”™è¯¯ä¿®æ­£**
- ç½‘é¡µå¯èƒ½å­˜åœ¨æ’ç‰ˆé—®é¢˜æˆ–OCRè¯†åˆ«é”™è¯¯
- è¯·ä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£åŒéŸ³å­—/è¯é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯ä¸“ä¸šæœ¯è¯­
- ä½¿ç”¨å‡†ç¡®ã€ä¸“ä¸šçš„æœ¯è¯­è¡¨è¾¾

ä½ éœ€è¦ï¼š
1. **ä½¿ç”¨ Markdown** è¾“å‡ºï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€å¼•ç”¨ã€è¡¨æ ¼ç­‰ï¼‰
2. æå–ä¸»è¦è§‚ç‚¹å’Œæ ¸å¿ƒå†…å®¹
3. è‡ªåŠ¨è¯†åˆ«"ä¸»é¢˜/ç« èŠ‚"å¹¶ç»“æ„åŒ–æ€»ç»“
4. æå–é‡è¦æ•°æ®ï¼šæ•°å­—ã€è§„åˆ™ã€å¼•ç”¨ã€æ—¥æœŸç­‰
5. ç”Ÿæˆæ ‡ç­¾å’Œæ‘˜è¦ï¼š
   - **æ ‡ç­¾ï¼ˆtagsï¼‰**ï¼š3-6ä¸ªé«˜åº¦æ¦‚æ‹¬çš„ä¸»é¢˜æ ‡ç­¾ï¼Œå¦‚"æŠ€æœ¯"ã€"æ•™è‚²"ã€"äººæ–‡"ç­‰
   - **æ‘˜è¦**ï¼šä¸è¶…è¿‡50ä¸ªå­—çš„ç³»ç»Ÿæ€§å†…å®¹æ¦‚æ‹¬

æ¨èç»“æ„ï¼š
## æ‘˜è¦
ï¼ˆä¸è¶…è¿‡50å­—çš„æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬ï¼‰

## ä¸»è¦å†…å®¹
## å…³é”®è§‚ç‚¹
## é‡è¦ä¿¡æ¯
## æ ‡ç­¾
æ ¼å¼ï¼šæ ‡ç­¾: æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3

ä»¥ä¸‹æ˜¯ç½‘é¡µå†…å®¹ï¼š
{content[:30000]}
"""

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ•´ç†åŠ©æ‰‹ï¼Œå…·å¤‡æ™ºèƒ½çº é”™èƒ½åŠ›ã€‚
                        ä½ çš„ä»»åŠ¡æ˜¯ä»ç½‘é¡µå†…å®¹ä¸­æå–æ ¸å¿ƒä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„çŸ¥è¯†æ¡£æ¡ˆã€‚
                        ç¡®ä¿è¾“å‡ºä½¿ç”¨å‡†ç¡®ã€ä¸“ä¸šçš„æœ¯è¯­è¡¨è¾¾ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            report_content = response.choices[0].message.content
            
            # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            report_path = output_dir / 'report.md'
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            return {
                'content': report_content,
                'model': model,
                'tags': self._parse_tags_from_content(report_content),
                'topics': []  # TODO: ä»æŠ¥å‘Šä¸­è§£æä¸»é¢˜
            }
        except Exception as e:
            print(f"  âœ— AIæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _read_archived_content(self, output_path: str) -> str:
        """è¯»å–å½’æ¡£çš„Markdownå†…å®¹"""
        if not output_path:
            return ""
        
        try:
            output_path_obj = Path(output_path)
            
            # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’æŸ¥æ‰¾ README.md
            if output_path_obj.is_dir():
                # å…ˆæ£€æŸ¥å½“å‰ç›®å½•
                readme_path = output_path_obj / "README.md"
                if readme_path.exists():
                    with open(readme_path, 'r', encoding='utf-8') as f:
                        return f.read()
                
                # æŸ¥æ‰¾å­ç›®å½•ä¸­çš„ README.md
                for readme in output_path_obj.rglob("README.md"):
                    try:
                        with open(readme, 'r', encoding='utf-8') as f:
                            return f.read()
                    except Exception:
                        continue
                
                print(f"  âš ï¸  æœªæ‰¾åˆ° README.md åœ¨: {output_path}")
            # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥è¯»å–
            elif output_path_obj.is_file():
                with open(output_path_obj, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                print(f"  âš ï¸  è·¯å¾„ä¸å­˜åœ¨: {output_path}")
        except Exception as e:
            print(f"  âš ï¸  è¯»å–å½’æ¡£å†…å®¹å¤±è´¥: {e}")
        
        return ""
    
    def _extract_plain_text(self, data: Dict) -> str:
        """ä»ç»“æ„åŒ–æ•°æ®æå–çº¯æ–‡æœ¬"""
        if isinstance(data, dict):
            if 'combined_text' in data:
                return data['combined_text']
            elif 'text' in data:
                return data['text']
            elif 'content' in data:
                return data['content']
            return json.dumps(data, ensure_ascii=False)
        return str(data)
    
    def _extract_tags(self, report_data: Dict) -> list:
        """ä»æŠ¥å‘Šä¸­æå–æ ‡ç­¾"""
        return report_data.get('tags', [])
    
    def _extract_topics(self, report_data: Dict) -> list:
        """ä»æŠ¥å‘Šä¸­æå–ä¸»é¢˜"""
        topics_data = report_data.get('topics', [])
        topics = []
        
        for i, topic_data in enumerate(topics_data):
            topic = Topic(
                video_id=0,  # ç¨åå¡«å……
                title=topic_data.get('title', ''),
                summary=topic_data.get('summary'),
                start_time=None,  # ç½‘é¡µå†…å®¹æ²¡æœ‰æ—¶é—´è½´
                end_time=None,
                keywords=topic_data.get('keywords', []),
                key_points=topic_data.get('key_points', []),
                sequence=i
            )
            topics.append(topic)
        
        return topics
    
    def _parse_tags_from_content(self, content: str) -> list:
        """ä»æŠ¥å‘Šå†…å®¹ä¸­è§£ææ ‡ç­¾"""
        import re
        # æŸ¥æ‰¾ "æ ‡ç­¾: xxx, xxx" æ ¼å¼
        tag_match = re.search(r'æ ‡ç­¾[ï¼š:]\s*(.+)', content)
        if tag_match:
            tags_str = tag_match.group(1)
            tags = [tag.strip() for tag in re.split(r'[,ï¼Œ]', tags_str)]
            return [tag for tag in tags if tag and len(tag) < 20]
        return []


async def archive_and_save(
    url: str,
    output_dir: str = "output",
    with_ocr: bool = False,
    headless: bool = True
) -> int:
    """
    å®Œæ•´çš„å½’æ¡£æµç¨‹ï¼šå½’æ¡£ç½‘é¡µ â†’ ç”ŸæˆæŠ¥å‘Š â†’ å­˜å…¥æ•°æ®åº“
    
    Args:
        url: ç½‘é¡µURL
        output_dir: è¾“å‡ºç›®å½•
        with_ocr: æ˜¯å¦è¿›è¡ŒOCRè¯†åˆ«
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    
    Returns:
        int: æ•°æ®åº“è®°å½•ID
    """
    from archiver import UniversalArchiver
    
    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    output_path = Path(output_dir) / f"archive_{url_hash}_{timestamp}"
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_path}")
    
    # 2. æ‰§è¡Œç½‘é¡µå½’æ¡£
    print(f"\nğŸŒ å¼€å§‹å½’æ¡£: {url}")
    archiver = UniversalArchiver(
        output_dir=str(output_path),
        headless=headless,
        verbose=True
    )
    
    archive_result = await archiver.archive(url)
    
    if not archive_result.get('success'):
        raise Exception(f"å½’æ¡£å¤±è´¥: {archive_result.get('error')}")
    
    print(f"âœ… å½’æ¡£å®Œæˆ: {archive_result['output_path']}")
    
    # 3. ä½¿ç”¨ LLM é‡å‘½åå¤–å±‚æ–‡ä»¶å¤¹
    print(f"\nğŸ¤– ç”Ÿæˆè¯­ä¹‰åŒ–æ–‡ä»¶å¤¹å...")
    new_folder_name = _generate_folder_name_with_llm_for_archive(
        archive_result=archive_result,
        original_folder=output_path
    )
    
    if new_folder_name and new_folder_name != output_path.name:
        new_output_path = Path(output_dir) / f"{new_folder_name}_{timestamp}"
        try:
            # å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œæ·»åŠ åç¼€
            counter = 1
            temp_path = new_output_path
            while temp_path.exists():
                temp_path = Path(output_dir) / f"{new_folder_name}_{timestamp}_{counter}"
                counter += 1
            new_output_path = temp_path
            
            output_path.rename(new_output_path)
            output_path = new_output_path
            print(f"âœ… æ–‡ä»¶å¤¹å·²é‡å‘½å: {output_path.name}")
        except Exception as e:
            print(f"âš ï¸  æ–‡ä»¶å¤¹é‡å‘½åå¤±è´¥: {e}")
    
    # 4. ä¿å­˜åˆ°æ•°æ®åº“
    print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
    processor = ArchiveProcessor()
    db_id = processor.process_and_save(
        url=url,
        output_dir=output_path,
        archive_result=archive_result,
        with_ocr=with_ocr,
        processing_config={
            'archive_mode': 'web',
            'with_ocr': with_ocr,
            'headless': headless
        }
    )
    
    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"   ğŸ“Š æ•°æ®åº“ID: {db_id}")
    print(f"   ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    print(f"   ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {output_path}/report.md")
    print(f"{'='*60}")
    
    return db_id


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç½‘é¡µå½’æ¡£ä¸æ•°æ®åº“é›†æˆ')
    parser.add_argument('url', help='ç½‘é¡µURL')
    parser.add_argument('--output-dir', default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--with-ocr', action='store_true', help='å¯ç”¨OCRè¯†åˆ«')
    parser.add_argument('--visible', action='store_true', help='æ˜¾ç¤ºæµè§ˆå™¨ï¼ˆè°ƒè¯•ï¼‰')
    
    args = parser.parse_args()
    
    # è¿è¡Œå¼‚æ­¥å½’æ¡£
    db_id = asyncio.run(archive_and_save(
        url=args.url,
        output_dir=args.output_dir,
        with_ocr=args.with_ocr,
        headless=not args.visible
    ))
    
    print(f"\nğŸ‰ å½’æ¡£æˆåŠŸï¼æ•°æ®åº“ID: {db_id}")


if __name__ == '__main__':
    main()
