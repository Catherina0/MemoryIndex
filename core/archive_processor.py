#!/usr/bin/env python3
"""
ç½‘é¡µå½’æ¡£å¤„ç†ä¸æ•°æ®åº“é›†æˆ
ç±»ä¼¼ db_integration.py çš„æ¶æ„ï¼Œç”¨äºç½‘é¡µå†…å®¹
"""
import sys
import json
import hashlib
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from db import VideoRepository, SearchRepository
from db.models import (
    Video, Artifact, Topic, TimelineEntry,
    SourceType, ProcessingStatus, ArtifactType
)


def _generate_folder_name_with_llm_for_archive(
    archive_result: Dict[str, Any],
    original_folder: Path
) -> Optional[str]:
    """
    ä½¿ç”¨ openai/gpt-oss-20b æ¨¡å‹æ ¹æ®å½’æ¡£å†…å®¹ç”Ÿæˆç®€æ´çš„æ–‡ä»¶å¤¹åç§°
    
    Args:
        archive_result: å½’æ¡£ç»“æœå­—å…¸
        original_folder: åŸå§‹æ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶å¤¹åç§°ï¼ˆä¸åŒ…å«æ—¶é—´æˆ³ï¼‰
    """
    import os
    try:
        from groq import Groq
    except ImportError:
        print("  âš ï¸  Groq SDK æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
        return None
    
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        print("  âš ï¸  GROQ_API_KEY æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
        return None
    
    try:
        # è¯»å–å½’æ¡£çš„ README.md å†…å®¹
        readme_path = Path(archive_result.get('markdown_path', ''))
        if not readme_path.exists():
            # å°è¯•æŸ¥æ‰¾ output_path ä¸‹çš„ README.md
            output_path = Path(archive_result.get('output_path', ''))
            readme_path = output_path / 'README.md'
            if not readme_path.exists():
                print("  âš ï¸  æœªæ‰¾åˆ° README.mdï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
                return None
        
        markdown_content = readme_path.read_text(encoding='utf-8')
        
        client = Groq(api_key=api_key)
        
        # æå–å†…å®¹æ‘˜è¦
        content_lines = markdown_content.split('\\n')
        content_start = 0
        
        # è·³è¿‡ YAML frontmatter
        if content_lines and content_lines[0].strip() == '---':
            for i, line in enumerate(content_lines[1:], 1):
                if line.strip() == '---':
                    content_start = i + 1
                    break
        
        # è·å–å®é™…å†…å®¹
        actual_content = '\\n'.join(content_lines[content_start:])
        # ç§»é™¤å›¾ç‰‡é“¾æ¥
        import re
        actual_content = re.sub(r'!\[.*?\]\(.*?\)', '', actual_content)
        # é™åˆ¶é•¿åº¦åˆ°å‰800å­—ç¬¦
        content_summary = actual_content[:800].strip()
        
        if not content_summary or len(content_summary) < 20:
            print("  âš ï¸  å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
            return None
        
        title = archive_result.get('title', 'æœªå‘½å')
        platform = archive_result.get('platform', 'web')
        url = archive_result.get('url', '')
        
            # Build prompt without backslashes in f-string
            newline = '\n'
            prompt = f"""æ ¹æ®ä»¥ä¸‹ç½‘é¡µå†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´ã€æè¿°æ€§çš„æ–‡ä»¶å¤¹åç§°ã€‚

ç½‘é¡µæ ‡é¢˜ï¼š{title}
å¹³å°ï¼š{platform}
URLï¼š{url}

å†…å®¹æ‘˜è¦ï¼š
{content_summary}

è¦æ±‚ï¼š
1. æ–‡ä»¶å¤¹åç§°åº”è¯¥ç®€æ´æ˜äº†ï¼Œèƒ½å¤Ÿåæ˜ å†…å®¹çš„æ ¸å¿ƒä¸»é¢˜
2. ä½¿ç”¨ä¸‹åˆ’çº¿(_)åˆ†éš”å•è¯ï¼Œä¸è¦ä½¿ç”¨ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
3. é•¿åº¦ä¸è¶…è¿‡30ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—ï¼‰
4. åªè¿”å›æ–‡ä»¶å¤¹åç§°ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·
5. ä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡å‡å¯ï¼Œä½†è¦ç¡®ä¿æ–‡ä»¶ç³»ç»Ÿå…¼å®¹
6. ä¸éœ€è¦åŒ…å«å¹³å°åç§°

ç¤ºä¾‹æ ¼å¼ï¼š
- æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—
- Pythonæ•°æ®åˆ†ææŠ€å·§
- æ·±åº¦å­¦ä¹ å›¾åƒåˆ†ç±»

è¯·ç›´æ¥è¿”å›æ–‡ä»¶å¤¹åç§°ï¼š"""

        response = client.chat.completions.create(
            model="openai/gpt-oss-20b",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶å‘½ååŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´ã€æè¿°æ€§çš„æ–‡ä»¶å¤¹åç§°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=50,
            temperature=0.3,
        )
        
        folder_name = response.choices[0].message.content.strip()
        
        # æ¸…ç†æ–‡ä»¶å¤¹åç§°
        folder_name = re.sub(r'["\'\n\r\t]', '', folder_name)
        folder_name = re.sub(r'[/\\]', '_', folder_name)
        folder_name = re.sub(r'[<>:"|?*]', '', folder_name)
        
        # é™åˆ¶é•¿åº¦
        if len(folder_name) > 50:
            folder_name = folder_name[:50]
        
        # å¦‚æœç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©ºï¼Œè¿”å› None
        if not folder_name or len(folder_name) < 3:
            print("  âš ï¸  LLM ç”Ÿæˆçš„æ–‡ä»¶å¤¹åæ— æ•ˆ")
            return None
        
        print(f"  âœ… LLM ç”Ÿæˆçš„æ–‡ä»¶å¤¹å: {folder_name}")
        return folder_name
        
    except Exception as e:
        print(f"  âš ï¸  LLM æ–‡ä»¶å¤¹å‘½åå¤±è´¥: {e}")
        return None


class ArchiveProcessor:
    """ç½‘é¡µå½’æ¡£å¤„ç†ä¸æ•°æ®åº“é›†æˆ"""
    
    # OSS-120b æ¨¡å‹é™åˆ¶
    MAX_CONTEXT_TOKENS = 131072  # æœ€å¤§ä¸Šä¸‹æ–‡çª—å£
    MAX_OUTPUT_TOKENS = 65536    # æœ€å¤§è¾“å‡º tokens
    LONG_TEXT_THRESHOLD = 100000  # å¯åŠ¨é•¿æ–‡æœ¬æ¨¡å¼çš„é˜ˆå€¼
    
    def __init__(self, db_path: Optional[str] = None):
        self.repo = VideoRepository(db_path)
    
    def process_and_save(
        self,
        url: str,
        output_dir: Path,
        archive_result: Dict[str, Any],
        source_type: str = 'web_archive',
        with_ocr: bool = False,
        processing_config: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        å¤„ç†ç½‘é¡µå½’æ¡£å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            url: ç½‘é¡µURL
            output_dir: è¾“å‡ºç›®å½•
            archive_result: å½’æ¡£ç»“æœå­—å…¸ï¼ˆæ¥è‡ª UniversalArchiver.archive()ï¼‰
            source_type: æ¥æºç±»å‹ï¼ˆå¦‚ zhihu, xiaohongshuç­‰ï¼‰
            with_ocr: æ˜¯å¦è¿›è¡ŒOCRè¯†åˆ«
            processing_config: å¤„ç†é…ç½®
        
        Returns:
            int: video_idï¼ˆæ•°æ®åº“ä¸»é”®ï¼Œå®é™…ä¸ºé€šç”¨å†…å®¹IDï¼‰
        """
        if not archive_result.get('success'):
            raise ValueError(f"å½’æ¡£å¤±è´¥: {archive_result.get('error')}")
        
        # 1. è®¡ç®—å†…å®¹hashï¼ˆåŸºäºURL+å†…å®¹ï¼‰
        content_for_hash = f"{url}_{archive_result.get('content', '')[:1000]}"
        content_hash = hashlib.sha256(content_for_hash.encode()).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = self.repo.get_video_by_hash(content_hash)
        if existing:
            print(f"âš ï¸  ç½‘é¡µå·²å­˜åœ¨ï¼ˆID: {existing.id}ï¼‰ï¼Œè·³è¿‡å¤„ç†")
            return existing.id
        
        # 2. ç¡®å®šsource_typeæšä¸¾
        source_type_map = {
            'zhihu': SourceType.ZHIHU,
            'xiaohongshu': SourceType.XIAOHONGSHU,
            'bilibili': SourceType.BILIBILI,
            'twitter': SourceType.TWITTER,
            'reddit': SourceType.REDDIT,
            'web': SourceType.WEB_ARCHIVE,
        }
        source_enum = source_type_map.get(
            archive_result.get('platform', source_type).lower(),
            SourceType.WEB_ARCHIVE
        )
        
        # 3. åˆ›å»ºè®°å½•ï¼ˆä½¿ç”¨Videoè¡¨ï¼Œä½†å®é™…æ˜¯ç½‘é¡µå†…å®¹ï¼‰
        video = Video(
            content_hash=content_hash,
            video_id=url,  # ä½¿ç”¨URLä½œä¸ºå”¯ä¸€æ ‡è¯†
            source_type=source_enum,
            source_url=url,
            title=archive_result.get('title', 'æœªå‘½åç½‘é¡µ'),
            platform_title=archive_result.get('title'),
            file_path=str(archive_result.get('output_path', '')),
            file_size_bytes=archive_result.get('content_length', 0),
            processing_config=processing_config or {
                'archive_mode': 'web',
                'with_ocr': with_ocr
            },
            status=ProcessingStatus.PROCESSING
        )
        
        try:
            db_id = self.repo.create_video(video)
            print(f"âœ… åˆ›å»ºå½’æ¡£è®°å½•: ID={db_id}")
            
            # 4. ä¿å­˜åŸå§‹å†…å®¹
            content_artifact = Artifact(
                video_id=db_id,
                artifact_type=ArtifactType.TRANSCRIPT,  # å¤ç”¨transcriptç±»å‹å­˜å‚¨ç½‘é¡µå†…å®¹
                content_text=archive_result.get('content', ''),
                content_json={
                    'url': url,
                    'title': archive_result.get('title'),
                    'platform': archive_result.get('platform'),
                    'content_length': archive_result.get('content_length'),
                    'archive_time': datetime.now().isoformat()
                },
                file_path=str(archive_result.get('output_path', '')),
                model_name='crawl4ai' if 'crawl4ai' in str(archive_result) else 'drissionpage'
            )
            self.repo.save_artifact(content_artifact)
            print("âœ… ä¿å­˜å½’æ¡£å†…å®¹")
            
            # 5. å¦‚æœæœ‰OCRï¼Œå¤„ç†å¹¶ä¿å­˜
            if with_ocr and archive_result.get('output_path'):
                ocr_result = self._process_ocr_for_archive(
                    archive_result.get('output_path'),
                    output_dir
                )
                if ocr_result:
                    ocr_artifact = Artifact(
                        video_id=db_id,
                        artifact_type=ArtifactType.OCR,
                        content_text=self._extract_plain_text(ocr_result),
                        content_json=ocr_result,
                        file_path=str(output_dir / 'archive_ocr.json'),
                        model_name=ocr_result.get('engine', 'vision_ocr')
                    )
                    self.repo.save_artifact(ocr_artifact)
                    print("âœ… ä¿å­˜OCRç»“æœ")
            
            # 6. ç”ŸæˆAIæŠ¥å‘Šï¼ˆå¦‚æœé…ç½®äº†GROQ_API_KEYï¼‰
            # è¯»å–å½’æ¡£çš„Markdownå†…å®¹ï¼ˆä½¿ç”¨å®é™…çš„output_dirï¼Œä¸æ˜¯archive_resultä¸­çš„æ—§è·¯å¾„ï¼‰
            archived_content = self._read_archived_content(str(output_dir))
            
            # å¦‚æœæœ‰OCRç»“æœï¼Œåˆå¹¶åˆ°å†…å®¹ä¸­
            if with_ocr and ocr_result:
                archived_content += f"\n\n## OCRè¯†åˆ«æ–‡å­—\n\n{ocr_result['combined_text']}"
            
            print(f"  ğŸ“ å†…å®¹é•¿åº¦: {len(archived_content)} å­—ç¬¦")
            
            report_data = self._generate_report_for_archive(
                archived_content,
                output_dir,
                with_ocr
            )
            if report_data:
                report_artifact = Artifact(
                    video_id=db_id,
                    artifact_type=ArtifactType.REPORT,
                    content_text=report_data.get('content', ''),
                    content_json=report_data,
                    file_path=str(output_dir / 'report.md'),
                    model_name=report_data.get('model', 'openai/gpt-oss-20b')
                )
                self.repo.save_artifact(report_artifact)
                print("âœ… ä¿å­˜AIæŠ¥å‘Š")
                
                # 7. æå–å¹¶ä¿å­˜æ ‡ç­¾
                tags = self._extract_tags(report_data)
                if tags:
                    self.repo.save_tags(db_id, tags, source='auto')
                    print(f"âœ… ä¿å­˜æ ‡ç­¾: {', '.join(tags)}")
                
                # 8. æå–å¹¶ä¿å­˜ä¸»é¢˜
                topics = self._extract_topics(report_data)
                if topics:
                    self.repo.save_topics(db_id, topics)
                    print(f"âœ… ä¿å­˜ {len(topics)} ä¸ªä¸»é¢˜")
            
            # 9. ä¿å­˜åˆ° archived/ æ–‡ä»¶å¤¹ï¼ˆç”¨äºå…¨æ–‡æœç´¢ï¼‰
            self._save_to_archived_folder(
                output_dir=output_dir,
                url=url,
                title=archive_result.get('title', 'æœªå‘½åç½‘é¡µ'),
                platform=archive_result.get('platform', 'web')
            )
            
            # 10. æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•
            self.repo.update_fts_index(db_id)
            print("âœ… æ›´æ–°æœç´¢ç´¢å¼•")
            
            # 11. æ ‡è®°å¤„ç†å®Œæˆ
            self.repo.update_video_status(db_id, ProcessingStatus.COMPLETED)
            print(f"ğŸ‰ å½’æ¡£å¤„ç†å®Œæˆ: ID={db_id}")
            
            return db_id
            
        except Exception as e:
            # æ ‡è®°å¤±è´¥
            if 'db_id' in locals():
                self.repo.update_video_status(
                    db_id,
                    ProcessingStatus.FAILED,
                    str(e)
                )
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _process_ocr_for_archive(
        self,
        markdown_path: str,
        output_dir: Path
    ) -> Optional[Dict]:
        """
        å¯¹å½’æ¡£çš„å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
        æ‰«æoutput_dir/imagesç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡å¹¶è¿›è¡ŒOCR
        """
        try:
            from ocr.ocr_vision import init_vision_ocr, ocr_image_vision
        except ImportError:
            print("  âš ï¸  OCRæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡OCRè¯†åˆ«")
            return None
        
        # æŸ¥æ‰¾imagesç›®å½•
        images_dir = None
        
        # å°è¯•åœ¨output_dirä¸­æŸ¥æ‰¾imagesç›®å½•
        for item in output_dir.iterdir():
            if item.is_dir():
                images_subdir = item / 'images'
                if images_subdir.exists() and images_subdir.is_dir():
                    images_dir = images_subdir
                    break
        
        if not images_dir or not images_dir.exists():
            print("  â„¹ï¸  æœªæ‰¾åˆ°imagesç›®å½•ï¼Œè·³è¿‡OCRè¯†åˆ«")
            return None
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = list(images_dir.glob('*.jpg')) + \
                     list(images_dir.glob('*.jpeg')) + \
                     list(images_dir.glob('*.png')) + \
                     list(images_dir.glob('*.webp'))
        
        if not image_files:
            print("  â„¹ï¸  imagesç›®å½•ä¸ºç©ºï¼Œè·³è¿‡OCRè¯†åˆ«")
            return None
        
        print(f"  ğŸ” å‘ç° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹OCRè¯†åˆ«...")
        
        # åˆå§‹åŒ–Vision OCR
        try:
            ocr_instance = init_vision_ocr()
        except Exception as e:
            print(f"  âš ï¸  Vision OCRåˆå§‹åŒ–å¤±è´¥: {e}")
            return None
        
        # å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡ŒOCR
        ocr_results = []
        for i, img_path in enumerate(image_files, 1):
            try:
                print(f"    å¤„ç†å›¾ç‰‡ {i}/{len(image_files)}: {img_path.name}")
                text = ocr_image_vision(ocr_instance, str(img_path))
                if text and text.strip():
                    ocr_results.append({
                        'image': img_path.name,
                        'text': text.strip(),
                        'length': len(text.strip())
                    })
                    print(f"      âœ“ è¯†åˆ«æ–‡å­— {len(text.strip())} å­—ç¬¦")
                else:
                    print(f"      - æœªè¯†åˆ«åˆ°æ–‡å­—")
            except Exception as e:
                print(f"      âœ— OCRå¤±è´¥: {e}")
        
        if not ocr_results:
            print("  â„¹ï¸  æ‰€æœ‰å›¾ç‰‡å‡æœªè¯†åˆ«åˆ°æ–‡å­—")
            return None
        
        print(f"  âœ… OCRå®Œæˆï¼š{len(ocr_results)} å¼ å›¾ç‰‡è¯†åˆ«å‡ºæ–‡å­—")
        
        return {
            'engine': 'vision_ocr',
            'total_images': len(image_files),
            'recognized_images': len(ocr_results),
            'results': ocr_results,
            'combined_text': '\n\n'.join([f"[{r['image']}]\n{r['text']}" for r in ocr_results])
        }
    
    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡
        ä½¿ç”¨ç®€å•è§„åˆ™ï¼šä¸­æ–‡å­—ç¬¦çº¦1.5 tokensï¼Œè‹±æ–‡å•è¯çº¦1 token
        """
        chinese_chars = len([c for c in text if 'ä¸€' <= c <= 'é¿¿'])
        other_chars = len(text) - chinese_chars
        # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡ 1.5 tokens/charï¼Œè‹±æ–‡ 4 chars/token
        return int(chinese_chars * 1.5 + other_chars / 4)
    
    def _split_content_by_tokens(self, content: str, max_tokens: int) -> list:
        """
        å°†å†…å®¹æŒ‰ token é™åˆ¶åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µ
        å°½é‡ä¿æŒæ®µè½å®Œæ•´æ€§
        """
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self._estimate_tokens(para)
            
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶åˆ†å‰²
            if para_tokens > max_tokens:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_tokens = 0
                
                # æŒ‰å­—ç¬¦å¼ºåˆ¶åˆ†å‰²è¶…é•¿æ®µè½
                chars_per_token = len(para) / para_tokens if para_tokens > 0 else 1
                chunk_size = int(max_tokens * chars_per_token * 0.9)  # ä¿ç•™10%ä½™é‡
                for i in range(0, len(para), chunk_size):
                    chunks.append(para[i:i + chunk_size])
                continue
            
            # å¦‚æœåŠ ä¸Šå½“å‰æ®µè½ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰chunk
            if current_tokens + para_tokens > max_tokens:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_tokens = para_tokens
            else:
                current_chunk.append(para)
                current_tokens += para_tokens
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def _generate_report_for_archive(
        self,
        content: str,
        output_dir: Path,
        with_ocr: bool = False
    ) -> Optional[Dict]:
        """
        ä½¿ç”¨AIç”Ÿæˆç½‘é¡µå†…å®¹æŠ¥å‘Š
        æ”¯æŒé•¿æ–‡æœ¬åˆ†æ®µå¤„ç†
        """
        import os
        try:
            from groq import Groq
        except ImportError:
            print("  âš ï¸  Groq SDK æœªå®‰è£…ï¼Œè·³è¿‡AIæŠ¥å‘Šç”Ÿæˆ")
            return None
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            print("  âš ï¸  GROQ_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡AIæŠ¥å‘Šç”Ÿæˆ")
            return None
        
        try:
            client = Groq(api_key=api_key)
            model = os.getenv("GROQ_LLM_MODEL", "openai/gpt-oss-120b")
            max_tokens = int(os.getenv("GROQ_MAX_TOKENS", "8192"))
            temperature = float(os.getenv("GROQ_TEMPERATURE", "0.7"))
            
            # ä¼°ç®— token æ•°é‡
            content_tokens = self._estimate_tokens(content)
            print(f"  ğŸ“Š å†…å®¹ä¼°ç®— tokens: {content_tokens:,}")
            
            # å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œå¯åŠ¨é•¿æ–‡æœ¬æ¨¡å¼
            if content_tokens > self.LONG_TEXT_THRESHOLD:
                print(f"  ğŸ”„ å¯åŠ¨é•¿æ–‡æœ¬åˆ†æ®µå¤„ç†æ¨¡å¼")
                return self._generate_report_long_text(
                    client, model, content, output_dir, 
                    max_tokens, temperature
                )
            
            # çŸ­æ–‡æœ¬æ¨¡å¼ï¼šç›´æ¥å¤„ç†
            prompt = f"""
è¯·å°†ä»¥ä¸‹ç½‘é¡µå†…å®¹æ•´ç†æˆä¸€ä»½**ç»“æ„åŒ– Markdown çŸ¥è¯†æ¡£æ¡ˆ**ã€‚

**âš ï¸ é‡è¦ï¼šè¯†åˆ«é”™è¯¯ä¿®æ­£**
- ç½‘é¡µå¯èƒ½å­˜åœ¨æ’ç‰ˆé—®é¢˜æˆ–OCRè¯†åˆ«é”™è¯¯
- è¯·ä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£åŒéŸ³å­—/è¯é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯ä¸“ä¸šæœ¯è¯­
- ä½¿ç”¨å‡†ç¡®ã€ä¸“ä¸šçš„æœ¯è¯­è¡¨è¾¾

ä½ éœ€è¦ï¼š
1. **ä½¿ç”¨ Markdown** è¾“å‡ºï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€å¼•ç”¨ã€è¡¨æ ¼ç­‰ï¼‰
2. æå–ä¸»è¦è§‚ç‚¹å’Œæ ¸å¿ƒå†…å®¹
3. è‡ªåŠ¨è¯†åˆ«"ä¸»é¢˜/ç« èŠ‚"å¹¶ç»“æ„åŒ–æ€»ç»“
4. æå–é‡è¦æ•°æ®ï¼šæ•°å­—ã€è§„åˆ™ã€å¼•ç”¨ã€æ—¥æœŸç­‰
5. ç”Ÿæˆæ ‡ç­¾å’Œæ‘˜è¦ï¼š
   - **æ ‡ç­¾ï¼ˆtagsï¼‰**ï¼š3-6ä¸ªé«˜åº¦æ¦‚æ‹¬çš„ä¸»é¢˜æ ‡ç­¾ï¼Œå¦‚"æŠ€æœ¯"ã€"æ•™è‚²"ã€"äººæ–‡"ç­‰
   - **æ‘˜è¦**ï¼šä¸è¶…è¿‡50ä¸ªå­—çš„ç³»ç»Ÿæ€§å†…å®¹æ¦‚æ‹¬

æ¨èç»“æ„ï¼š
## æ‘˜è¦
ï¼ˆä¸è¶…è¿‡50å­—çš„æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬ï¼‰

## ä¸»è¦å†…å®¹
## å…³é”®è§‚ç‚¹
## é‡è¦ä¿¡æ¯
## æ ‡ç­¾
æ ¼å¼ï¼šæ ‡ç­¾: æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3

ä»¥ä¸‹æ˜¯ç½‘é¡µå†…å®¹ï¼š
{content}
"""

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ•´ç†åŠ©æ‰‹ï¼Œå…·å¤‡æ™ºèƒ½çº é”™èƒ½åŠ›ã€‚
                        ä½ çš„ä»»åŠ¡æ˜¯ä»ç½‘é¡µå†…å®¹ä¸­æå–æ ¸å¿ƒä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„çŸ¥è¯†æ¡£æ¡ˆã€‚
                        ç¡®ä¿è¾“å‡ºä½¿ç”¨å‡†ç¡®ã€ä¸“ä¸šçš„æœ¯è¯­è¡¨è¾¾ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            report_content = response.choices[0].message.content
            
            # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            report_path = output_dir / 'report.md'
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            return {
                'content': report_content,
                'model': model,
                'tags': self._parse_tags_from_content(report_content),
                'topics': []  # TODO: ä»æŠ¥å‘Šä¸­è§£æä¸»é¢˜
            }
        except Exception as e:
            print(f"  âœ— AIæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_report_long_text(
        self,
        client,
        model: str,
        content: str,
        output_dir: Path,
        max_tokens: int,
        temperature: float
    ) -> Optional[Dict]:
        """
        é•¿æ–‡æœ¬åˆ†æ®µå¤„ç†æ¨¡å¼
        å°†å†…å®¹åˆ†æ®µï¼Œé€æ®µç”ŸæˆæŠ¥å‘Šï¼Œå¹¶å°†å‰ä¸€æ®µçš„æŠ¥å‘Šä½œä¸ºèƒŒæ™¯
        """
        # åˆ†å‰²å†…å®¹ï¼ˆæ¯æ®µçº¦ 80,000 tokensï¼Œä¿ç•™ä½™é‡ï¼‰
        chunks = self._split_content_by_tokens(content, 80000)
        print(f"  ğŸ“„ åˆ†å‰²ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
        
        previous_summary = ""
        all_reports = []
        
        for i, chunk in enumerate(chunks, 1):
            chunk_tokens = self._estimate_tokens(chunk)
            print(f"\n  ğŸ”¹ å¤„ç†ç‰‡æ®µ {i}/{len(chunks)} ({chunk_tokens:,} tokens)...")
            
            # æ„å»ºæç¤ºè¯
            if previous_summary:
                context_info = f"""
**å‰æ–‡èƒŒæ™¯æ€»ç»“ï¼š**
{previous_summary}

---

"""
            else:
                context_info = ""
            
            last_segment_instruction = ""
            if i == len(chunks):
                last_segment_instruction = "6. **è¿™æ˜¯æœ€åä¸€éƒ¨åˆ†**ï¼Œè¯·ç”Ÿæˆæœ€ç»ˆçš„æ ‡ç­¾å’Œæ‘˜è¦"
            
            prompt = f"""{context_info}è¯·å°†ä»¥ä¸‹ç½‘é¡µå†…å®¹ç‰‡æ®µï¼ˆç¬¬ {i}/{len(chunks)} éƒ¨åˆ†ï¼‰æ•´ç†æˆ**ç»“æ„åŒ– Markdown çŸ¥è¯†æ¡£æ¡ˆ**ã€‚

**âš ï¸ é‡è¦è¦æ±‚ï¼š**
1. è¯†åˆ«å¹¶ä¿®æ­£åŒéŸ³å­—/è¯é”™è¯¯ï¼Œä½¿ç”¨å‡†ç¡®ä¸“ä¸šçš„æœ¯è¯­
2. ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€å¼•ç”¨ã€è¡¨æ ¼ç­‰ï¼‰
3. æå–ä¸»è¦è§‚ç‚¹å’Œæ ¸å¿ƒå†…å®¹
4. è¯†åˆ«ä¸»é¢˜/ç« èŠ‚å¹¶ç»“æ„åŒ–æ€»ç»“
5. æå–é‡è¦æ•°æ®ï¼šæ•°å­—ã€è§„åˆ™ã€å¼•ç”¨ã€æ—¥æœŸç­‰
{last_segment_instruction}

æ¨èç»“æ„ï¼š
## ç‰‡æ®µ {i} - æ ¸å¿ƒå†…å®¹
ï¼ˆæœ¬ç‰‡æ®µçš„ä¸»è¦å†…å®¹ï¼‰

## å…³é”®è§‚ç‚¹
## é‡è¦ä¿¡æ¯
{"## æ ‡ç­¾\næ ¼å¼ï¼šæ ‡ç­¾: æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3\n\n## å…¨æ–‡æ‘˜è¦\nï¼ˆä¸è¶…è¿‡100å­—çš„æ•´ä½“æ¦‚æ‹¬ï¼‰" if i == len(chunks) else ""}

ä»¥ä¸‹æ˜¯å†…å®¹ç‰‡æ®µï¼š
{chunk}
"""

            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                            "role": "system",
                            "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ•´ç†åŠ©æ‰‹ï¼Œå…·å¤‡æ™ºèƒ½çº é”™èƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ•´åˆèƒ½åŠ›ã€‚
                            ä½ çš„ä»»åŠ¡æ˜¯å¤„ç†é•¿æ–‡æœ¬çš„ç‰‡æ®µï¼Œå¹¶ç»“åˆå‰æ–‡èƒŒæ™¯ç”Ÿæˆè¿è´¯çš„çŸ¥è¯†æ¡£æ¡ˆã€‚"""
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
                
                segment_report = response.choices[0].message.content
                all_reports.append(f"\n\n---\n\n{segment_report}")
                
                # æå–æœ¬æ®µçš„æ ¸å¿ƒå†…å®¹ä½œä¸ºä¸‹ä¸€æ®µçš„èƒŒæ™¯
                # ç®€å•æˆªå–å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦
                lines = segment_report.split('\n')
                summary_lines = []
                char_count = 0
                for line in lines:
                    if char_count > 500:
                        break
                    summary_lines.append(line)
                    char_count += len(line)
                previous_summary = '\n'.join(summary_lines)
                
                print(f"  âœ… ç‰‡æ®µ {i} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                print(f"  âœ— ç‰‡æ®µ {i} å¤„ç†å¤±è´¥: {e}")
                all_reports.append(f"\n\n---\n\n## ç‰‡æ®µ {i}\nï¼ˆå¤„ç†å¤±è´¥ï¼š{e}ï¼‰\n\n")
        
        # åˆå¹¶æ‰€æœ‰æŠ¥å‘Š
        final_report = f"""# é•¿æ–‡æœ¬çŸ¥è¯†æ¡£æ¡ˆ

> æœ¬æ–‡æ¡£ç”± {len(chunks)} ä¸ªç‰‡æ®µåˆ†æ®µå¤„ç†ç”Ÿæˆ
> æ€»è®¡çº¦ {self._estimate_tokens(content):,} tokens

{''.join(all_reports)}
"""
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_path = output_dir / 'report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(final_report)
        
        print(f"\n  âœ… é•¿æ–‡æœ¬æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        return {
            'content': final_report,
            'model': model,
            'tags': self._parse_tags_from_content(final_report),
            'topics': [],
            'segments': len(chunks)
        }
    
    def _read_archived_content(self, output_path: str) -> str:
        """è¯»å–å½’æ¡£çš„åŸå§‹å†…å®¹ï¼ˆä»archive_raw.mdï¼‰"""
        if not output_path:
            return ""
        
        try:
            output_path_obj = Path(output_path)
            
            # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾ archive_raw.md
            if output_path_obj.is_dir():
                # å…ˆæ£€æŸ¥å½“å‰ç›®å½•
                archive_raw_path = output_path_obj / "archive_raw.md"
                if archive_raw_path.exists():
                    with open(archive_raw_path, 'r', encoding='utf-8') as f:
                        return f.read()
                
                # æŸ¥æ‰¾å­ç›®å½•ä¸­çš„ archive_raw.md
                for archive_raw in output_path_obj.rglob("archive_raw.md"):
                    try:
                        with open(archive_raw, 'r', encoding='utf-8') as f:
                            return f.read()
                    except Exception:
                        continue
                
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šå¦‚æœæ‰¾ä¸åˆ° archive_raw.mdï¼Œå°è¯•è¯»å– README.md
                readme_path = output_path_obj / "README.md"
                if readme_path.exists():
                    print(f"  âš ï¸  æœªæ‰¾åˆ° archive_raw.mdï¼Œä½¿ç”¨ README.md")
                    with open(readme_path, 'r', encoding='utf-8') as f:
                        return f.read()
                
                print(f"  âš ï¸  æœªæ‰¾åˆ° archive_raw.md æˆ– README.md åœ¨: {output_path}")
            # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥è¯»å–
            elif output_path_obj.is_file():
                with open(output_path_obj, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                print(f"  âš ï¸  è·¯å¾„ä¸å­˜åœ¨: {output_path}")
        except Exception as e:
            print(f"  âš ï¸  è¯»å–å½’æ¡£å†…å®¹å¤±è´¥: {e}")
        
        return ""
    
    def _save_to_archived_folder(
        self, 
        output_dir: Path,
        url: str,
        title: str,
        platform: str
    ) -> Optional[str]:
        """
        å°†å½’æ¡£çš„markdownä¿å­˜åˆ° archived/ æ–‡ä»¶å¤¹
        ä½¿ç”¨ä¸archiverç›¸åŒçš„å‘½åè§„åˆ™
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆåŒ…å«archive_raw.mdçš„æ–‡ä»¶å¤¹ï¼‰
            url: åŸå§‹URL
            title: ç½‘é¡µæ ‡é¢˜
            platform: å¹³å°åç§°
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # ç¡®å®šarchivedç›®å½•
            project_root = Path(__file__).parent.parent
            archived_dir = project_root / "archived"
            archived_dir.mkdir(exist_ok=True)
            
            # æŸ¥æ‰¾archive_raw.md
            archive_raw_path = None
            if output_dir.is_dir():
                # å…ˆæ£€æŸ¥å½“å‰ç›®å½•
                temp_path = output_dir / "archive_raw.md"
                if temp_path.exists():
                    archive_raw_path = temp_path
                else:
                    # æŸ¥æ‰¾å­ç›®å½•
                    for item in output_dir.rglob("archive_raw.md"):
                        archive_raw_path = item
                        break
            
            if not archive_raw_path or not archive_raw_path.exists():
                print(f"  âš ï¸  æœªæ‰¾åˆ° archive_raw.mdï¼Œè·³è¿‡ä¿å­˜åˆ° archived/")
                return None
            
            # è¯»å–å†…å®¹
            with open(archive_raw_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä¸archiverä¿æŒä¸€è‡´ï¼‰
            # æ ¼å¼: platform_hash_timestamp.md
            url_hash = hashlib.md5(url.encode()).hexdigest()[:4]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # å¹³å°åç§°æ˜ å°„
            platform_map = {
                'twitter': 'twitter',
                'x.com': 'twitter',
                'zhihu': 'zhihu',
                'xiaohongshu': 'xhs',
                'bilibili': 'bilibili',
                'reddit': 'reddit'
            }
            platform_short = platform_map.get(platform.lower(), platform[:10].lower())
            
            filename = f"{platform_short}_{url_hash}_{timestamp}.md"
            archived_path = archived_dir / filename
            
            # æ·»åŠ å…ƒä¿¡æ¯å¤´éƒ¨
            archived_content = f"""---
title: {title}
url: {url}
platform: {platform}
archived_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
---

{content}
"""
            
            # ä¿å­˜æ–‡ä»¶
            with open(archived_path, 'w', encoding='utf-8') as f:
                f.write(archived_content)
            
            print(f"âœ… ä¿å­˜åˆ° archived/: {filename}")
            return str(archived_path)
            
        except Exception as e:
            print(f"  âš ï¸  ä¿å­˜åˆ° archived/ å¤±è´¥: {e}")
            return None
    
    def _extract_plain_text(self, data: Dict) -> str:
        """ä»ç»“æ„åŒ–æ•°æ®æå–çº¯æ–‡æœ¬"""
        if isinstance(data, dict):
            if 'combined_text' in data:
                return data['combined_text']
            elif 'text' in data:
                return data['text']
            elif 'content' in data:
                return data['content']
            return json.dumps(data, ensure_ascii=False)
        return str(data)
    
    def _extract_tags(self, report_data: Dict) -> list:
        """ä»æŠ¥å‘Šä¸­æå–æ ‡ç­¾"""
        return report_data.get('tags', [])
    
    def _extract_topics(self, report_data: Dict) -> list:
        """ä»æŠ¥å‘Šä¸­æå–ä¸»é¢˜"""
        topics_data = report_data.get('topics', [])
        topics = []
        
        for i, topic_data in enumerate(topics_data):
            topic = Topic(
                video_id=0,  # ç¨åå¡«å……
                title=topic_data.get('title', ''),
                summary=topic_data.get('summary'),
                start_time=None,  # ç½‘é¡µå†…å®¹æ²¡æœ‰æ—¶é—´è½´
                end_time=None,
                keywords=topic_data.get('keywords', []),
                key_points=topic_data.get('key_points', []),
                sequence=i
            )
            topics.append(topic)
        
        return topics
    
    def _parse_tags_from_content(self, content: str) -> list:
        """ä»æŠ¥å‘Šå†…å®¹ä¸­è§£ææ ‡ç­¾"""
        import re
        # æŸ¥æ‰¾ "æ ‡ç­¾: xxx, xxx" æ ¼å¼
        tag_match = re.search(r'æ ‡ç­¾[ï¼š:]\s*(.+)', content)
        if tag_match:
            tags_str = tag_match.group(1)
            tags = [tag.strip() for tag in re.split(r'[,ï¼Œ]', tags_str)]
            return [tag for tag in tags if tag and len(tag) < 20]
        return []


async def archive_and_save(
    url: str,
    output_dir: str = "output",
    with_ocr: bool = False,
    headless: bool = True
) -> int:
    """
    å®Œæ•´çš„å½’æ¡£æµç¨‹ï¼šå½’æ¡£ç½‘é¡µ â†’ ç”ŸæˆæŠ¥å‘Š â†’ å­˜å…¥æ•°æ®åº“
    
    Args:
        url: ç½‘é¡µURL
        output_dir: è¾“å‡ºç›®å½•
        with_ocr: æ˜¯å¦è¿›è¡ŒOCRè¯†åˆ«
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    
    Returns:
        int: æ•°æ®åº“è®°å½•ID
    """
    from archiver import UniversalArchiver
    
    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    output_path = Path(output_dir) / f"archive_{url_hash}_{timestamp}"
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_path}")
    
    # 2. æ‰§è¡Œç½‘é¡µå½’æ¡£
    print(f"\nğŸŒ å¼€å§‹å½’æ¡£: {url}")
    archiver = UniversalArchiver(
        output_dir=str(output_path),
        headless=headless,
        verbose=True
    )
    
    # archive-run å’Œ archive-ocr éƒ½ä¼šç”Ÿæˆ report
    archive_result = await archiver.archive(url, generate_report=True)
    
    if not archive_result.get('success'):
        raise Exception(f"å½’æ¡£å¤±è´¥: {archive_result.get('error')}")
    
    print(f"âœ… å½’æ¡£å®Œæˆ: {archive_result['output_path']}")
    
    # 3. ä½¿ç”¨ LLM é‡å‘½åå¤–å±‚æ–‡ä»¶å¤¹
    print(f"\nğŸ¤– ç”Ÿæˆè¯­ä¹‰åŒ–æ–‡ä»¶å¤¹å...")
    new_folder_name = _generate_folder_name_with_llm_for_archive(
        archive_result=archive_result,
        original_folder=output_path
    )
    
    if new_folder_name and new_folder_name != output_path.name:
        new_output_path = Path(output_dir) / f"{new_folder_name}_{timestamp}"
        try:
            # å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œæ·»åŠ åç¼€
            counter = 1
            temp_path = new_output_path
            while temp_path.exists():
                temp_path = Path(output_dir) / f"{new_folder_name}_{timestamp}_{counter}"
                counter += 1
            new_output_path = temp_path
            
            output_path.rename(new_output_path)
            output_path = new_output_path
            print(f"âœ… æ–‡ä»¶å¤¹å·²é‡å‘½å: {output_path.name}")
        except Exception as e:
            print(f"âš ï¸  æ–‡ä»¶å¤¹é‡å‘½åå¤±è´¥: {e}")
    
    # 4. ä¿å­˜åˆ°æ•°æ®åº“
    print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
    processor = ArchiveProcessor()
    db_id = processor.process_and_save(
        url=url,
        output_dir=output_path,
        archive_result=archive_result,
        with_ocr=with_ocr,
        processing_config={
            'archive_mode': 'web',
            'with_ocr': with_ocr,
            'headless': headless
        }
    )
    
    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"   ğŸ“Š æ•°æ®åº“ID: {db_id}")
    print(f"   ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    print(f"   ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {output_path}/report.md")
    print(f"{'='*60}")
    
    return db_id


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç½‘é¡µå½’æ¡£ä¸æ•°æ®åº“é›†æˆ')
    parser.add_argument('url', help='ç½‘é¡µURL')
    parser.add_argument('--output-dir', default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--with-ocr', action='store_true', help='å¯ç”¨OCRè¯†åˆ«')
    parser.add_argument('--visible', action='store_true', help='æ˜¾ç¤ºæµè§ˆå™¨ï¼ˆè°ƒè¯•ï¼‰')
    
    args = parser.parse_args()
    
    # è¿è¡Œå¼‚æ­¥å½’æ¡£
    db_id = asyncio.run(archive_and_save(
        url=args.url,
        output_dir=args.output_dir,
        with_ocr=args.with_ocr,
        headless=not args.visible
    ))
    
    print(f"\nğŸ‰ å½’æ¡£æˆåŠŸï¼æ•°æ®åº“ID: {db_id}")


if __name__ == '__main__':
    main()
