#!/usr/bin/env python3
"""
ç»Ÿä¸€å½’æ¡£å‘½ä»¤è¡Œå·¥å…· - è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¼•æ“
"""

import sys
from pathlib import Path
from archiver.utils.url_parser import extract_url_from_text, detect_platform


def should_use_drissionpage(platform: str) -> bool:
    """
    æ ¹æ®å¹³å°å’Œé…ç½®å†³å®šä½¿ç”¨å“ªä¸ªå¼•æ“
    
    è§„åˆ™ï¼š
    1. å°çº¢ä¹¦ â†’ å¼ºåˆ¶ DrissionPageï¼ˆJSæ¸²æŸ“ + åçˆ¬ä¸¥æ ¼ï¼‰
    2. çŸ¥ä¹ã€Bç«™ â†’ ä¼˜å…ˆ DrissionPageï¼ˆå¦‚æœæœ‰ç™»å½•æ€æˆ–æ‰‹åŠ¨cookieï¼‰
    3. å…¶ä»–å¹³å° â†’ Crawl4AIï¼ˆå¿«é€Ÿï¼‰
    """
    # å°çº¢ä¹¦å¼ºåˆ¶ä½¿ç”¨ DrissionPage
    if platform == 'xiaohongshu':
        return True
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹åŠ¨é…ç½®çš„ Cookie
    cookie_file = Path(f"archiver/config/{platform}_drission_cookie.txt")
    if cookie_file.exists() and cookie_file.stat().st_size > 0:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ browser_dataï¼ˆç™»å½•æ€ï¼‰
    browser_data = Path('browser_data/Default/Cookies')
    if browser_data.exists() and browser_data.stat().st_size > 1000:
        # å¦‚æœæ˜¯éœ€è¦ç™»å½•çš„å¹³å°ä¸”æœ‰ç™»å½•æ€ï¼Œä½¿ç”¨ DrissionPage
        if platform in ['zhihu', 'bilibili']:
            return True
    
    # é»˜è®¤ä½¿ç”¨ Crawl4AIï¼ˆæ›´å¿«ï¼‰
    return False


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("âŒ é”™è¯¯: è¯·æä¾›URLå‚æ•°")
        print("ç”¨æ³•: python unified_archive_cli.py <URL>")
        print("\nğŸ’¡ æ”¯æŒåˆ†äº«æ–‡æœ¬æ ¼å¼ï¼Œä¼šè‡ªåŠ¨æå–URL")
        sys.exit(1)
    
    input_text = sys.argv[1]
    
    # è§£ææ¨¡å¼å‚æ•°
    mode = "default"
    for arg in sys.argv:
        if arg.startswith("--mode="):
            mode = arg.split("=", 1)[1]
    
    # æå– URL
    url = extract_url_from_text(input_text)
    if not url:
        print(f"âŒ é”™è¯¯ï¼šæ— æ³•ä»è¾“å…¥ä¸­æå–æœ‰æ•ˆçš„URL")
        print(f"   è¾“å…¥å†…å®¹: {input_text}")
        sys.exit(1)
    
    # å¦‚æœæå–çš„URLä¸è¾“å…¥ä¸åŒï¼Œæç¤ºç”¨æˆ·
    if url != input_text:
        print(f"ğŸ“ ä»åˆ†äº«æ–‡æœ¬ä¸­æå–URL: {url}\n")
    
    # æ£€æµ‹å¹³å°
    platform = detect_platform(url)
    print(f"ğŸ” æ£€æµ‹å¹³å°: {platform}")
    
    # å†³å®šä½¿ç”¨å“ªä¸ªå¼•æ“
    use_drission = should_use_drissionpage(platform)
    engine = "DrissionPage" if use_drission else "Crawl4AI"
    print(f"âš™ï¸  é€‰æ‹©å¼•æ“: {engine}")
    print()
    
    # æ‰§è¡Œå½’æ¡£
    if use_drission:
        # ä½¿ç”¨ DrissionPageï¼ˆçœŸå®æµè§ˆå™¨ï¼‰
        from archiver.core.drission_crawler import DrissionArchiver
        
        with DrissionArchiver(output_dir='archived', headless=True, verbose=True) as archiver:
            result = archiver.archive(url, mode=mode)
            
            if result['success']:
                print(f"\nâœ“ å½’æ¡£æˆåŠŸ: {result['output_path']}")
                print(f"  å¹³å°: {result.get('platform', 'unknown')}")
                print(f"  æ ‡é¢˜: {result.get('title', 'N/A')}")
                print(f"  å›¾ç‰‡: {result.get('images_downloaded', 0)}/{result.get('images_total', 0)}")
                print(f"  å†…å®¹: {result['content_length']} å­—ç¬¦")
            else:
                print(f"\nâœ— å½’æ¡£å¤±è´¥: {result.get('error', 'Unknown error')}")
                sys.exit(1)
    else:
        # ä½¿ç”¨ Crawl4AIï¼ˆå¼‚æ­¥ï¼‰
        import asyncio
        from archiver import UniversalArchiver
        
        async def archive_with_crawl4ai():
            archiver = UniversalArchiver(output_dir='archived', verbose=True)
            result = await archiver.archive(url, mode=mode)
            
            if result['success']:
                print(f"\nâœ“ å½’æ¡£æˆåŠŸ: {result['output_path']}")
                print(f"  å¹³å°: {result['platform']}")
                print(f"  æ ‡é¢˜: {result['title']}")
                print(f"  å†…å®¹: {result['content_length']} å­—ç¬¦")
            else:
                print(f"\nâœ— å½’æ¡£å¤±è´¥: {result.get('error', 'Unknown error')}")
                sys.exit(1)
        
        asyncio.run(archive_with_crawl4ai())


if __name__ == '__main__':
    main()
