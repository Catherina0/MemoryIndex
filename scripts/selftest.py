#!/usr/bin/env python3
"""
å…¨åŠŸèƒ½è‡ªæ£€å’Œæµ‹è¯•è„šæœ¬
æ£€æŸ¥ç³»ç»Ÿæ‰€æœ‰ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import subprocess
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{'â”€' * 40}")
    print(f"  {title}")
    print(f"{'â”€' * 40}")


def check_module_imports():
    """1. æ ¸å¿ƒæ¨¡å—å¯¼å…¥æµ‹è¯•"""
    print_header("ğŸ“¦ 1. æ ¸å¿ƒæ¨¡å—å¯¼å…¥æµ‹è¯•")
    
    modules = [
        ('db.models', ['SourceType', 'ProcessingStatus', 'ArtifactType', 'SearchResult']),
        ('db.schema', ['get_connection', 'init_database']),
        ('db.repository', ['VideoRepository']),
        ('db.search', ['SearchRepository']),
        ('db.whoosh_search', ['WhooshSearchIndex', 'check_whoosh_status']),
        ('core.video_downloader', ['VideoDownloader']),
        ('core.process_video', ['process_video']),
        ('archiver', ['UniversalArchiver', 'detect_platform']),
        ('archiver.utils.cookie_manager', ['CookieManager', 'get_xiaohongshu_cookies']),
        ('archiver.platforms', ['ZhihuAdapter', 'XiaohongshuAdapter', 'BilibiliAdapter']),
    ]
    
    errors = []
    for mod_name, items in modules:
        try:
            mod = __import__(mod_name, fromlist=items)
            for item in items:
                getattr(mod, item)
            print(f"   âœ… {mod_name}")
        except Exception as e:
            print(f"   âŒ {mod_name}: {e}")
            errors.append(mod_name)
    
    return errors


def check_dependencies():
    """2. ä¾èµ–åº“æ£€æŸ¥"""
    print_header("ğŸ”§ 2. ä¾èµ–åº“æ£€æŸ¥")
    
    errors = []
    
    # å¿…éœ€ä¾èµ–
    required = ['groq', 'paddleocr', 'tabulate']
    for dep in required:
        try:
            __import__(dep)
            print(f"   âœ… {dep}")
        except ImportError:
            print(f"   âŒ {dep} æœªå®‰è£…")
            errors.append(dep)
    
    # dotenv ç‰¹æ®Šå¤„ç†
    try:
        import dotenv
        print("   âœ… python-dotenv")
    except ImportError:
        print("   âŒ python-dotenv æœªå®‰è£…")
        errors.append('python-dotenv')
    
    # å¯é€‰ä¾èµ– - æœç´¢
    try:
        import whoosh
        print("   âœ… whoosh")
    except ImportError:
        print("   âš ï¸  whoosh æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œç”¨äºä¸­æ–‡æœç´¢ï¼‰")
    
    try:
        import jieba
        print("   âœ… jieba")
    except ImportError:
        print("   âš ï¸  jieba æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œç”¨äºä¸­æ–‡åˆ†è¯ï¼‰")
    
    # å¯é€‰ä¾èµ– - ç½‘é¡µå½’æ¡£
    try:
        import crawl4ai
        print("   âœ… crawl4aiï¼ˆå½’æ¡£ï¼‰")
    except ImportError:
        print("   âš ï¸  crawl4ai æœªå®‰è£…ï¼ˆç½‘é¡µå½’æ¡£éœ€è¦ï¼‰")
    
    try:
        import playwright
        print("   âœ… playwrightï¼ˆå½’æ¡£ï¼‰")
    except ImportError:
        print("   âš ï¸  playwright æœªå®‰è£…ï¼ˆç½‘é¡µå½’æ¡£éœ€è¦ï¼‰")
    
    try:
        import bs4
        print("   âœ… beautifulsoup4ï¼ˆå½’æ¡£ï¼‰")
    except ImportError:
        print("   âš ï¸  beautifulsoup4 æœªå®‰è£…ï¼ˆç½‘é¡µå½’æ¡£éœ€è¦ï¼‰")
    
    try:
        import html2text
        print("   âœ… html2textï¼ˆå½’æ¡£ï¼‰")
    except ImportError:
        print("   âš ï¸  html2text æœªå®‰è£…ï¼ˆç½‘é¡µå½’æ¡£éœ€è¦ï¼‰")
    
    try:
        import browser_cookie3
        print("   âœ… browser-cookie3ï¼ˆCookieç®¡ç†ï¼‰")
    except ImportError:
        print("   âš ï¸  browser-cookie3 æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œç”¨äºæµè§ˆå™¨Cookieï¼‰")
    
    return errors


def check_database():
    """3. æ•°æ®åº“çŠ¶æ€"""
    print_header("ğŸ—„ï¸  3. æ•°æ®åº“çŠ¶æ€")
    
    errors = []
    try:
        from db.schema import check_database_health
        stats = check_database_health()
        
        print(f"   ğŸ“Š è§†é¢‘æ•°: {stats.get('videos', 0)}")
        print(f"   ğŸ“Š äº§ç‰©æ•°: {stats.get('artifacts', 0)}")
        print(f"   ğŸ“Š æ ‡ç­¾æ•°: {stats.get('tags', 0)}")
        print(f"   ğŸ“Š FTSç´¢å¼•: {stats.get('fts_content', 0)} æ¡")
        print(f"   ğŸ’¾ æ•°æ®åº“å¤§å°: {stats.get('db_size_mb', 0):.2f} MB")
    except Exception as e:
        print(f"   âŒ æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {e}")
        errors.append('database')
    
    return errors


def check_whoosh():
    """4. Whoosh æœç´¢å¼•æ“"""
    print_header("ğŸ” 4. Whoosh æœç´¢å¼•æ“")
    
    try:
        from db.whoosh_search import check_whoosh_status, get_whoosh_index
        status = check_whoosh_status()
        
        print(f"   Whoosh å®‰è£…: {'âœ…' if status['whoosh_installed'] else 'âŒ'}")
        print(f"   jieba å®‰è£…: {'âœ…' if status['jieba_installed'] else 'âŒ'}")
        
        if status['ready']:
            idx = get_whoosh_index()
            st = idx.get_stats()
            print(f"   ç´¢å¼•æ–‡æ¡£æ•°: {st.get('doc_count', 0)}")
            print(f"   ç´¢å¼•ç›®å½•: {st.get('index_dir', 'N/A')}")
    except Exception as e:
        print(f"   âš ï¸  Whoosh æ£€æŸ¥è·³è¿‡: {e}")
    
    return []


def check_search():
    """5. æœç´¢åŠŸèƒ½æµ‹è¯•"""
    print_header("ğŸ” 5. æœç´¢åŠŸèƒ½æµ‹è¯•")
    
    errors = []
    try:
        from db.search import SearchRepository
        repo = SearchRepository()
        
        # æµ‹è¯•è‹±æ–‡æœç´¢ (FTS)
        r1 = repo.search('test', limit=1)
        print(f"   âœ… FTSæœç´¢æ­£å¸¸ (è‹±æ–‡) - æ‰¾åˆ° {len(r1)} æ¡")
        
        # æµ‹è¯•ä¸­æ–‡æœç´¢ (Whoosh)
        r2 = repo.search('æµ‹è¯•', limit=1)
        print(f"   âœ… Whooshæœç´¢æ­£å¸¸ (ä¸­æ–‡) - æ‰¾åˆ° {len(r2)} æ¡")
        
    except Exception as e:
        print(f"   âŒ æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        errors.append('search')
    
    return errors


def check_downloader():
    """6. ä¸‹è½½å™¨çŠ¶æ€"""
    print_header("â¬‡ï¸  6. ä¸‹è½½å™¨çŠ¶æ€")
    
    try:
        from core.video_downloader import VideoDownloader
        dl = VideoDownloader()
        
        print(f"   yt-dlp: {'âœ… ' + dl.ytdlp_path if dl.ytdlp_path else 'âŒ æœªå®‰è£…'}")
        print(f"   BBDown: {'âœ… ' + dl.bbdown_path if dl.bbdown_path else 'âš ï¸  æœªå®‰è£…ï¼ˆBç«™å¤‡ç”¨ï¼‰'}")
    except Exception as e:
        print(f"   âŒ ä¸‹è½½å™¨æ£€æŸ¥å¤±è´¥: {e}")
    
    return []


def check_ffmpeg():
    """7. FFmpeg æ£€æŸ¥"""
    print_header("ğŸ¬ 7. FFmpeg æ£€æŸ¥")
    
    errors = []
    try:
        result = subprocess.run(
            ['ffmpeg', '-version'], 
            capture_output=True, 
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            version = result.stdout.split('\n')[0]
            print(f"   âœ… {version[:60]}...")
        else:
            print("   âŒ ffmpeg è¿è¡Œå¼‚å¸¸")
            errors.append('ffmpeg')
    except FileNotFoundError:
        print("   âŒ ffmpeg æœªå®‰è£…")
        print("      å®‰è£…æ–¹æ³•: brew install ffmpeg")
        errors.append('ffmpeg')
    except Exception as e:
        print(f"   âŒ ffmpeg æ£€æŸ¥å¤±è´¥: {e}")
        errors.append('ffmpeg')
    
    return errors


def check_api_config():
    """8. API é…ç½®"""
    print_header("ğŸ”‘ 8. API é…ç½®æ£€æŸ¥")
    
    env_file = Path('.env')
    
    if env_file.exists():
        content = env_file.read_text()
        
        # æ£€æŸ¥ GROQ_API_KEY
        if 'GROQ_API_KEY' in content:
            # æ’é™¤å ä½ç¬¦
            lines = [l for l in content.split('\n') if 'GROQ_API_KEY' in l and not l.strip().startswith('#')]
            if lines:
                value = lines[0].split('=', 1)[1].strip() if '=' in lines[0] else ''
                if value and 'your' not in value.lower() and value != '':
                    print("   âœ… GROQ_API_KEY å·²é…ç½®")
                else:
                    print("   âš ï¸  GROQ_API_KEY æœªè®¾ç½®æˆ–ä¸ºå ä½ç¬¦")
            else:
                print("   âš ï¸  GROQ_API_KEY è¢«æ³¨é‡Š")
        else:
            print("   âš ï¸  GROQ_API_KEY æœªé…ç½®")
    else:
        print("   âŒ .env æ–‡ä»¶ä¸å­˜åœ¨")
        print("      è¯·å¤åˆ¶ config_example.py åˆ›å»º .env æ–‡ä»¶")
    
    return []


def check_disk_space():
    """9. ç£ç›˜ç©ºé—´æ£€æŸ¥"""
    print_header("ğŸ’¾ 9. ç£ç›˜ç©ºé—´æ£€æŸ¥")
    
    import shutil
    
    # æ£€æŸ¥å½“å‰ç›®å½•ç£ç›˜ç©ºé—´
    total, used, free = shutil.disk_usage('.')
    free_gb = free / (1024 ** 3)
    
    if free_gb < 1:
        print(f"   âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³: {free_gb:.1f} GB å¯ç”¨")
    elif free_gb < 5:
        print(f"   âš ï¸  ç£ç›˜ç©ºé—´è¾ƒä½: {free_gb:.1f} GB å¯ç”¨")
    else:
        print(f"   âœ… ç£ç›˜ç©ºé—´å……è¶³: {free_gb:.1f} GB å¯ç”¨")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•å¤§å°
    output_dir = Path('output')
    if output_dir.exists():
        total_size = sum(f.stat().st_size for f in output_dir.rglob('*') if f.is_file())
        print(f"   ğŸ“ output/ ç›®å½•: {total_size / (1024**2):.1f} MB")
    
    videos_dir = Path('videos')
    if videos_dir.exists():
        total_size = sum(f.stat().st_size for f in videos_dir.rglob('*') if f.is_file())
        print(f"   ğŸ“ videos/ ç›®å½•: {total_size / (1024**2):.1f} MB")
    
    return []


def check_archiver():
    """10. ç½‘é¡µå½’æ¡£åŠŸèƒ½"""
    print_header("ğŸŒ 10. ç½‘é¡µå½’æ¡£åŠŸèƒ½")
    
    errors = []
    try:
        # å¯¼å…¥æµ‹è¯•
        from archiver import UniversalArchiver, detect_platform
        from archiver.utils.url_parser import normalize_url, is_valid_url
        from archiver.platforms import (
            ZhihuAdapter, XiaohongshuAdapter, BilibiliAdapter,
            RedditAdapter, WordPressAdapter
        )
        
        print("   âœ… å½’æ¡£æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # å¹³å°æ£€æµ‹æµ‹è¯•
        test_cases = [
            ("https://www.zhihu.com/question/123", "zhihu"),
            ("https://www.xiaohongshu.com/explore/abc", "xiaohongshu"),
            ("https://www.bilibili.com/video/BV123", "bilibili"),
            ("https://www.reddit.com/r/python/", "reddit"),
        ]
        
        platform_ok = True
        for url, expected in test_cases:
            result = detect_platform(url)
            if result == expected:
                print(f"   âœ… å¹³å°æ£€æµ‹: {expected}")
            else:
                print(f"   âŒ å¹³å°æ£€æµ‹å¤±è´¥: {url} â†’ {result} (åº”ä¸º {expected})")
                platform_ok = False
        
        if not platform_ok:
            errors.append('archiver-platform-detection')
        
        # é€‚é…å™¨æµ‹è¯•
        adapters = [
            (ZhihuAdapter(), "zhihu"),
            (XiaohongshuAdapter(), "xiaohongshu"),
            (BilibiliAdapter(), "bilibili"),
            (RedditAdapter(), "reddit"),
            (WordPressAdapter(), "wordpress"),
        ]
        
        for adapter, name in adapters:
            if adapter.name == name and adapter.content_selector:
                print(f"   âœ… {name} é€‚é…å™¨é…ç½®æ­£å¸¸")
            else:
                print(f"   âŒ {name} é€‚é…å™¨é…ç½®å¼‚å¸¸")
                errors.append(f'archiver-{name}')
        
        # URLå·¥å…·æµ‹è¯•
        assert normalize_url("example.com") == "https://example.com"
        assert is_valid_url("https://example.com") == True
        print("   âœ… URLå·¥å…·å‡½æ•°æ­£å¸¸")
        
        # æ£€æŸ¥å½’æ¡£è¾“å‡ºç›®å½•
        archived_dir = Path('archived')
        if archived_dir.exists():
            count = len(list(archived_dir.glob('*.md')))
            print(f"   ğŸ“ å·²å½’æ¡£æ–‡ä»¶: {count} ä¸ª")
        
    except Exception as e:
        print(f"   âŒ å½’æ¡£åŠŸèƒ½æ£€æŸ¥å¤±è´¥: {e}")
        errors.append('archiver')
    
    return errors


def check_cookie_management():
    """11. Cookieç»Ÿä¸€ç®¡ç†"""
    print_header("ğŸª 11. Cookieç»Ÿä¸€ç®¡ç†")
    
    try:
        from archiver.utils.cookie_manager import (
            CookieManager, 
            get_xiaohongshu_cookies
        )
        
        print("   âœ… Cookieç®¡ç†å™¨å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºç®¡ç†å™¨
        manager = CookieManager()
        print("   âœ… CookieManager åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥XHSé…ç½®
        config_path = Path("XHS-Downloader") / "Volume" / "settings.json"
        if config_path.exists():
            import json
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            has_cookie = bool(config.get('cookie'))
            has_ua = bool(config.get('user_agent'))
            
            print(f"   {'âœ…' if has_cookie else 'âš ï¸ '} å°çº¢ä¹¦Cookie: {'å·²é…ç½®' if has_cookie else 'æœªé…ç½®'}")
            print(f"   {'âœ…' if has_ua else 'âš ï¸ '} User-Agent: {'å·²é…ç½®' if has_ua else 'æœªé…ç½®'}")
            
            # æµ‹è¯•CookieåŠ è½½
            cookies = get_xiaohongshu_cookies()
            if cookies:
                print(f"   âœ… CookieåŠ è½½æˆåŠŸ ({len(cookies)} ä¸ªå­—æ®µ)")
                
                # æ£€æŸ¥å…³é”®å­—æ®µ
                if 'web_session' in cookies:
                    print("   âœ… åŒ…å« web_session å­—æ®µ")
                else:
                    print("   âš ï¸  ç¼ºå°‘ web_session å­—æ®µ")
            else:
                print("   âš ï¸  CookieåŠ è½½å¤±è´¥")
        else:
            print("   âš ï¸  XHS-Downloader é…ç½®ä¸å­˜åœ¨")
            print("      è¿è¡Œ make config-xhs-cookie é…ç½®")
        
        # æµ‹è¯•ä»XHSé…ç½®åŠ è½½
        xhs_cookies = manager.load_from_xhs_config()
        if xhs_cookies:
            print(f"   âœ… XHSé…ç½®åŠ è½½åŠŸèƒ½æ­£å¸¸")
        else:
            print(f"   âš ï¸  XHSé…ç½®æœªè®¾ç½®ï¼ˆå¯é€‰ï¼‰")
        
    except Exception as e:
        print(f"   âŒ Cookieç®¡ç†æ£€æŸ¥å¤±è´¥: {e}")
        return ['cookie-management']
    
    return []


def check_archiver_integration():
    """12. å½’æ¡£é›†æˆæµ‹è¯•"""
    print_header("ğŸ”— 12. å½’æ¡£é›†æˆæµ‹è¯•")
    
    try:
        # æµ‹è¯•è‡ªåŠ¨CookieåŠ è½½
        from archiver.utils.url_parser import detect_platform
        from archiver.utils.cookie_manager import get_xiaohongshu_cookies
        
        # å¹³å°æ£€æµ‹
        xhs_url = "https://www.xiaohongshu.com/explore/abc123"
        platform = detect_platform(xhs_url)
        print(f"   âœ… URLå¹³å°æ£€æµ‹: {platform}")
        
        # Cookieå¯ç”¨æ€§
        if platform == "xiaohongshu":
            cookies = get_xiaohongshu_cookies()
            if cookies:
                print(f"   âœ… å°çº¢ä¹¦Cookieè‡ªåŠ¨åŠ è½½å¯ç”¨")
            else:
                print(f"   âš ï¸  å°çº¢ä¹¦Cookieæœªé…ç½®ï¼ˆéœ€è¦æ—¶é…ç½®ï¼‰")
        
        # æµ‹è¯•Markdownè½¬æ¢å™¨
        from archiver.core.markdown_converter import MarkdownConverter
        converter = MarkdownConverter()
        
        test_html = "<p>æµ‹è¯•<strong>å†…å®¹</strong></p>"
        markdown = converter.convert(
            html=test_html,
            title="æµ‹è¯•",
            url="https://example.com",
            platform="test"
        )
        
        if "title: æµ‹è¯•" in markdown and "æµ‹è¯•" in markdown:
            print("   âœ… Markdownè½¬æ¢å™¨æ­£å¸¸")
        else:
            print("   âŒ Markdownè½¬æ¢å™¨å¼‚å¸¸")
            return ['markdown-converter']
        
        # æ£€æŸ¥æ–‡æ¡£
        docs = [
            "docs/ARCHIVER_GUIDE.md",
            "docs/ARCHIVER_QUICKREF.md",
            "docs/COOKIE_UNIFIED.md",
            "archiver/README.md",
        ]
        
        missing_docs = []
        for doc in docs:
            if Path(doc).exists():
                print(f"   âœ… {doc}")
            else:
                print(f"   âŒ {doc} ç¼ºå¤±")
                missing_docs.append(doc)
        
        if missing_docs:
            return ['archiver-docs']
        
    except Exception as e:
        print(f"   âŒ å½’æ¡£é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return ['archiver-integration']
    
    return []


def main():
    """ä¸»å‡½æ•°"""
    print("â”" * 50)
    print("ğŸ”¬ å…¨åŠŸèƒ½è‡ªæ£€å’Œæµ‹è¯•")
    print("â”" * 50)
    
    all_errors = []
    
    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
    all_errors.extend(check_module_imports())
    all_errors.extend(check_dependencies())
    all_errors.extend(check_database())
    all_errors.extend(check_whoosh())
    all_errors.extend(check_search())
    all_errors.extend(check_downloader())
    all_errors.extend(check_ffmpeg())
    all_errors.extend(check_api_config())
    all_errors.extend(check_disk_space())
    all_errors.extend(check_archiver())
    all_errors.extend(check_cookie_management())
    all_errors.extend(check_archiver_integration())
    
    # æ€»ç»“
    print("\n" + "â”" * 50)
    if all_errors:
        print(f"âš ï¸  å‘ç° {len(all_errors)} ä¸ªé—®é¢˜:")
        for err in all_errors:
            print(f"   â€¢ {err}")
        print("\nğŸ’¡ å»ºè®®:")
        
        if 'archiver' in all_errors or any('archiver' in e for e in all_errors):
            print("   â€¢ ç½‘é¡µå½’æ¡£åŠŸèƒ½é—®é¢˜:")
            print("     - å®‰è£…ä¾èµ–: make install")
            print("     - å®‰è£…æµè§ˆå™¨: playwright install chromium")
            print("     - è¿è¡Œæµ‹è¯•: make test-archiver")
        
        if 'cookie-management' in all_errors:
            print("   â€¢ Cookieç®¡ç†é—®é¢˜:")
            print("     - é…ç½®å°çº¢ä¹¦Cookie: make config-xhs-cookie")
            print("     - æµ‹è¯•Cookie: python scripts/test_cookie_unified.py")
        
        if 'ffmpeg' in all_errors:
            print("   â€¢ FFmpegé—®é¢˜:")
            print("     - å®‰è£…: brew install ffmpeg")
        
        if 'database' in all_errors:
            print("   â€¢ æ•°æ®åº“é—®é¢˜:")
            print("     - åˆå§‹åŒ–: make db-init")
        
        print("\nè¯·ä¿®å¤ä»¥ä¸Šé—®é¢˜åé‡æ–°è¿è¡Œ make selftest")
        print("â”" * 50)
        return 1
    else:
        print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        print("\nğŸ¯ åŠŸèƒ½çŠ¶æ€:")
        print("   âœ… è§†é¢‘å¤„ç†ç³»ç»Ÿ")
        print("   âœ… è§†é¢‘ä¸‹è½½ç³»ç»Ÿ")
        print("   âœ… æ•°æ®åº“ä¸æœç´¢")
        print("   âœ… ç½‘é¡µå½’æ¡£ç³»ç»Ÿ")
        print("   âœ… Cookieç»Ÿä¸€ç®¡ç†")
        print("\nğŸ’¡ å¿«é€Ÿå¼€å§‹:")
        print("   â€¢ å¤„ç†è§†é¢‘: make run VIDEO=video.mp4")
        print("   â€¢ ä¸‹è½½è§†é¢‘: make download-run URL=è§†é¢‘é“¾æ¥")
        print("   â€¢ å½’æ¡£ç½‘é¡µ: make archive URL=ç½‘é¡µé“¾æ¥")
        print("   â€¢ æœç´¢å†…å®¹: make search Q='å…³é”®è¯'")
        print("â”" * 50)
        return 0


if __name__ == '__main__':
    sys.exit(main())
