#!/usr/bin/env python3
"""
å…¨åŠŸèƒ½è‡ªæ£€å’Œæµ‹è¯•è„šæœ¬
æ£€æŸ¥ç³»ç»Ÿæ‰€æœ‰ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import subprocess
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{'â”€' * 40}")
    print(f"  {title}")
    print(f"{'â”€' * 40}")


def check_module_imports():
    """1. æ ¸å¿ƒæ¨¡å—å¯¼å…¥æµ‹è¯•"""
    print_header("ğŸ“¦ 1. æ ¸å¿ƒæ¨¡å—å¯¼å…¥æµ‹è¯•")
    
    modules = [
        ('db.models', ['SourceType', 'ProcessingStatus', 'ArtifactType', 'SearchResult']),
        ('db.schema', ['get_connection', 'init_database']),
        ('db.repository', ['VideoRepository']),
        ('db.search', ['SearchRepository']),
        ('db.whoosh_search', ['WhooshSearchIndex', 'check_whoosh_status']),
        ('core.video_downloader', ['VideoDownloader']),
        ('core.process_video', ['process_video']),
        ('archiver', ['UniversalArchiver', 'detect_platform']),
        ('archiver.utils.cookie_manager', ['CookieManager', 'get_xiaohongshu_cookies']),
        ('archiver.platforms', ['ZhihuAdapter', 'XiaohongshuAdapter', 'BilibiliAdapter']),
    ]
    
    errors = []
    for mod_name, items in modules:
        try:
            mod = __import__(mod_name, fromlist=items)
            for item in items:
                getattr(mod, item)
            print(f"   âœ… {mod_name}")
        except Exception as e:
            print(f"   âŒ {mod_name}: {e}")
            errors.append(mod_name)
    
    return errors


def check_dependencies():
    """2. ä¾èµ–åº“æ£€æŸ¥"""
    print_header("ğŸ”§ 2. ä¾èµ–åº“æ£€æŸ¥")
    
    errors = []
    
    # å¿…éœ€ä¾èµ–
    required = [
        ('numpy', 'æ•°æ®å¤„ç†'),
        ('groq', 'Groq API'),
        ('dotenv', 'ç¯å¢ƒå˜é‡'),
        ('tqdm', 'è¿›åº¦æ¡'),
        ('tabulate', 'è¡¨æ ¼è¾“å‡º'),
    ]
    for dep, desc in required:
        try:
            if dep == 'dotenv':
                __import__('dotenv')
            else:
                __import__(dep)
            print(f"   âœ… {dep} ({desc})")
        except ImportError:
            print(f"   âŒ {dep} ({desc}) æœªå®‰è£…")
            errors.append(dep)
    
    # Gemini APIï¼ˆå¯é€‰ï¼Œç”¨äºè¶…é•¿æ–‡æœ¬ï¼‰
    try:
        from google import genai
        print("   âœ… google-genaiï¼ˆå¯é€‰ï¼Œé•¿æ–‡æœ¬å¤„ç†ï¼‰")
    except ImportError:
        print("   âš ï¸  google-genai æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œç”¨äºå¤„ç†è¶…è¿‡ 13 ä¸‡ token çš„é•¿æ–‡æœ¬ï¼‰")
        print("      å®‰è£…: pip install google-genai")
    
    # è§†é¢‘ä¸‹è½½
    try:
        import yt_dlp
        print("   âœ… yt-dlpï¼ˆè§†é¢‘ä¸‹è½½ï¼‰")
    except ImportError:
        print("   âš ï¸  yt-dlp æœªå®‰è£…ï¼ˆè§†é¢‘ä¸‹è½½åŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
        print("      å®‰è£…: pip install yt-dlp")
    
    # å…¨æ–‡æœç´¢
    try:
        import whoosh
        print("   âœ… Whooshï¼ˆå…¨æ–‡æœç´¢å¼•æ“ï¼‰")
    except ImportError:
        print("   âš ï¸  Whoosh æœªå®‰è£…ï¼ˆæœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
        print("      å®‰è£…: pip install Whoosh")
    
    try:
        import jieba
        print("   âœ… jiebaï¼ˆä¸­æ–‡åˆ†è¯ï¼‰")
    except ImportError:
        print("   âš ï¸  jieba æœªå®‰è£…ï¼ˆä¸­æ–‡æœç´¢å°†å—å½±å“ï¼‰")
        print("      å®‰è£…: pip install jieba")
    
    # ç½‘é¡µå½’æ¡£
    try:
        import crawl4ai
        print("   âœ… crawl4aiï¼ˆç½‘é¡µçˆ¬è™«ï¼‰")
    except ImportError:
        print("   âš ï¸  crawl4ai æœªå®‰è£…ï¼ˆç½‘é¡µå½’æ¡£åŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
        print("      å®‰è£…: pip install crawl4ai")
    
    try:
        import playwright
        print("   âœ… playwrightï¼ˆæµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼‰")
    except ImportError:
        print("   âš ï¸  playwright æœªå®‰è£…ï¼ˆéƒ¨åˆ†ç½‘é¡µå½’æ¡£åŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
        print("      å®‰è£…: pip install playwright && playwright install")
    
    try:
        import bs4
        print("   âœ… beautifulsoup4ï¼ˆHTMLè§£æï¼‰")
    except ImportError:
        print("   âš ï¸  beautifulsoup4 æœªå®‰è£…ï¼ˆHTMLè§£æåŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
        print("      å®‰è£…: pip install beautifulsoup4")
    
    try:
        import html2text
        print("   âœ… html2textï¼ˆHTMLè½¬Markdownï¼‰")
    except ImportError:
        print("   âš ï¸  html2text æœªå®‰è£…ï¼ˆHTMLè½¬æ¢åŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
        print("      å®‰è£…: pip install html2text")
    
    # å°çº¢ä¹¦ç›¸å…³ï¼ˆå¯é€‰ï¼‰
    try:
        import httpx
        print("   âœ… httpxï¼ˆHTTPå®¢æˆ·ç«¯ï¼Œå°çº¢ä¹¦ä¸‹è½½éœ€è¦ï¼‰")
    except ImportError:
        print("   âš ï¸  httpx æœªå®‰è£…ï¼ˆå°çº¢ä¹¦ä¸‹è½½åŠŸèƒ½å°†ä¸å¯ç”¨ï¼‰")
    
    # OCR å¼•æ“ï¼ˆè‡³å°‘éœ€è¦ä¸€ä¸ªï¼‰
    ocr_available = False
    try:
        import paddleocr
        print("   âœ… paddleocrï¼ˆå¯é€‰ OCR å¼•æ“ï¼‰")
        ocr_available = True
    except ImportError:
        print("   âš ï¸  paddleocr æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œè·¨å¹³å° OCRï¼‰")
    
    # Vision OCR (macOS ç³»ç»Ÿè‡ªå¸¦)
    import platform
    if platform.system() == 'Darwin':
        try:
            result = subprocess.run(['swift', '--version'], capture_output=True, timeout=2)
            if result.returncode == 0:
                print("   âœ… Apple Vision OCRï¼ˆç³»ç»Ÿè‡ªå¸¦ï¼‰")
                ocr_available = True
            else:
                print("   âš ï¸  Swift ä¸å¯ç”¨")
        except:
            print("   âš ï¸  Swift ä¸å¯ç”¨")
    
    if not ocr_available:
        print("   âš ï¸  æœªæ‰¾åˆ°å¯ç”¨çš„ OCR å¼•æ“")
        print("      macOS: åº”è‡ªåŠ¨ä½¿ç”¨ Vision OCR")
        print("      å…¶ä»–å¹³å°: è¿è¡Œ 'make install-paddle-ocr'")
    
    return errors


def check_database():
    """3. æ•°æ®åº“çŠ¶æ€"""
    print_header("ğŸ—„ï¸  3. æ•°æ®åº“çŠ¶æ€")
    
    errors = []
    try:
        from db.schema import check_database_health
        stats = check_database_health()
        
        print(f"   ğŸ“Š è§†é¢‘æ•°: {stats.get('videos', 0)}")
        print(f"   ğŸ“Š äº§ç‰©æ•°: {stats.get('artifacts', 0)}")
        print(f"   ğŸ“Š æ ‡ç­¾æ•°: {stats.get('tags', 0)}")
        print(f"   ğŸ“Š FTSç´¢å¼•: {stats.get('fts_content', 0)} æ¡")
        print(f"   ğŸ’¾ æ•°æ®åº“å¤§å°: {stats.get('db_size_mb', 0):.2f} MB")
    except Exception as e:
        print(f"   âŒ æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {e}")
        errors.append('database')
    
    return errors


def check_whoosh():
    """4. Whoosh æœç´¢å¼•æ“"""
    print_header("ğŸ” 4. Whoosh æœç´¢å¼•æ“")
    
    try:
        from db.whoosh_search import check_whoosh_status, get_whoosh_index
        status = check_whoosh_status()
        
        print(f"   Whoosh å®‰è£…: {'âœ…' if status['whoosh_installed'] else 'âŒ'}")
        print(f"   jieba å®‰è£…: {'âœ…' if status['jieba_installed'] else 'âŒ'}")
        
        if status['ready']:
            idx = get_whoosh_index()
            st = idx.get_stats()
            print(f"   ç´¢å¼•æ–‡æ¡£æ•°: {st.get('doc_count', 0)}")
            print(f"   ç´¢å¼•ç›®å½•: {st.get('index_dir', 'N/A')}")
    except Exception as e:
        print(f"   âš ï¸  Whoosh æ£€æŸ¥è·³è¿‡: {e}")
    
    return []


def check_search():
    """5. æœç´¢åŠŸèƒ½æµ‹è¯•"""
    print_header("ğŸ” 5. æœç´¢åŠŸèƒ½æµ‹è¯•")
    
    errors = []
    try:
        from db.search import SearchRepository
        repo = SearchRepository()
        
        # æµ‹è¯•è‹±æ–‡æœç´¢ (FTS)
        r1 = repo.search('test', limit=1)
        print(f"   âœ… FTSæœç´¢æ­£å¸¸ (è‹±æ–‡) - æ‰¾åˆ° {len(r1)} æ¡")
        
        # æµ‹è¯•ä¸­æ–‡æœç´¢ (Whoosh)
        r2 = repo.search('æµ‹è¯•', limit=1)
        print(f"   âœ… Whooshæœç´¢æ­£å¸¸ (ä¸­æ–‡) - æ‰¾åˆ° {len(r2)} æ¡")
        
    except Exception as e:
        print(f"   âŒ æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        errors.append('search')
    
    return errors


def check_downloader():
    """6. ä¸‹è½½å™¨çŠ¶æ€"""
    print_header("â¬‡ï¸  6. ä¸‹è½½å™¨çŠ¶æ€")
    
    try:
        from core.video_downloader import VideoDownloader
        dl = VideoDownloader()
        
        print(f"   yt-dlp: {'âœ… ' + dl.ytdlp_path if dl.ytdlp_path else 'âŒ æœªå®‰è£…'}")
        print(f"   BBDown: {'âœ… ' + dl.bbdown_path if dl.bbdown_path else 'âš ï¸  æœªå®‰è£…ï¼ˆBç«™å¤‡ç”¨ï¼‰'}")
    except Exception as e:
        print(f"   âŒ ä¸‹è½½å™¨æ£€æŸ¥å¤±è´¥: {e}")
    
    return []


def check_ffmpeg():
    """7. FFmpeg æ£€æŸ¥"""
    print_header("ğŸ¬ 7. FFmpeg æ£€æŸ¥")
    
    errors = []
    try:
        result = subprocess.run(
            ['ffmpeg', '-version'], 
            capture_output=True, 
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            version = result.stdout.split('\n')[0]
            print(f"   âœ… {version[:60]}...")
        else:
            print("   âŒ ffmpeg è¿è¡Œå¼‚å¸¸")
            errors.append('ffmpeg')
    except FileNotFoundError:
        print("   âŒ ffmpeg æœªå®‰è£…")
        print("      å®‰è£…æ–¹æ³•: brew install ffmpeg")
        errors.append('ffmpeg')
    except Exception as e:
        print(f"   âŒ ffmpeg æ£€æŸ¥å¤±è´¥: {e}")
        errors.append('ffmpeg')
    
    return errors


def check_ocr_engines():
    """7.5 OCR å¼•æ“æ£€æŸ¥"""
    print_header("ğŸ” 7.5. OCR å¼•æ“æ£€æŸ¥")
    
    import platform
    errors = []
    ocr_engines = []
    
    # 1. æ£€æŸ¥ Vision OCRï¼ˆmacOSï¼‰
    if platform.system() == 'Darwin':
        print("   ğŸ Apple Vision OCR (macOS åŸç”Ÿ):")
        
        # æ£€æŸ¥ macOS ç‰ˆæœ¬
        try:
            mac_ver = platform.mac_ver()[0]
            major_ver = int(mac_ver.split('.')[0]) if mac_ver else 0
            if major_ver >= 10:
                print(f"      âœ… macOS ç‰ˆæœ¬: {mac_ver}")
            else:
                print(f"      âŒ macOS ç‰ˆæœ¬è¿‡ä½: {mac_ver} (éœ€è¦ 10.15+)")
                errors.append('vision-ocr-version')
        except:
            print("      âš ï¸  æ— æ³•æ£€æµ‹ macOS ç‰ˆæœ¬")
        
        # æ£€æŸ¥ Swift
        try:
            result = subprocess.run(
                ['swift', '--version'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                swift_ver = result.stdout.split('\n')[0]
                print(f"      âœ… Swift: {swift_ver[:50]}")
                
                # æ£€æŸ¥ Swift è„šæœ¬
                swift_script = Path('ocr/vision_ocr.swift')
                if swift_script.exists():
                    print(f"      âœ… Swift OCR è„šæœ¬: {swift_script}")
                    ocr_engines.append('vision')
                else:
                    print(f"      âŒ Swift OCR è„šæœ¬ä¸å­˜åœ¨: {swift_script}")
                    errors.append('vision-ocr-script')
            else:
                print("      âŒ Swift ä¸å¯ç”¨")
                errors.append('vision-ocr-swift')
        except FileNotFoundError:
            print("      âŒ Swift æœªå®‰è£…ï¼ˆç³»ç»Ÿåº”è‡ªå¸¦ï¼‰")
            errors.append('vision-ocr-swift')
        except Exception as e:
            print(f"      âš ï¸  Swift æ£€æŸ¥å¤±è´¥: {e}")
        
        # æµ‹è¯• Vision OCR Python æ¥å£
        try:
            from ocr.ocr_vision import init_vision_ocr
            print("      âœ… Vision OCR Python æ¨¡å—")
        except ImportError as e:
            print(f"      âš ï¸  Vision OCR Python æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    else:
        print(f"   âš ï¸  é macOS ç³»ç»Ÿ ({platform.system()})ï¼ŒVision OCR ä¸å¯ç”¨")
    
    print()
    
    # 2. æ£€æŸ¥ PaddleOCRï¼ˆè·¨å¹³å°ï¼‰
    print("   ğŸ¼ PaddleOCR (è·¨å¹³å°):")
    try:
        import paddleocr
        print("      âœ… PaddleOCR å·²å®‰è£…")
        
        # æ£€æŸ¥ Paddle
        try:
            import paddle
            paddle_ver = paddle.__version__
            print(f"      âœ… PaddlePaddle: {paddle_ver}")
            
            # æ£€æŸ¥ GPU æ”¯æŒ
            if paddle.is_compiled_with_cuda():
                print("      âœ… GPU æ”¯æŒ: å¯ç”¨")
            else:
                print("      â„¹ï¸  GPU æ”¯æŒ: ä¸å¯ç”¨ï¼ˆä½¿ç”¨ CPUï¼‰")
            
            ocr_engines.append('paddle')
            
        except Exception as e:
            print(f"      âš ï¸  Paddle æ£€æŸ¥å¤±è´¥: {e}")
        
        # æ£€æŸ¥ OCR å·¥å…·æ¨¡å—
        try:
            from ocr.ocr_utils import init_ocr
            print("      âœ… OCR å·¥å…·æ¨¡å—")
        except ImportError as e:
            print(f"      âš ï¸  OCR å·¥å…·æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            
    except ImportError:
        print("      âš ï¸  PaddleOCR æœªå®‰è£…")
        print("      å®‰è£…æ–¹æ³•: make install-paddle-ocr")
    
    print()
    
    # 3. æ€»ç»“
    if ocr_engines:
        print(f"   âœ… å¯ç”¨çš„ OCR å¼•æ“: {', '.join(ocr_engines)}")
        if 'vision' in ocr_engines:
            print("   ğŸ’¡ æ¨è: ä½¿ç”¨ Vision OCR (macOS åŸç”Ÿï¼Œé€Ÿåº¦å¿«)")
    else:
        print("   âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ OCR å¼•æ“")
        if platform.system() == 'Darwin':
            print("   ğŸ’¡ macOS ç”¨æˆ·: åº”è‡ªåŠ¨ä½¿ç”¨ Vision OCRï¼Œè¯·æ£€æŸ¥ Swift ç¯å¢ƒ")
        else:
            print("   ğŸ’¡ è¯·å®‰è£… PaddleOCR: make install-paddle-ocr")
        errors.append('no-ocr-engine')
    
    return errors


def check_api_config():
    """8. API é…ç½®"""
    print_header("ğŸ”‘ 8. API é…ç½®æ£€æŸ¥")
    
    env_file = Path('.env')
    
    if env_file.exists():
        content = env_file.read_text()
        
        # æ£€æŸ¥ GROQ_API_KEY
        if 'GROQ_API_KEY' in content:
            # æ’é™¤å ä½ç¬¦
            lines = [l for l in content.split('\n') if 'GROQ_API_KEY' in l and not l.strip().startswith('#')]
            if lines:
                value = lines[0].split('=', 1)[1].strip() if '=' in lines[0] else ''
                if value and 'your' not in value.lower() and value != '':
                    print("   âœ… GROQ_API_KEY å·²é…ç½®")
                else:
                    print("   âš ï¸  GROQ_API_KEY æœªè®¾ç½®æˆ–ä¸ºå ä½ç¬¦")
            else:
                print("   âš ï¸  GROQ_API_KEY è¢«æ³¨é‡Š")
        else:
            print("   âš ï¸  GROQ_API_KEY æœªé…ç½®")
        
        # æ£€æŸ¥ GEMINI_API_KEY
        if 'GEMINI_API_KEY' in content:
            lines = [l for l in content.split('\n') if 'GEMINI_API_KEY' in l and not l.strip().startswith('#')]
            if lines:
                value = lines[0].split('=', 1)[1].strip() if '=' in lines[0] else ''
                if value and 'your' not in value.lower() and value != '':
                    print("   âœ… GEMINI_API_KEY å·²é…ç½®ï¼ˆç”¨äºè¶…é•¿æ–‡æœ¬å¤„ç†ï¼‰")
                else:
                    print("   âš ï¸  GEMINI_API_KEY æœªè®¾ç½®æˆ–ä¸ºå ä½ç¬¦ï¼ˆå¯é€‰ï¼Œä»…å¤„ç†è¶…é•¿æ–‡æœ¬æ—¶éœ€è¦ï¼‰")
            else:
                print("   âš ï¸  GEMINI_API_KEY è¢«æ³¨é‡Šï¼ˆå¯é€‰ï¼‰")
        else:
            print("   â„¹ï¸  GEMINI_API_KEY æœªé…ç½®ï¼ˆå¯é€‰ï¼Œä»…åœ¨å¤„ç†è¶…è¿‡ 13 ä¸‡ token çš„é•¿æ–‡æœ¬æ—¶éœ€è¦ï¼‰")
    else:
        print("   âŒ .env æ–‡ä»¶ä¸å­˜åœ¨")
        print("      è¯·å¤åˆ¶ config_example.py åˆ›å»º .env æ–‡ä»¶")
    
    return []


def check_disk_space():
    """9. ç£ç›˜ç©ºé—´æ£€æŸ¥"""
    print_header("ğŸ’¾ 9. ç£ç›˜ç©ºé—´æ£€æŸ¥")
    
    import shutil
    
    # æ£€æŸ¥å½“å‰ç›®å½•ç£ç›˜ç©ºé—´
    total, used, free = shutil.disk_usage('.')
    free_gb = free / (1024 ** 3)
    
    if free_gb < 1:
        print(f"   âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³: {free_gb:.1f} GB å¯ç”¨")
    elif free_gb < 5:
        print(f"   âš ï¸  ç£ç›˜ç©ºé—´è¾ƒä½: {free_gb:.1f} GB å¯ç”¨")
    else:
        print(f"   âœ… ç£ç›˜ç©ºé—´å……è¶³: {free_gb:.1f} GB å¯ç”¨")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•å¤§å°
    output_dir = Path('output')
    if output_dir.exists():
        total_size = sum(f.stat().st_size for f in output_dir.rglob('*') if f.is_file())
        print(f"   ğŸ“ output/ ç›®å½•: {total_size / (1024**2):.1f} MB")
    
    videos_dir = Path('videos')
    if videos_dir.exists():
        total_size = sum(f.stat().st_size for f in videos_dir.rglob('*') if f.is_file())
        print(f"   ğŸ“ videos/ ç›®å½•: {total_size / (1024**2):.1f} MB")
    
    return []


def check_archiver():
    """10. ç½‘é¡µå½’æ¡£åŠŸèƒ½"""
    print_header("ğŸŒ 10. ç½‘é¡µå½’æ¡£åŠŸèƒ½")
    
    errors = []
    try:
        # å¯¼å…¥æµ‹è¯•
        from archiver import UniversalArchiver, detect_platform
        from archiver.utils.url_parser import normalize_url, is_valid_url
        from archiver.platforms import (
            ZhihuAdapter, XiaohongshuAdapter, BilibiliAdapter,
            RedditAdapter, WordPressAdapter
        )
        
        print("   âœ… å½’æ¡£æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # å¹³å°æ£€æµ‹æµ‹è¯•
        test_cases = [
            ("https://www.zhihu.com/question/123", "zhihu"),
            ("https://www.xiaohongshu.com/explore/abc", "xiaohongshu"),
            ("https://www.bilibili.com/video/BV123", "bilibili"),
            ("https://www.reddit.com/r/python/", "reddit"),
        ]
        
        platform_ok = True
        for url, expected in test_cases:
            result = detect_platform(url)
            if result == expected:
                print(f"   âœ… å¹³å°æ£€æµ‹: {expected}")
            else:
                print(f"   âŒ å¹³å°æ£€æµ‹å¤±è´¥: {url} â†’ {result} (åº”ä¸º {expected})")
                platform_ok = False
        
        if not platform_ok:
            errors.append('archiver-platform-detection')
        
        # é€‚é…å™¨æµ‹è¯•
        adapters = [
            (ZhihuAdapter(), "zhihu"),
            (XiaohongshuAdapter(), "xiaohongshu"),
            (BilibiliAdapter(), "bilibili"),
            (RedditAdapter(), "reddit"),
            (WordPressAdapter(), "wordpress"),
        ]
        
        for adapter, name in adapters:
            if adapter.name == name and adapter.content_selector:
                print(f"   âœ… {name} é€‚é…å™¨é…ç½®æ­£å¸¸")
            else:
                print(f"   âŒ {name} é€‚é…å™¨é…ç½®å¼‚å¸¸")
                errors.append(f'archiver-{name}')
        
        # URLå·¥å…·æµ‹è¯•
        assert normalize_url("example.com") == "https://example.com"
        assert is_valid_url("https://example.com") == True
        print("   âœ… URLå·¥å…·å‡½æ•°æ­£å¸¸")
        
        # æ£€æŸ¥å½’æ¡£è¾“å‡ºç›®å½•
        archived_dir = Path('archived')
        if archived_dir.exists():
            count = len(list(archived_dir.glob('*.md')))
            print(f"   ğŸ“ å·²å½’æ¡£æ–‡ä»¶: {count} ä¸ª")
        
    except Exception as e:
        print(f"   âŒ å½’æ¡£åŠŸèƒ½æ£€æŸ¥å¤±è´¥: {e}")
        errors.append('archiver')
    
    return errors


def check_cookie_management():
    """11. Cookieç»Ÿä¸€ç®¡ç†"""
    print_header("ğŸª 11. Cookieç»Ÿä¸€ç®¡ç†")
    
    errors = []
    configured_platforms = []
    
    try:
        from archiver.utils.cookie_manager import (
            CookieManager, 
            get_xiaohongshu_cookies
        )
        
        print("   âœ… Cookieç®¡ç†å™¨å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºç®¡ç†å™¨
        manager = CookieManager()
        print("   âœ… CookieManager åˆå§‹åŒ–æˆåŠŸ")
        print()
        
        # ========== æ£€æŸ¥å„å¹³å° Cookie é…ç½®çŠ¶æ€ ==========
        print("   ğŸ“‹ å¹³å° Cookie é…ç½®çŠ¶æ€:")
        print()
        
        # 1. å°çº¢ä¹¦ (XHS-Downloader)
        print("   ğŸ”´ å°çº¢ä¹¦ (XiaohongShu):")
        
        # æ£€æŸ¥ç»Ÿä¸€ä½ç½®ï¼ˆä¼˜å…ˆï¼‰
        unified_config = Path("archiver") / "config" / "xiaohongshu_cookie.json"
        xhs_config = Path("XHS-Downloader") / "Volume" / "settings.json"
        
        has_cookie = False
        cookie_source = None
        
        # ä¼˜å…ˆæ£€æŸ¥ç»Ÿä¸€ä½ç½®
        if unified_config.exists():
            try:
                with open(unified_config, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                cookie = config.get('cookie', '')
                if cookie:
                    has_cookie = True
                    cookie_source = "unified"
                    print(f"      âœ… Cookie: å·²é…ç½® (ç»Ÿä¸€ä½ç½®)")
                    print(f"         ğŸ“ archiver/config/xiaohongshu_cookie.json")
                    print(f"         ğŸ“Š {len(cookie)} å­—ç¬¦")
                    configured_platforms.append('xiaohongshu')
                    
                    # æµ‹è¯•CookieåŠ è½½
                    cookies = get_xiaohongshu_cookies()
                    if cookies:
                        cookie_count = len(cookies)
                        print(f"      âœ… åŠ è½½æˆåŠŸ: {cookie_count} ä¸ªå­—æ®µ")
                        
                        # æ£€æŸ¥å…³é”®å­—æ®µ
                        if 'web_session' in cookies:
                            print("      âœ… web_session: å·²åŒ…å«")
                        else:
                            print("      âš ï¸  web_session: ç¼ºå¤±")
                    else:
                        print("      âš ï¸  CookieåŠ è½½å¤±è´¥")
            except Exception as e:
                print(f"      âš ï¸  ç»Ÿä¸€ä½ç½®é…ç½®è¯»å–å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ—§ä½ç½®ï¼ˆXHS-Downloaderï¼‰
        if not has_cookie and xhs_config.exists():
            try:
                with open(xhs_config, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                cookie = config.get('cookie', '')
                if cookie:
                    has_cookie = True
                    cookie_source = "legacy"
                    print(f"      âš ï¸  Cookie: ä½¿ç”¨æ—§ä½ç½®ï¼ˆå»ºè®®è¿ç§»ï¼‰")
                    print(f"         ğŸ“ XHS-Downloader/Volume/settings.json")
                    print(f"         ğŸ“Š {len(cookie)} å­—ç¬¦")
                    print(f"         ğŸ’¡ è¿è¡Œ 'make export-cookies' è¿ç§»åˆ°ç»Ÿä¸€ä½ç½®")
                    configured_platforms.append('xiaohongshu')
                    
                    # æµ‹è¯•CookieåŠ è½½
                    cookies = get_xiaohongshu_cookies()
                    if cookies:
                        cookie_count = len(cookies)
                        print(f"      âœ… åŠ è½½æˆåŠŸ: {cookie_count} ä¸ªå­—æ®µ")
            except Exception as e:
                print(f"      âš ï¸  æ—§ä½ç½®é…ç½®è¯»å–å¤±è´¥: {e}")
        
        if not has_cookie:
            print("      âš ï¸  Cookie: æœªé…ç½®")
            print("      ğŸ’¡ é…ç½®æ–¹æ³•:")
            print("         1. make config-xhs-cookie (ä¼ ç»Ÿæ–¹å¼)")
            print("         2. æ‰‹åŠ¨åˆ›å»º archiver/config/xiaohongshu_cookie.json (æ¨è)")
        
        print()
        
        # 2. çŸ¥ä¹ (Zhihu)
        print("   ğŸ”µ çŸ¥ä¹ (Zhihu):")
        zhihu_config = Path("archiver") / "config" / "zhihu_cookie.json"
        if zhihu_config.exists():
            try:
                with open(zhihu_config, 'r', encoding='utf-8') as f:
                    zhihu_data = json.load(f)
                    zhihu_cookie = zhihu_data.get('cookie', '')
                if zhihu_cookie and len(zhihu_cookie) > 50:  # æ£€æŸ¥ cookie å­—ç¬¦ä¸²
                    print(f"      âœ… Cookie: å·²é…ç½® ({len(zhihu_cookie)} å­—ç¬¦)")
                    configured_platforms.append('zhihu')
                else:
                    print("      âš ï¸  Cookie: å·²åˆ›å»ºä½†ä¸ºç©º")
                    print("      ğŸ’¡ é…ç½®æ–¹æ³•: make config-zhihu-cookie")
            except Exception as e:
                print(f"      âš ï¸  é…ç½®è¯»å–å¤±è´¥: {e}")
        else:
            print("      â„¹ï¸  Cookie: æœªé…ç½®ï¼ˆå¯é€‰ï¼Œæ— éœ€ç™»å½•å¯è®¿é—®éƒ¨åˆ†å†…å®¹ï¼‰")
            print("      ğŸ’¡ é…ç½®æ–¹æ³•: make config-zhihu-cookie")
        print()
        
        # 3. Bç«™ (Bilibili)
        print("   ğŸ©· Bç«™ (Bilibili):")
        bilibili_config = Path("archiver") / "config" / "bilibili_cookies.json"
        if bilibili_config.exists():
            try:
                with open(bilibili_config, 'r', encoding='utf-8') as f:
                    bilibili_cookies = json.load(f)
                if bilibili_cookies and len(bilibili_cookies) > 0:
                    print(f"      âœ… Cookie: å·²é…ç½® ({len(bilibili_cookies)} ä¸ªå­—æ®µ)")
                    configured_platforms.append('bilibili')
                else:
                    print("      âš ï¸  Cookie: å·²åˆ›å»ºä½†ä¸ºç©º")
            except Exception as e:
                print(f"      âš ï¸  é…ç½®è¯»å–å¤±è´¥: {e}")
        else:
            print("      â„¹ï¸  Cookie: æœªé…ç½®ï¼ˆå¯é€‰ï¼Œæ— éœ€ç™»å½•å¯è®¿é—®éƒ¨åˆ†å†…å®¹ï¼‰")
            print("      ğŸ’¡ é€šè¿‡æµè§ˆå™¨æ‰©å±•æˆ–æ‰‹åŠ¨é…ç½®")
        print()
        
        # 4. Reddit
        print("   ğŸŸ  Reddit:")
        reddit_config = Path("archiver") / "config" / "reddit_cookies.json"
        if reddit_config.exists():
            try:
                with open(reddit_config, 'r', encoding='utf-8') as f:
                    reddit_cookies = json.load(f)
                if reddit_cookies and len(reddit_cookies) > 0:
                    print(f"      âœ… Cookie: å·²é…ç½® ({len(reddit_cookies)} ä¸ªå­—æ®µ)")
                    configured_platforms.append('reddit')
                else:
                    print("      âš ï¸  Cookie: å·²åˆ›å»ºä½†ä¸ºç©º")
            except Exception as e:
                print(f"      âš ï¸  é…ç½®è¯»å–å¤±è´¥: {e}")
        else:
            print("      â„¹ï¸  Cookie: æœªé…ç½®ï¼ˆå¯é€‰ï¼Œæ— éœ€ç™»å½•å¯è®¿é—®å…¬å¼€å†…å®¹ï¼‰")
        print()
        
        # 5. æ¨ç‰¹/X (Twitter)
        print("   ğŸ¦ æ¨ç‰¹ (Twitter/X):")
        twitter_config = Path("archiver") / "config" / "twitter_cookie.json"
        twitter_browser_data = Path("browser_data") / "Default" / "Cookies"
        
        has_json_config = False
        has_browser_data = False
        
        # æ£€æŸ¥ JSON é…ç½®
        if twitter_config.exists():
            try:
                with open(twitter_config, 'r', encoding='utf-8') as f:
                    twitter_data = json.load(f)
                    twitter_cookie = twitter_data.get('cookie', '')
                if twitter_cookie and len(twitter_cookie) > 50:
                    print(f"      âœ… Cookie(JSON): å·²é…ç½® ({len(twitter_cookie)} å­—ç¬¦)")
                    configured_platforms.append('twitter')
                    has_json_config = True
                else:
                    print("      âš ï¸  Cookie(JSON): å·²åˆ›å»ºä½†ä¸ºç©º")
            except Exception as e:
                print(f"      âš ï¸  Cookie(JSON): è¯»å–å¤±è´¥ - {e}")
        
        # æ£€æŸ¥ DrissionPage browser_data
        if twitter_browser_data.exists():
            try:
                import sqlite3
                conn = sqlite3.connect(str(twitter_browser_data))
                cursor = conn.cursor()
                # æŸ¥è¯¢æ¨ç‰¹ç›¸å…³çš„ cookie
                cursor.execute("SELECT COUNT(*) FROM cookies WHERE host_key LIKE '%twitter.com%' OR host_key LIKE '%x.com%'")
                count = cursor.fetchone()[0]
                conn.close()
                
                if count > 0:
                    print(f"      âœ… Cookie(DrissionPage): å·²é…ç½® ({count} æ¡)")
                    if not has_json_config:
                        configured_platforms.append('twitter')
                    has_browser_data = True
                else:
                    print(f"      â„¹ï¸  Cookie(DrissionPage): æœªæ‰¾åˆ°æ¨ç‰¹ç›¸å…³ cookie")
            except Exception as e:
                print(f"      â„¹ï¸  Cookie(DrissionPage): æ£€æŸ¥å¤±è´¥")
        
        # å¦‚æœä¸¤ç§éƒ½æ²¡æœ‰
        if not has_json_config and not has_browser_data:
            print("      â„¹ï¸  Cookie: æœªé…ç½®ï¼ˆæ¨èé…ç½®ä»¥è®¿é—®å®Œæ•´å†…å®¹ï¼‰")
            print("      ğŸ’¡ æ–¹æ³•1: python scripts/login_twitter.py (DrissionPage)")
            print("      ğŸ’¡ æ–¹æ³•2: æ‰‹åŠ¨åˆ›å»º archiver/config/twitter_cookie.json")
        elif has_browser_data and not has_json_config:
            print("      ğŸ’¡ å¯é€‰: å¯¼å‡ºä¸º JSON æ ¼å¼ä»¥æé«˜å…¼å®¹æ€§")
        
        print()
        
        # æ‰«æå…¶ä»–æœªçŸ¥çš„ cookie é…ç½®
        print("   ğŸ” æ‰«æå…¶ä»– Cookie é…ç½®:")
        config_dir = Path("archiver") / "config"
        if config_dir.exists():
            all_cookie_files = list(config_dir.glob("*cookie*.json"))
            known_files = {
                Path("archiver") / "config" / "zhihu_cookie.json",
                Path("archiver") / "config" / "bilibili_cookies.json",
                Path("archiver") / "config" / "bilibili_cookie.json",
                Path("archiver") / "config" / "reddit_cookies.json",
                Path("archiver") / "config" / "reddit_cookie.json",
                Path("archiver") / "config" / "twitter_cookie.json",
                Path("archiver") / "config" / "twitter_cookies.json",
            }
            unknown_files = [f for f in all_cookie_files if f not in known_files]
            
            if unknown_files:
                for unknown_file in unknown_files:
                    print(f"      âš ï¸  å‘ç°å…¶ä»–é…ç½®: {unknown_file.name}")
                    try:
                        with open(unknown_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            cookie = data.get('cookie', '')
                            if cookie and len(cookie) > 50:
                                print(f"         âœ… å·²é…ç½® ({len(cookie)} å­—ç¬¦)")
                            else:
                                print(f"         â„¹ï¸  æœªé…ç½®æˆ–ä¸ºç©º")
                    except:
                        pass
            else:
                print(f"      âœ… æ— å…¶ä»–é…ç½®æ–‡ä»¶")
        print()
        
        # æ€»ç»“
        print("   " + "â”€" * 50)
        if configured_platforms:
            platform_map = {
                'xiaohongshu': 'å°çº¢ä¹¦',
                'zhihu': 'çŸ¥ä¹', 
                'bilibili': 'Bç«™',
                'reddit': 'Reddit',
                'twitter': 'æ¨ç‰¹'
            }
            platform_names = [platform_map.get(p, p) for p in configured_platforms]
            print(f"   âœ… å·²é…ç½®å¹³å° ({len(configured_platforms)}): {', '.join(platform_names)}")
        else:
            print("   âš ï¸  å°šæœªé…ç½®ä»»ä½•å¹³å°çš„ Cookie")
            print("   ğŸ’¡ å°çº¢ä¹¦ç­‰å¹³å°éœ€è¦ Cookie æ‰èƒ½æ­£å¸¸è®¿é—®")
        
        print()
        print("   ğŸ’¡ é…ç½®ä¼˜å…ˆçº§:")
        print("      ğŸ”´ å¿…éœ€: å°çº¢ä¹¦ï¼ˆåçˆ¬è™«ä¸¥æ ¼ï¼‰- make config-xhs-cookie")
        print("      ğŸŸ¡ æ¨è: çŸ¥ä¹ã€æ¨ç‰¹ï¼ˆå¢å¼ºè®¿é—®èƒ½åŠ›ï¼‰- make config-zhihu-cookie")
        print("      ğŸŸ¢ å¯é€‰: Bç«™ã€Redditï¼ˆå…¬å¼€å†…å®¹æ— éœ€ç™»å½•ï¼‰")
        
    except Exception as e:
        print(f"   âŒ Cookieç®¡ç†æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        errors.append('cookie-management')
    
    return errors


def check_archiver_integration():
    """12. å½’æ¡£é›†æˆæµ‹è¯•"""
    print_header("ğŸ”— 12. å½’æ¡£é›†æˆæµ‹è¯•")
    
    try:
        # æµ‹è¯•è‡ªåŠ¨CookieåŠ è½½
        from archiver.utils.url_parser import detect_platform
        from archiver.utils.cookie_manager import get_xiaohongshu_cookies
        
        # å¹³å°æ£€æµ‹
        xhs_url = "https://www.xiaohongshu.com/explore/abc123"
        platform = detect_platform(xhs_url)
        print(f"   âœ… URLå¹³å°æ£€æµ‹: {platform}")
        
        # Cookieå¯ç”¨æ€§
        if platform == "xiaohongshu":
            cookies = get_xiaohongshu_cookies()
            if cookies:
                print(f"   âœ… å°çº¢ä¹¦Cookieè‡ªåŠ¨åŠ è½½å¯ç”¨")
            else:
                print(f"   âš ï¸  å°çº¢ä¹¦Cookieæœªé…ç½®ï¼ˆéœ€è¦æ—¶é…ç½®ï¼‰")
        
        # æµ‹è¯•Markdownè½¬æ¢å™¨
        from archiver.core.markdown_converter import MarkdownConverter
        converter = MarkdownConverter()
        
        test_html = "<p>æµ‹è¯•<strong>å†…å®¹</strong></p>"
        markdown = converter.convert(
            html=test_html,
            title="æµ‹è¯•",
            url="https://example.com",
            platform="test"
        )
        
        if "title: æµ‹è¯•" in markdown and "æµ‹è¯•" in markdown:
            print("   âœ… Markdownè½¬æ¢å™¨æ­£å¸¸")
        else:
            print("   âŒ Markdownè½¬æ¢å™¨å¼‚å¸¸")
            return ['markdown-converter']
        
        # æ£€æŸ¥æ–‡æ¡£
        docs = [
            "docs/ARCHIVER_GUIDE.md",
            "docs/ARCHIVER_QUICKREF.md",
            "docs/COOKIE_UNIFIED.md",
            "archiver/README.md",
        ]
        
        missing_docs = []
        for doc in docs:
            if Path(doc).exists():
                print(f"   âœ… {doc}")
            else:
                print(f"   âŒ {doc} ç¼ºå¤±")
                missing_docs.append(doc)
        
        if missing_docs:
            return ['archiver-docs']
        
    except Exception as e:
        print(f"   âŒ å½’æ¡£é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return ['archiver-integration']
    
    return []


def main():
    """ä¸»å‡½æ•°"""
    print("â”" * 50)
    print("ğŸ”¬ å…¨åŠŸèƒ½è‡ªæ£€å’Œæµ‹è¯•")
    print("â”" * 50)
    
    all_errors = []
    
    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
    all_errors.extend(check_module_imports())
    all_errors.extend(check_dependencies())
    all_errors.extend(check_database())
    all_errors.extend(check_whoosh())
    all_errors.extend(check_search())
    all_errors.extend(check_downloader())
    all_errors.extend(check_ffmpeg())
    all_errors.extend(check_ocr_engines())  # æ–°å¢ OCR æ£€æŸ¥
    all_errors.extend(check_api_config())
    all_errors.extend(check_disk_space())
    all_errors.extend(check_archiver())
    all_errors.extend(check_cookie_management())
    all_errors.extend(check_archiver_integration())
    
    # æ€»ç»“
    print("\n" + "â”" * 50)
    if all_errors:
        print(f"âš ï¸  å‘ç° {len(all_errors)} ä¸ªé—®é¢˜:")
        for err in all_errors:
            print(f"   â€¢ {err}")
        print("\nğŸ’¡ å»ºè®®:")
        
        if 'archiver' in all_errors or any('archiver' in e for e in all_errors):
            print("   â€¢ ç½‘é¡µå½’æ¡£åŠŸèƒ½é—®é¢˜:")
            print("     - å®‰è£…ä¾èµ–: make install")
            print("     - å®‰è£…æµè§ˆå™¨: playwright install chromium")
            print("     - è¿è¡Œæµ‹è¯•: make test-archiver")
        
        if 'cookie-management' in all_errors:
            print("   â€¢ Cookieç®¡ç†é—®é¢˜:")
            print("     - é…ç½®å°çº¢ä¹¦Cookie: make config-xhs-cookie")
            print("     - æµ‹è¯•Cookie: python scripts/test_cookie_unified.py")
        
        if 'ffmpeg' in all_errors:
            print("   â€¢ FFmpegé—®é¢˜:")
            print("     - å®‰è£…: brew install ffmpeg")
        
        if any('ocr' in e for e in all_errors):
            print("   â€¢ OCR å¼•æ“é—®é¢˜:")
            print("     - macOS: æ£€æŸ¥ Swift ç¯å¢ƒ (swift --version)")
            print("     - è·¨å¹³å°: å®‰è£… PaddleOCR (make install-paddle-ocr)")
            print("     - æµ‹è¯• Vision OCR: make test-vision-ocr")
        
        if 'database' in all_errors:
            print("   â€¢ æ•°æ®åº“é—®é¢˜:")
            print("     - åˆå§‹åŒ–: memidx init")
        
        print("\nè¯·ä¿®å¤ä»¥ä¸Šé—®é¢˜åé‡æ–°è¿è¡Œ make selftest")
        print("â”" * 50)
        return 1
    else:
        print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        print("\nğŸ¯ åŠŸèƒ½çŠ¶æ€:")
        print("   âœ… è§†é¢‘å¤„ç†ç³»ç»Ÿ")
        print("   âœ… è§†é¢‘ä¸‹è½½ç³»ç»Ÿ")
        print("   âœ… æ•°æ®åº“ä¸æœç´¢")
        print("   âœ… ç½‘é¡µå½’æ¡£ç³»ç»Ÿ")
        print("   âœ… Cookieç»Ÿä¸€ç®¡ç†")
        print("\nğŸ’¡ å¿«é€Ÿå¼€å§‹:")
        print("   â€¢ å¤„ç†è§†é¢‘: make run VIDEO=video.mp4")
        print("   â€¢ ä¸‹è½½è§†é¢‘: make download-run URL=è§†é¢‘é“¾æ¥")
        print("   â€¢ å½’æ¡£ç½‘é¡µ: make archive URL=ç½‘é¡µé“¾æ¥")
        print("   â€¢ æœç´¢å†…å®¹: make search Q='å…³é”®è¯'")
        print("â”" * 50)
        return 0


if __name__ == '__main__':
    sys.exit(main())
