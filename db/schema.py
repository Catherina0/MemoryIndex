"""
æ•°æ®åº“åˆå§‹åŒ–å’Œè¿æ¥ç®¡ç†
"""
import sqlite3
from pathlib import Path
from typing import Optional
import json


def _json_adapter(data):
    """å°† Python å¯¹è±¡è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²"""
    return json.dumps(data, ensure_ascii=False)


def _json_converter(data):
    """å°† JSON å­—ç¬¦ä¸²è½¬æ¢ä¸º Python å¯¹è±¡"""
    return json.loads(data)


# æ³¨å†Œ JSON ç±»å‹è½¬æ¢å™¨
sqlite3.register_adapter(dict, _json_adapter)
sqlite3.register_adapter(list, _json_adapter)
sqlite3.register_converter("JSON", _json_converter)


def get_connection(db_path: Optional[str] = None) -> sqlite3.Connection:
    """
    è·å–æ•°æ®åº“è¿æ¥
    
    Args:
        db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º storage/database/knowledge.db
    
    Returns:
        sqlite3.Connection: æ•°æ®åº“è¿æ¥å¯¹è±¡
    """
    if db_path is None:
        # é»˜è®¤è·¯å¾„
        project_root = Path(__file__).parent.parent
        db_path = project_root / "storage" / "database" / "knowledge.db"
    
    db_path = Path(db_path)
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è¿æ¥æ•°æ®åº“ï¼Œå¯ç”¨ JSON æ”¯æŒå’Œå¤–é”®çº¦æŸ
    conn = sqlite3.connect(
        str(db_path),
        detect_types=sqlite3.PARSE_DECLTYPES,
        check_same_thread=False
    )
    
    # é…ç½®è¿æ¥
    conn.row_factory = sqlite3.Row  # è¿”å›å­—å…¸å¼è¡Œ
    conn.execute("PRAGMA foreign_keys = ON")  # å¯ç”¨å¤–é”®
    conn.execute("PRAGMA journal_mode = WAL")  # å¯ç”¨ WAL æ¨¡å¼æå‡å¹¶å‘
    
    return conn


def init_database(db_path: Optional[str] = None, force_recreate: bool = False):
    """
    åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåˆ›å»ºè¡¨ã€ç´¢å¼•ã€è§¦å‘å™¨ç­‰ï¼‰
    
    Args:
        db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼‰
    """
    if db_path is None:
        project_root = Path(__file__).parent.parent
        db_path = project_root / "storage" / "database" / "knowledge.db"
    
    db_path = Path(db_path)
    
    # å¦‚æœå¼ºåˆ¶é‡å»ºï¼Œåˆ é™¤æ—§æ•°æ®åº“
    if force_recreate and db_path.exists():
        db_path.unlink()
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§æ•°æ®åº“: {db_path}")
    
    # è¯»å– schema.sql
    schema_file = Path(__file__).parent / "schema.sql"
    with open(schema_file, 'r', encoding='utf-8') as f:
        schema_sql = f.read()
    
    # æ‰§è¡Œå»ºè¡¨è¯­å¥
    conn = get_connection(str(db_path))
    try:
        # åˆ†å‰²å¹¶æ‰§è¡Œæ¯ä¸ªè¯­å¥ï¼ˆSQLite executescript ä¸æ”¯æŒå‚æ•°åŒ–ï¼‰
        conn.executescript(schema_sql)
        conn.commit()
        
        print(f"âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {db_path}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
        cursor = conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
            ORDER BY name
        """)
        tables = [row['name'] for row in cursor.fetchall()]
        print(f"ğŸ“Š å·²åˆ›å»º {len(tables)} å¼ è¡¨: {', '.join(tables)}")
        
        # æ£€æŸ¥ FTS5 è¡¨
        cursor = conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name LIKE 'fts_%'
        """)
        fts_tables = [row['name'] for row in cursor.fetchall()]
        if fts_tables:
            print(f"ğŸ” å…¨æ–‡æœç´¢è¡¨: {', '.join(fts_tables)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    finally:
        conn.close()


def check_database_health(db_path: Optional[str] = None) -> dict:
    """
    æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    conn = get_connection(db_path)
    try:
        stats = {}
        
        # ç»Ÿè®¡å„è¡¨è®°å½•æ•°
        tables = ['videos', 'artifacts', 'tags', 'topics', 'timeline_entries']
        for table in tables:
            cursor = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
            stats[table] = cursor.fetchone()['count']
        
        # FTS è¡¨ç»Ÿè®¡
        try:
            cursor = conn.execute("SELECT COUNT(*) as count FROM fts_content")
            stats['fts_content'] = cursor.fetchone()['count']
        except Exception:
            # FTS è¡¨å¯èƒ½ä¸å­˜åœ¨
            stats['fts_content'] = 0
        
        # æŒ‰æ¥æºç±»å‹ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT source_type, COUNT(*) as count 
            FROM videos 
            GROUP BY source_type
            ORDER BY count DESC
        """)
        stats['by_source'] = {row['source_type']: row['count'] for row in cursor.fetchall()}
        
        # æŒ‰å¤„ç†çŠ¶æ€ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT status, COUNT(*) as count 
            FROM videos 
            GROUP BY status
        """)
        stats['by_status'] = {row['status']: row['count'] for row in cursor.fetchall()}
        
        # ç½‘é¡µå½’æ¡£ç»Ÿè®¡ï¼ˆåŒ…æ‹¬ zhihu, reddit, twitter, web_archiveï¼‰
        cursor = conn.execute("""
            SELECT COUNT(*) as count 
            FROM videos 
            WHERE source_type IN ('zhihu', 'reddit', 'twitter', 'web_archive')
        """)
        stats['web_archives'] = cursor.fetchone()['count']
        
        # è§†é¢‘æ–‡ä»¶ç»Ÿè®¡ï¼ˆæœ¬åœ°è§†é¢‘ï¼‰
        cursor = conn.execute("""
            SELECT COUNT(*) as count 
            FROM videos 
            WHERE source_type IN ('local', 'bilibili', 'youtube', 'xiaohongshu')
        """)
        stats['video_files'] = cursor.fetchone()['count']
        
        # æœ€è¿‘å¤„ç†è®°å½•ï¼ˆæœ€è¿‘7å¤©ï¼‰
        cursor = conn.execute("""
            SELECT COUNT(*) as count 
            FROM videos 
            WHERE processed_at > datetime('now', '-7 days')
        """)
        stats['recent_processed'] = cursor.fetchone()['count']
        
        # ç»Ÿè®¡æœ‰OCRçš„è®°å½•
        cursor = conn.execute("""
            SELECT COUNT(DISTINCT video_id) as count 
            FROM artifacts 
            WHERE artifact_type = 'ocr'
        """)
        stats['with_ocr'] = cursor.fetchone()['count']
        
        # ç»Ÿè®¡æœ‰AIæŠ¥å‘Šçš„è®°å½•
        cursor = conn.execute("""
            SELECT COUNT(DISTINCT video_id) as count 
            FROM artifacts 
            WHERE artifact_type = 'report'
        """)
        stats['with_report'] = cursor.fetchone()['count']
        
        # ç»Ÿè®¡å¤±è´¥çš„è®°å½•
        cursor = conn.execute("""
            SELECT COUNT(*) as count 
            FROM videos 
            WHERE status = 'failed'
        """)
        stats['failed_count'] = cursor.fetchone()['count']
        
        # å¹³å‡æ ‡ç­¾æ•°ï¼ˆæ¯ä¸ªè§†é¢‘ï¼‰
        cursor = conn.execute("""
            SELECT AVG(tag_count) as avg_tags
            FROM (
                SELECT video_id, COUNT(*) as tag_count
                FROM video_tags
                GROUP BY video_id
            )
        """)
        result = cursor.fetchone()
        stats['avg_tags_per_video'] = result['avg_tags'] if result and result['avg_tags'] else 0
        
        # æ•°æ®åº“æ–‡ä»¶å¤§å°
        if db_path:
            db_file = Path(db_path)
        else:
            project_root = Path(__file__).parent.parent
            db_file = project_root / "storage" / "database" / "knowledge.db"
        
        if db_file.exists():
            stats['db_size_mb'] = db_file.stat().st_size / 1024 / 1024
        
        return stats
        
    finally:
        conn.close()


if __name__ == '__main__':
    """å‘½ä»¤è¡Œå·¥å…·ï¼šåˆå§‹åŒ–æ•°æ®åº“"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆå§‹åŒ–çŸ¥è¯†åº“æ•°æ®åº“')
    parser.add_argument('--db', type=str, help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡å»ºï¼ˆåˆ é™¤æ‰€æœ‰æ•°æ®ï¼‰')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€')
    
    args = parser.parse_args()
    
    if args.check:
        stats = check_database_health(args.db)
        
        print("\n" + "â”€" * 44)
        print("  ğŸ—„ï¸  1. åŸºç¡€ç»Ÿè®¡")
        print("â”€" * 44)
        print(f"   ğŸ“Š è§†é¢‘æ€»æ•°: {stats.get('videos', 0)}")
        print(f"   ğŸ“Š äº§ç‰©æ•°: {stats.get('artifacts', 0)}")
        print(f"   ğŸ“Š æ ‡ç­¾æ•°: {stats.get('tags', 0)}")
        print(f"   ğŸ“Š ä¸»é¢˜æ•°: {stats.get('topics', 0)}")
        print(f"   ğŸ“Š æ—¶é—´çº¿æ¡ç›®: {stats.get('timeline_entries', 0)}")
        print(f"   ğŸ“Š FTSç´¢å¼•: {stats.get('fts_content', 0)} æ¡")
        print(f"   ğŸ’¾ æ•°æ®åº“å¤§å°: {stats.get('db_size_mb', 0):.2f} MB")
        
        print("\n" + "â”€" * 44)
        print("  ğŸ“ 2. æŒ‰æ¥æºç±»å‹ç»Ÿè®¡")
        print("â”€" * 44)
        by_source = stats.get('by_source', {})
        source_names = {
            'local': 'æœ¬åœ°è§†é¢‘',
            'bilibili': 'Bç«™',
            'youtube': 'YouTube',
            'xiaohongshu': 'å°çº¢ä¹¦',
            'twitter': 'Twitter/X',
            'zhihu': 'çŸ¥ä¹',
            'reddit': 'Reddit',
            'web_archive': 'é€šç”¨ç½‘é¡µ'
        }
        for source_type, count in sorted(by_source.items(), key=lambda x: x[1], reverse=True):
            source_name = source_names.get(source_type, source_type)
            print(f"   â€¢ {source_name}: {count} æ¡")
        
        print("\n" + "â”€" * 44)
        print("  ğŸ”„ 3. å¤„ç†çŠ¶æ€ç»Ÿè®¡")
        print("â”€" * 44)
        by_status = stats.get('by_status', {})
        status_names = {
            'completed': 'âœ… å·²å®Œæˆ',
            'processing': 'â³ å¤„ç†ä¸­',
            'failed': 'âŒ å¤±è´¥',
            'pending': 'â¸ï¸  å¾…å¤„ç†'
        }
        for status, count in by_status.items():
            status_name = status_names.get(status, status)
            print(f"   {status_name}: {count} æ¡")
        
        print("\n" + "â”€" * 44)
        print("  ğŸ“Š 4. å†…å®¹ç±»å‹ç»Ÿè®¡")
        print("â”€" * 44)
        print(f"   ğŸ¥ è§†é¢‘æ–‡ä»¶: {stats.get('video_files', 0)} æ¡")
        print(f"   ğŸŒ ç½‘é¡µå½’æ¡£: {stats.get('web_archives', 0)} æ¡")
        print(f"   ğŸ” å«OCRè¯†åˆ«: {stats.get('with_ocr', 0)} æ¡")
        print(f"   ğŸ“„ å«AIæŠ¥å‘Š: {stats.get('with_report', 0)} æ¡")
        avg_tags = stats.get('avg_tags_per_video', 0)
        print(f"   ğŸ·ï¸  å¹³å‡æ ‡ç­¾æ•°: {avg_tags:.1f} ä¸ª/æ¡")
        
        print("\n" + "â”€" * 44)
        print("  â° 5. æ´»è·ƒåº¦ä¸å¥åº·çŠ¶å†µ")
        print("â”€" * 44)
        print(f"   ğŸ“… æœ€è¿‘7å¤©å¤„ç†: {stats.get('recent_processed', 0)} æ¡")
        failed = stats.get('failed_count', 0)
        total = stats.get('videos', 0)
        if total > 0:
            success_rate = ((total - failed) / total) * 100
            print(f"   âœ… å¤„ç†æˆåŠŸç‡: {success_rate:.1f}%")
        if failed > 0:
            print(f"   âš ï¸  å¤±è´¥è®°å½•: {failed} æ¡")
        
        print("\n" + "â”€" * 44 + "\n")
    else:
        init_database(args.db, args.force)
