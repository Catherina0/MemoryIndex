#!/usr/bin/env python3
"""
æ•°æ®åº“å¯¼å…¥æµ‹è¯•è„šæœ¬
ä½¿ç”¨ output ç›®å½•ä¸­çš„çœŸå®æ•°æ®æµ‹è¯•æ•°æ®åº“åŠŸèƒ½
"""
import sys
import json
from pathlib import Path
from datetime import datetime
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from db import VideoRepository
from db.models import Video, Artifact, Topic, TimelineEntry, SourceType, ArtifactType, ProcessingStatus


def extract_tags_from_text(text: str) -> list:
    """ä»æ–‡æœ¬ä¸­æå–æ ‡ç­¾"""
    tags = []
    
    tag_patterns = [
        r'æ ‡ç­¾[ï¼š:]\s*(.+)',
        r'Tags[ï¼š:]\s*(.+)',
        r'å…³é”®è¯[ï¼š:]\s*(.+)',
        r'Keywords[ï¼š:]\s*(.+)',
    ]
    
    for pattern in tag_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            clean_match = re.sub(r'\*\*|\*|`|#|["""\'\'"]', '', match)
            tag_list = re.split(r'[,ï¼Œã€\s;ï¼›]+', clean_match.strip())
            tags.extend([t.strip() for t in tag_list if t.strip()])
    
    seen = set()
    unique_tags = []
    for tag in tags:
        tag = re.sub(r'[^\w\u4e00-\u9fa5\-]', '', tag)
        tag_lower = tag.lower()
        if tag_lower not in seen and len(tag) > 1 and len(tag) < 20:
            seen.add(tag_lower)
            unique_tags.append(tag)
    
    return unique_tags[:10]


def extract_topics_from_text(text: str) -> list:
    """ä»æ–‡æœ¬ä¸­æå–ä¸»é¢˜"""
    topics = []
    lines = text.split('\n')
    
    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith('##') and not line.startswith('###'):
            title = line.lstrip('#').strip()
            
            skip_titles = ['AI æ™ºèƒ½æ€»ç»“', 'æ•°æ®ç»Ÿè®¡', 'åŸå§‹æ•°æ®', 'æ€»ç»“', 'æ ‡ç­¾', 'Tags', 'å…³é”®è¯']
            if any(skip in title for skip in skip_titles):
                continue
            
            time_match = re.search(r'\[?(\d{1,2}):(\d{2})\s*-\s*(\d{1,2}):(\d{2})\]?', line)
            
            if time_match:
                start_min, start_sec, end_min, end_sec = map(int, time_match.groups())
                start_time = start_min * 60 + start_sec
                end_time = end_min * 60 + end_sec
            else:
                start_time = 0
                end_time = 0
            
            description_lines = []
            for j in range(i + 1, min(i + 5, len(lines))):
                desc_line = lines[j].strip()
                if desc_line and not desc_line.startswith('#'):
                    description_lines.append(desc_line)
                elif desc_line.startswith('##'):
                    break
            
            description = ' '.join(description_lines)[:200]
            
            topics.append({
                'title': title[:100],
                'start_time': start_time,
                'end_time': end_time,
                'summary': description,
            })
    
    return topics[:20]


def import_directory(session_dir: Path, video_title: str = None, source_url: str = None):
    """
    å¯¼å…¥ä¸€ä¸ª output ç›®å½•åˆ°æ•°æ®åº“
    
    Args:
        session_dir: output å­ç›®å½•è·¯å¾„
        video_title: è§†é¢‘æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œä»ç›®å½•åæå–ï¼‰
        source_url: æºURLï¼ˆå¯é€‰ï¼‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ å¯¼å…¥ç›®å½•: {session_dir.name}")
    print(f"{'='*60}")
    
    # ä»ç›®å½•åæå–è§†é¢‘æ ‡é¢˜
    if not video_title:
        # å»æ‰æ—¶é—´æˆ³éƒ¨åˆ†
        dir_name = session_dir.name
        parts = dir_name.rsplit('_', 2)
        if len(parts) >= 3 and parts[-2].isdigit() and parts[-1].isdigit():
            video_title = '_'.join(parts[:-2])
        else:
            video_title = dir_name
    
    # åˆ¤æ–­æ¥æº
    if 'bilibili' in session_dir.name.lower() or 'BV' in session_dir.name:
        source_type = SourceType.BILIBILI
    elif 'youtube' in session_dir.name.lower():
        source_type = SourceType.YOUTUBE
    else:
        source_type = SourceType.LOCAL
    
    # è¯»å–æ–‡ä»¶
    transcript_file = session_dir / "transcript_raw.md"
    ocr_file = session_dir / "ocr_raw.md"
    report_file = session_dir / "report.md"
    timeline_file = session_dir / "timeline.md"
    
    transcript_text = ""
    ocr_text = ""
    report_text = ""
    timeline_text = ""
    
    if transcript_file.exists():
        transcript_text = transcript_file.read_text(encoding='utf-8')
        print(f"   âœ… è¯»å–è¯­éŸ³è½¬å†™ ({len(transcript_text)} å­—ç¬¦)")
    
    if ocr_file.exists():
        ocr_text = ocr_file.read_text(encoding='utf-8')
        print(f"   âœ… è¯»å–OCRè¯†åˆ« ({len(ocr_text)} å­—ç¬¦)")
    
    if report_file.exists():
        report_text = report_file.read_text(encoding='utf-8')
        print(f"   âœ… è¯»å–AIæŠ¥å‘Š ({len(report_text)} å­—ç¬¦)")
    
    if timeline_file.exists():
        timeline_text = timeline_file.read_text(encoding='utf-8')
        print(f"   âœ… è¯»å–æ—¶é—´çº¿ ({len(timeline_text)} å­—ç¬¦)")
    
    if not transcript_text and not ocr_text and not report_text:
        print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å…¥çš„æ•°æ®æ–‡ä»¶")
        return None
    
    # åˆ›å»ºè§†é¢‘è®°å½•
    repo = VideoRepository()
    
    # ä½¿ç”¨ç›®å½•è·¯å¾„ä½œä¸ºå”¯ä¸€æ ‡è¯†
    import hashlib
    content_hash = hashlib.sha256(str(session_dir).encode()).hexdigest()
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing = repo.get_video_by_hash(content_hash)
    if existing:
        print(f"   âš ï¸  è§†é¢‘å·²å­˜åœ¨ (ID: {existing.id})ï¼Œè·³è¿‡...")
        return existing.id
    
    video = Video(
        content_hash=content_hash,
        video_id=None,
        source_type=source_type,
        source_url=source_url,
        platform_title=video_title,
        title=video_title,
        duration_seconds=None,
        file_path=str(session_dir),
        file_size_bytes=0,
        processing_config={'imported_from': str(session_dir)},
        status=ProcessingStatus.COMPLETED
    )
    
    video_id = repo.create_video(video)
    print(f"   âœ… åˆ›å»ºè§†é¢‘è®°å½• (ID: {video_id})")
    
    # ä¿å­˜äº§ç‰©
    if transcript_text:
        artifact = Artifact(
            video_id=video_id,
            artifact_type=ArtifactType.TRANSCRIPT,
            content_text=transcript_text,
            file_path=str(transcript_file),
            model_name="imported",
            char_count=len(transcript_text)
        )
        repo.save_artifact(artifact)
        print(f"   âœ… ä¿å­˜è¯­éŸ³è½¬å†™äº§ç‰©")
    
    if ocr_text:
        artifact = Artifact(
            video_id=video_id,
            artifact_type=ArtifactType.OCR,
            content_text=ocr_text,
            file_path=str(ocr_file),
            model_name="imported",
            char_count=len(ocr_text)
        )
        repo.save_artifact(artifact)
        print(f"   âœ… ä¿å­˜OCRäº§ç‰©")
    
    if report_text:
        artifact = Artifact(
            video_id=video_id,
            artifact_type=ArtifactType.REPORT,
            content_text=report_text,
            file_path=str(report_file),
            model_name="imported",
            char_count=len(report_text)
        )
        repo.save_artifact(artifact)
        print(f"   âœ… ä¿å­˜AIæŠ¥å‘Šäº§ç‰©")
    
    # æå–æ ‡ç­¾
    tags = extract_tags_from_text(report_text)
    if tags:
        repo.save_tags(video_id, tags, source='auto', confidence=0.8)
        print(f"   âœ… ä¿å­˜æ ‡ç­¾: {', '.join(tags[:5])}{'...' if len(tags) > 5 else ''}")
    
    # æå–ä¸»é¢˜
    topics = extract_topics_from_text(report_text)
    if topics:
        topic_objects = []
        for t in topics:
            topic = Topic(
                video_id=video_id,
                title=t['title'],
                start_time=t['start_time'],
                end_time=t['end_time'],
                summary=t['summary'],
                keywords=[]
            )
            topic_objects.append(topic)
        
        repo.save_topics(video_id, topic_objects)
        print(f"   âœ… ä¿å­˜ä¸»é¢˜: {len(topics)} ä¸ªç« èŠ‚")
    
    # æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•
    repo.update_fts_index(video_id)
    print(f"   âœ… æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•")
    
    return video_id


def test_search(repo):
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    from db import SearchRepository
    
    print(f"\n{'='*60}")
    print("ğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½")
    print(f"{'='*60}")
    
    search_repo = SearchRepository()
    
    # æµ‹è¯•1: å…¨æ–‡æœç´¢
    print("\n1ï¸âƒ£ å…¨æ–‡æœç´¢ 'INTP':")
    results = search_repo.search("INTP", limit=5)
    for i, r in enumerate(results, 1):
        print(f"   [{i}] {r.video_title} - {r.source_field} - ç›¸å…³æ€§: {r.relevance_score:.2f}")
        print(f"       ç‰‡æ®µ: {r.matched_snippet[:80]}...")
    
    # æµ‹è¯•2: æŒ‰æ ‡ç­¾æœç´¢
    print("\n2ï¸âƒ£ æŒ‰æ ‡ç­¾æœç´¢:")
    popular_tags = search_repo.get_popular_tags(limit=5)
    if popular_tags:
        print(f"   çƒ­é—¨æ ‡ç­¾: {', '.join([t['name'] for t in popular_tags])}")
        
        tag_name = popular_tags[0]['name']
        results = search_repo.search_by_tags([tag_name], match_all=False, limit=3)
        print(f"\n   æœç´¢æ ‡ç­¾ '{tag_name}' çš„è§†é¢‘:")
        for i, r in enumerate(results, 1):
            tags_str = r['tags'] if isinstance(r['tags'], str) else ', '.join(r.get('tags', []))
            print(f"   [{i}] {r['title']} - æ ‡ç­¾: {tags_str[:50]}...")
    else:
        print("   æš‚æ— æ ‡ç­¾")
    
    # æµ‹è¯•3: æœç´¢ä¸»é¢˜
    print("\n3ï¸âƒ£ æœç´¢ä¸»é¢˜ 'ç›®æ ‡':")
    results = search_repo.search_topics("ç›®æ ‡", limit=5)
    if results:
        for i, r in enumerate(results, 1):
            print(f"   [{i}] {r['title']} - è§†é¢‘: {r['video_title']}")
    else:
        print("   æœªæ‰¾åˆ°ç›¸å…³ä¸»é¢˜")


def main():
    """ä¸»å‡½æ•°"""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘        æ•°æ®åº“å¯¼å…¥æµ‹è¯• - ä½¿ç”¨çœŸå® output æ•°æ®              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    output_dir = Path("/Users/catherina/Documents/GitHub/knowledge/output")
    
    # è¦å¯¼å…¥çš„ç›®å½•åˆ—è¡¨ï¼ˆé€‰æ‹©æœ‰å®Œæ•´æ•°æ®çš„ï¼‰
    target_dirs = [
        "INTPï¼šä½ ä¸æ˜¯è¿·èŒ«ï¼Œè€Œæ˜¯åœ¨é€ƒé¿çœŸæ­£çš„ç›®æ ‡_bilibili_BV1ngCyBiEkc_20251212_000338",
        "INFJ_Â·_INFP_Â·_INTP_Â·_INTJï¼šå››ç§çµé­‚ï¼Œå››ç§çˆ±æƒ…ç†æƒ³_å¤§èµæ_bilibili_BV12uUPBkEsZ_20251212_010216",
        "ã€å¹²è´§ã€‘éšèº«æºå¸¦æ ¸æ­¦å™¨çš„åº”ç”¨ä¸é£é™©ï¼_bilibili_BV1tmKozuEJ9_20251212_003402",
        "test_20251211_214039",
    ]
    
    imported_ids = []
    
    for dir_name in target_dirs:
        session_dir = output_dir / dir_name
        if session_dir.exists() and session_dir.is_dir():
            video_id = import_directory(session_dir)
            if video_id:
                imported_ids.append(video_id)
        else:
            print(f"\nâš ï¸  ç›®å½•ä¸å­˜åœ¨: {dir_name}")
    
    # æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å¯¼å…¥å®Œæˆ")
    print(f"{'='*60}")
    print(f"   æˆåŠŸå¯¼å…¥: {len(imported_ids)} ä¸ªè§†é¢‘")
    print(f"   è§†é¢‘ID: {', '.join(map(str, imported_ids))}")
    
    # æµ‹è¯•æœç´¢
    if imported_ids:
        repo = VideoRepository()
        test_search(repo)
    
    print(f"\n{'='*60}")
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*60}")
    print("\nğŸ’¡ åç»­æ“ä½œ:")
    print("   â€¢ æŸ¥çœ‹æ‰€æœ‰è§†é¢‘: make db-list")
    print("   â€¢ å…¨æ–‡æœç´¢: make search Q=\"INTP\"")
    print("   â€¢ æŒ‰æ ‡ç­¾æœç´¢: make search-tags TAGS=\"æ ‡ç­¾å\"")
    print("   â€¢ æŸ¥çœ‹æ•°æ®åº“çŠ¶æ€: make db-status")


if __name__ == "__main__":
    main()
