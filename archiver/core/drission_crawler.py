"""
åŸºäº DrissionPage çš„ç½‘é¡µå½’æ¡£å™¨
ä½¿ç”¨çœŸå®æµè§ˆå™¨ç¯å¢ƒï¼Œæ”¯æŒå¤æ‚çš„ JS æ¸²æŸ“å’Œç™»å½•æ€
"""

import os
import time
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

try:
    from DrissionPage import Chromium
    DRISSIONPAGE_AVAILABLE = True
except ImportError:
    DRISSIONPAGE_AVAILABLE = False
    logging.warning("DrissionPage not installed. Run: pip install DrissionPage")

try:
    import html2text
    HTML2TEXT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False
    logging.warning("html2text not installed. Run: pip install html2text")

from archiver.platforms.base import PlatformAdapter
from archiver.utils.url_parser import detect_platform
from archiver.utils.image_downloader import ImageDownloader
from archiver.utils.browser_manager import get_browser_manager


logger = logging.getLogger(__name__)


class DrissionArchiver:
    """åŸºäº DrissionPage çš„ç½‘é¡µå½’æ¡£å™¨"""
    
    def __init__(
        self,
        output_dir: str = "archived",
        browser_data_dir: str = "./browser_data",
        headless: bool = True,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–å½’æ¡£å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            browser_data_dir: æµè§ˆå™¨æ•°æ®ç›®å½•ï¼ˆå­˜å‚¨ Cookies å’Œç™»å½•æ€ï¼‰
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        if not DRISSIONPAGE_AVAILABLE:
            raise ImportError("Please install DrissionPage: pip install DrissionPage")
        
        if not HTML2TEXT_AVAILABLE:
            raise ImportError("Please install html2text: pip install html2text")
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.browser_data_dir = Path(browser_data_dir)
        self.browser_data_dir.mkdir(exist_ok=True)
        
        self.headless = headless
        self.verbose = verbose
        
        # è·å–æµè§ˆå™¨ç®¡ç†å™¨ï¼ˆå…¨å±€å•ä¾‹ï¼‰
        self.browser_manager = get_browser_manager()
        
        # é…ç½® HTML2Text
        self.converter = html2text.HTML2Text()
        self.converter.ignore_links = False
        self.converter.ignore_images = False
        self.converter.body_width = 0
        
        # å½“å‰ä»»åŠ¡çš„æ ‡ç­¾é¡µï¼ˆæ¯ä¸ªä»»åŠ¡ä¸€ä¸ª tabï¼‰
        self.current_tab = None
        
        # é…ç½®æ—¥å¿—
        if verbose:
            logging.basicConfig(level=logging.INFO)
    
    def _init_tab(self):
        """ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºæ–°æ ‡ç­¾é¡µï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        # è·å–å…¨å±€æµè§ˆå™¨å®ä¾‹
        browser = self.browser_manager.get_browser(
            browser_data_dir=str(self.browser_data_dir),
            headless=self.headless
        )
        
        # åˆ›å»ºæ–°æ ‡ç­¾é¡µï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        tab = self.browser_manager.new_tab()
        logger.info("âœ“ æ–°æ ‡ç­¾é¡µå·²åˆ›å»º")
        return tab
    
    def _close_tab(self):
        """å…³é—­å½“å‰ä»»åŠ¡çš„æ ‡ç­¾é¡µ"""
        if self.current_tab is not None:
            try:
                self.browser_manager.close_tab(self.current_tab)
                self.current_tab = None
            except Exception as e:
                logger.debug(f"å…³é—­æ ‡ç­¾é¡µæ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
                self.current_tab = None
    
    def _deduplicate_twitter_images(self, image_urls: list) -> list:
        """
        Twitter å›¾ç‰‡å»é‡ï¼šç§»é™¤åŒä¸€å›¾ç‰‡çš„ä¸åŒå°ºå¯¸ç‰ˆæœ¬
        
        Twitter å›¾ç‰‡ URL æ ¼å¼ï¼š
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=small
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=medium
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=large
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=orig
        
        ç­–ç•¥ï¼šåªä¿ç•™æ¯å¼ å›¾ç‰‡çš„æœ€å¤§å°ºå¯¸ç‰ˆæœ¬ï¼ˆä¼˜å…ˆçº§ï¼šorig > large > medium > smallï¼‰
        """
        if not image_urls:
            return image_urls
        
        # æŒ‰å›¾ç‰‡IDåˆ†ç»„
        image_groups = {}
        size_priority = {'orig': 4, 'large': 3, '4096x4096': 3, 'medium': 2, 'small': 1, '900x900': 1, '360x360': 0}
        
        for url in image_urls:
            if 'twimg.com/media/' in url:
                # æå–å›¾ç‰‡IDï¼ˆå»é™¤å‚æ•°ï¼‰
                base_url = url.split('?')[0]
                
                # æå–å°ºå¯¸å‚æ•°
                size = 'medium'  # é»˜è®¤
                if 'name=' in url:
                    import re
                    match = re.search(r'name=(\w+)', url)
                    if match:
                        size = match.group(1)
                
                # è®°å½•æˆ–æ›´æ–°æœ€å¤§å°ºå¯¸ç‰ˆæœ¬
                if base_url not in image_groups:
                    image_groups[base_url] = {'url': url, 'size': size, 'priority': size_priority.get(size, 0)}
                else:
                    current_priority = size_priority.get(size, 0)
                    if current_priority > image_groups[base_url]['priority']:
                        image_groups[base_url] = {'url': url, 'size': size, 'priority': current_priority}
            else:
                # é Twitter å›¾ç‰‡ï¼Œç›´æ¥ä¿ç•™
                image_groups[url] = {'url': url, 'size': 'unknown', 'priority': 999}
        
        # è¿”å›å»é‡åçš„ URL åˆ—è¡¨
        result = [item['url'] for item in image_groups.values()]
        
        if len(result) < len(image_urls):
            logger.info(f"Twitter å›¾ç‰‡å»é‡: {len(image_urls)} -> {len(result)} å¼ ï¼ˆç§»é™¤äº†é‡å¤å°ºå¯¸ï¼‰")
        
        return result
    
    def _load_manual_cookies(self, platform_name: str, url: str):
        """
        åŠ è½½æ‰‹åŠ¨é…ç½®çš„ Cookieï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
        Args:
            platform_name: å¹³å°åç§°ï¼ˆzhihu, xiaohongshu, bilibiliï¼‰
            url: ç›®æ ‡URL
        """
        # é…ç½®æ–‡ä»¶è·¯å¾„
        config_dir = Path(__file__).parent.parent / "config"
        cookie_file = config_dir / f"{platform_name}_drission_cookie.txt"
        
        if not cookie_file.exists():
            logger.debug(f"æœªæ‰¾åˆ°æ‰‹åŠ¨é…ç½®çš„ Cookie: {cookie_file}")
            return False
        
        try:
            # è¯»å– Cookie
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookie_string = f.read().strip()
            
            if not cookie_string:
                logger.warning(f"Cookie æ–‡ä»¶ä¸ºç©º: {cookie_file}")
                return False
            
            logger.info(f"åŠ è½½æ‰‹åŠ¨é…ç½®çš„ Cookie: {platform_name}")
            
            # ç¡®ä¿å·²è®¿é—®é¡µé¢ï¼ˆCookie éœ€è¦åŸŸåï¼‰
            if not self.current_tab.url or self.current_tab.url == 'about:blank':
                logger.info(f"é¦–æ¬¡è®¿é—®é¡µé¢ä»¥è®¾ç½® Cookie...")
                self.current_tab.get(url)
                time.sleep(1)
            
            # è§£æå¹¶è®¾ç½® Cookie
            # æ ¼å¼ï¼šname1=value1; name2=value2; ...
            cookie_pairs = [pair.strip() for pair in cookie_string.split(';') if pair.strip()]
            
            for pair in cookie_pairs:
                if '=' not in pair:
                    continue
                
                name, value = pair.split('=', 1)
                name = name.strip()
                value = value.strip()
                
                try:
                    # è®¾ç½® Cookie
                    self.current_tab.set.cookies({
                        'name': name,
                        'value': value,
                        'domain': self._get_cookie_domain(url),
                        'path': '/'
                    })
                    logger.debug(f"âœ“ è®¾ç½® Cookie: {name}")
                except Exception as e:
                    logger.warning(f"âœ— è®¾ç½® Cookie å¤±è´¥ {name}: {e}")
            
            logger.info(f"âœ“ æˆåŠŸåŠ è½½ {len(cookie_pairs)} ä¸ª Cookie")
            
            # åˆ·æ–°é¡µé¢ä½¿ Cookie ç”Ÿæ•ˆ
            logger.info("åˆ·æ–°é¡µé¢...")
            self.current_tab.refresh()
            time.sleep(1)
            
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ‰‹åŠ¨ Cookie å¤±è´¥: {e}")
            return False
    
    def _get_cookie_domain(self, url: str) -> str:
        """ä» URL æå– Cookie åŸŸå"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        # è¿”å› .domain.com æ ¼å¼
        domain_parts = parsed.netloc.split('.')
        if len(domain_parts) >= 2:
            return f".{'.'.join(domain_parts[-2:])}"
        return parsed.netloc
    

    
    def archive(
        self,
        url: str,
        platform_adapter: Optional[PlatformAdapter] = None,
        mode: str = "default",
        generate_report: bool = False,
        with_ocr: bool = False
    ) -> Dict[str, Any]:
        """
        å½’æ¡£æŒ‡å®šURLçš„ç½‘é¡µå†…å®¹
        
        Args:
            url: ç›®æ ‡URLï¼ˆæ”¯æŒåˆ†äº«æ–‡æœ¬æ ¼å¼ï¼Œä¼šè‡ªåŠ¨æå–URLï¼‰
            platform_adapter: å¹³å°é€‚é…å™¨ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
            mode: å½’æ¡£æ¨¡å¼ (default=åªæå–æ­£æ–‡/full=å®Œæ•´å†…å®¹å«è¯„è®º)
            generate_report: æ˜¯å¦ç”Ÿæˆ LLM ç»“æ„åŒ–æŠ¥å‘Š
            with_ocr: æ˜¯å¦å¯¹ä¸‹è½½çš„å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
        
        Returns:
            åŒ…å«å½’æ¡£ç»“æœçš„å­—å…¸
        """
        # ä»è¾“å…¥æ–‡æœ¬ä¸­æå– URLï¼ˆæ”¯æŒåˆ†äº«æ–‡æœ¬æ ¼å¼ï¼‰
        from archiver.utils.url_parser import extract_url_from_text
        original_input = url
        url = extract_url_from_text(url)
        
        if not url:
            return {
                'success': False,
                'error': 'æ— æ³•ä»è¾“å…¥ä¸­æå–æœ‰æ•ˆçš„URL',
                'input': original_input
            }
        
        # å¦‚æœæå–çš„URLä¸è¾“å…¥ä¸åŒï¼Œè®°å½•æ—¥å¿—
        if url != original_input:
            logger.info(f"ä»åˆ†äº«æ–‡æœ¬ä¸­æå–URL: {url}")
        
        logger.info(f"å¼€å§‹å½’æ¡£: {url}")
        
        # ä¸ºæ­¤ä»»åŠ¡åˆ›å»ºæ–°æ ‡ç­¾é¡µ
        self.current_tab = self._init_tab()
        
        try:
            # è‡ªåŠ¨æ£€æµ‹å¹³å°
            if platform_adapter is None:
                platform_name = detect_platform(url)
                logger.info(f"æ£€æµ‹å¹³å°: {platform_name} (æ¨¡å¼: {mode})")
                from archiver.platforms import (
                    ZhihuAdapter, XiaohongshuAdapter, BilibiliAdapter,
                    RedditAdapter, WordPressAdapter, TwitterAdapter
                )
                
                adapters = {
                    "zhihu": ZhihuAdapter(),
                    "xiaohongshu": XiaohongshuAdapter(),
                    "bilibili": BilibiliAdapter(),
                    "reddit": RedditAdapter(),
                    "twitter": TwitterAdapter(),
                    "wordpress": WordPressAdapter(),
                }
                platform_adapter = adapters.get(platform_name, WordPressAdapter())
            
            # è®¿é—®é¡µé¢ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            logger.info(f"æ­£åœ¨è®¿é—®: {url}")
            
            # å°è¯•åŠ è½½æ‰‹åŠ¨é…ç½®çš„ Cookie
            if platform_adapter.name in ['zhihu', 'xiaohongshu', 'bilibili', 'twitter']:
                self._load_manual_cookies(platform_adapter.name, url)
            
            # é‡è¯•è®¿é—®é¡µé¢
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    self.current_tab.get(url)
                    # æ™ºèƒ½ç­‰å¾…é¡µé¢åŠ è½½
                    self.current_tab.wait.load_start()
                    time.sleep(2)  # ç­‰å¾… JS æ‰§è¡Œ
                    break
                except Exception as e:
                    logger.warning(f"è®¿é—®é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        # é‡æ–°åˆ›å»ºæ ‡ç­¾é¡µ
                        self._close_tab()
                        self.current_tab = self._init_tab()
                    else:
                        raise RuntimeError(f"è®¿é—®é¡µé¢å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {e}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•ï¼ˆæ¨ç‰¹ç‰¹æ®Šå¤„ç†ï¼‰
            if platform_adapter.name == 'twitter':
                current_url = self.current_tab.url
                if 'login' in current_url or 'i/flow/login' in current_url:
                    logger.warning("âš ï¸  æ¨ç‰¹éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹å†…å®¹")
                    logger.info("ğŸ’¡ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ç™»å½•æ¨ç‰¹ï¼š")
                    logger.info("   make login-twitter")
                    logger.info("   æˆ–è€…è®¿é—® https://twitter.com æ‰‹åŠ¨ç™»å½•")
                    return {
                        "success": False,
                        "error": "æ¨ç‰¹éœ€è¦ç™»å½•ã€‚è¯·è¿è¡Œ 'make login-twitter' ç™»å½•è´¦å·",
                        "url": url
                    }
            
            # æ»šåŠ¨é¡µé¢ç¡®ä¿æ‡’åŠ è½½å†…å®¹åŠ è½½å®Œæˆ
            logger.info("æ»šåŠ¨é¡µé¢åŠ è½½æ‡’åŠ è½½å†…å®¹...")
            self.current_tab.scroll.to_bottom()
            time.sleep(1)
            self.current_tab.scroll.to_top()
            time.sleep(1)
            
            # è·å–é¡µé¢æ ‡é¢˜
            page_title = self.current_tab.title
            if not page_title:
                page_title = "Untitled"
            
            # ğŸ†• æå‰æå–å›¾ç‰‡URLï¼ˆä»å®Œæ•´é¡µé¢ï¼‰
            full_page_html = self.current_tab.html
            logger.info("ä»å®Œæ•´é¡µé¢æå–å›¾ç‰‡URL...")
            
            # æå–å†…å®¹
            content_html = self._extract_content(platform_adapter, mode=mode)
            
            if not content_html:
                return {
                    "success": False,
                    "error": "æ— æ³•æå–é¡µé¢å†…å®¹",
                    "url": url
                }
            
            # è½¬æ¢ä¸º Markdown
            markdown_content = self._convert_to_markdown(
                html=content_html,
                title=page_title,
                url=url,
                platform=platform_adapter.name,
                mode=mode
            )
            
            # æå–çº¯å†…å®¹ï¼ˆå»æ‰ YAML frontmatterï¼‰
            content_lines = markdown_content.split('\n')
            content_start = 0
            
            # è·³è¿‡ YAML frontmatter
            if content_lines and content_lines[0].strip() == '---':
                for i, line in enumerate(content_lines[1:], 1):
                    if line.strip() == '---':
                        content_start = i + 1
                        break
            
            # çº¯OCRå†…å®¹ï¼ˆä¸å«å…ƒä¿¡æ¯ï¼‰
            pure_ocr_content = '\n'.join(content_lines[content_start:]).strip()
            
            # åˆ›å»ºæ–‡ä»¶å¤¹
            folder_name = self._generate_folder_name(page_title, platform_adapter.name)
            folder_path = self.output_dir / folder_name
            folder_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½å›¾ç‰‡
            logger.info("å¼€å§‹ä¸‹è½½å›¾ç‰‡...")
            image_downloader = ImageDownloader(
                output_dir=str(folder_path / "images"),
                format="jpg"
            )
            
            # æå–å›¾ç‰‡URL
            # é»˜è®¤åªä»å†…å®¹æå–ï¼Œç‰¹æ®Šæƒ…å†µï¼ˆå¦‚æ— æ³•æå–åˆ°å†…å®¹å›¾ç‰‡ï¼‰æ‰ä»å…¨é¡µæå–
            image_urls = image_downloader.extract_image_urls(content_html, url)
            
            # æ¨ç‰¹ç‰¹æ®Šå¤„ç†ï¼šå®Œæ•´æ¨¡å¼ä¸‹ï¼Œæˆ–è€…å†…å®¹æå–ä¸åˆ°å›¾ç‰‡æ—¶ï¼Œå°è¯•ä»å®Œæ•´é¡µé¢æå–
            if platform_adapter.name == 'twitter':
                if mode == 'full' or not image_urls:
                    logger.info("æ¨ç‰¹ï¼šå°è¯•ä»å®Œæ•´é¡µé¢æå–å›¾ç‰‡...")
                    more_urls = image_downloader.extract_image_urls(full_page_html, url)
                    image_urls = list(set(image_urls + more_urls))
                
                # è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹æå–çš„å›¾ç‰‡ URL
                logger.debug(f"åŸå§‹æå–çš„å›¾ç‰‡ URLs: {image_urls}")
            
            # è¿‡æ»¤å›¾ç‰‡ï¼ˆé’ˆå¯¹é»˜è®¤æ¨¡å¼ï¼‰
            if image_urls and mode == 'default':
                # æ¨ç‰¹ï¼šç§»é™¤å¤´åƒå’Œè¡¨æƒ…åŒ…ï¼Œåªä¿ç•™åª’ä½“å›¾ç‰‡
                if platform_adapter.name == 'twitter':
                    filtered_urls = []
                    for img_url in image_urls:
                        # æ’é™¤å¤´åƒ (profile_images)
                        if 'profile_images' in img_url:
                            continue
                        # æ’é™¤å°å›¾æ ‡/è¡¨æƒ… (emoji)
                        if 'emoji' in img_url:
                            continue
                        filtered_urls.append(img_url)
                    
                    if len(filtered_urls) < len(image_urls):
                        logger.info(f"è¿‡æ»¤äº† {len(image_urls) - len(filtered_urls)} å¼ æ— å…³å›¾ç‰‡ï¼ˆå¤´åƒ/è¡¨æƒ…ï¼‰")
                    image_urls = filtered_urls
                
                # Twitter å›¾ç‰‡å»é‡ï¼ˆç§»é™¤åŒä¸€å›¾ç‰‡çš„ä¸åŒå°ºå¯¸ç‰ˆæœ¬ï¼‰
                if platform_adapter.name == 'twitter':
                    image_urls = self._deduplicate_twitter_images(image_urls)
            
            url_mapping = {}
            if image_urls:
                logger.info(f"å‘ç° {len(image_urls)} å¼ å›¾ç‰‡")
                url_mapping = image_downloader.download_all(image_urls, referer=url)
                
                # æ›´æ–°markdownä¸­çš„å›¾ç‰‡é“¾æ¥
                if url_mapping:
                    for orig_url, local_path in url_mapping.items():
                        rel_path = f"images/{local_path}"
                        markdown_content = markdown_content.replace(orig_url, rel_path)
                    logger.info(f"å·²æ›´æ–° {len(url_mapping)} ä¸ªå›¾ç‰‡é“¾æ¥")
            
            # 1. ä¿å­˜ archive_raw.mdï¼ˆç½‘é¡µå†…å®¹+å›¾ç‰‡å¼•ç”¨ï¼Œä¸å«å…ƒä¿¡æ¯ï¼‰
            archive_raw_path = folder_path / "archive_raw.md"
            with open(archive_raw_path, "w", encoding="utf-8") as f:
                # æ›´æ–°å›¾ç‰‡é“¾æ¥åçš„çº¯å†…å®¹
                updated_pure_content = pure_ocr_content
                if url_mapping:
                    for orig_url, local_path in url_mapping.items():
                        rel_path = f"images/{local_path}"
                        updated_pure_content = updated_pure_content.replace(orig_url, rel_path)
                f.write(updated_pure_content)
            logger.info(f"âœ… ä¿å­˜ç½‘é¡µå½’æ¡£åŸå§‹æ•°æ®: {archive_raw_path.name}")
            
            # 1.5 å¦‚æœå¯ç”¨OCRï¼Œå¯¹å›¾ç‰‡è¿›è¡Œè¯†åˆ«
            ocr_text = ""
            if with_ocr and image_urls and url_mapping:
                logger.info(">> å¼€å§‹OCRè¯†åˆ«å›¾ç‰‡...")
                ocr_text = self._perform_ocr_on_images(folder_path / "images")
                
                if ocr_text.strip():
                    ocr_raw_path = folder_path / "ocr_raw.md"
                    with open(ocr_raw_path, "w", encoding="utf-8") as f:
                        ocr_content = f"# ğŸ” å›¾ç‰‡OCRè¯†åˆ«ç»“æœ\n\n"
                        ocr_content += f"**è¯†åˆ«æ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  \n"
                        ocr_content += f"**å›¾ç‰‡æ•°é‡**: {len(url_mapping)}  \n"
                        ocr_content += f"**è¯†åˆ«å­—ç¬¦æ•°**: {len(ocr_text)}  \n\n"
                        ocr_content += "---\n\n"
                        ocr_content += ocr_text
                        f.write(ocr_content)
                    logger.info(f"âœ… ä¿å­˜OCRè¯†åˆ«ç»“æœ: {ocr_raw_path.name} ({len(ocr_text)} å­—ç¬¦)")
                else:
                    logger.info("â„¹ï¸  æœªè¯†åˆ«åˆ°æ–‡å­—")
            
            # 2. ä¿å­˜ README.mdï¼ˆå…ƒä¿¡æ¯ + å¼•ç”¨ï¼‰
            readme_content = f"""---
title: {page_title}
url: {url}
platform: {platform_adapter.name}
archived_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
---

# {page_title}

**æ¥æº**: [{url}]({url})  
**å¹³å°**: {platform_adapter.name}  
**å½’æ¡£æ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## ğŸ“„ åŸå§‹å†…å®¹

è¯¦è§ [archive_raw.md](archive_raw.md)ï¼ˆç½‘é¡µå†…å®¹+å›¾ç‰‡ï¼‰

---

> ğŸ’¡ **æç¤º**: ä½¿ç”¨ `report.md` æŸ¥çœ‹ LLM å¤„ç†åçš„ç»“æ„åŒ–å†…å®¹
"""
            
            md_filename = "README.md"
            md_path = folder_path / md_filename
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(readme_content)
            
            logger.info(f"å½’æ¡£æˆåŠŸ: {folder_path}")
            logger.info(f"  - README: {md_path.name}")
            logger.info(f"  - å½’æ¡£åŸå§‹: {archive_raw_path.name}")
            if image_urls:
                logger.info(f"  - å›¾ç‰‡: {len(url_mapping)}/{len(image_urls)} å¼ ")
            
            # 3. ç”Ÿæˆ report.mdï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
            if generate_report:
                logger.info(">> ä½¿ç”¨ LLM ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š...")
                report_content = self._generate_report_with_llm(
                    archive_content=updated_pure_content,
                    title=page_title,
                    url=url,
                    platform=platform_adapter.name
                )
                
                if report_content:
                    report_path = folder_path / "report.md"
                    with open(report_path, "w", encoding="utf-8") as f:
                        f.write(report_content)
                    logger.info(f"âœ… ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š: {report_path.name}")
            else:
                logger.info("â„¹ï¸  è·³è¿‡ report.md ç”Ÿæˆï¼ˆä½¿ç”¨ --generate-report å¯ç”¨ï¼‰")
            
            # ä½¿ç”¨ LLM ç”Ÿæˆè¯­ä¹‰åŒ–çš„æ–‡ä»¶å¤¹åç§°å¹¶é‡å‘½å
            logger.info(">> ä½¿ç”¨ LLM ç”Ÿæˆè¯­ä¹‰åŒ–æ–‡ä»¶å¤¹å...")
            new_folder_name = self._generate_folder_name_with_llm(
                markdown_content=markdown_content,
                title=page_title,
                platform=platform_adapter.name,
                url=url
            )
            
            # å¦‚æœç”Ÿæˆçš„åç§°ä¸å½“å‰æ–‡ä»¶å¤¹åä¸åŒï¼Œåˆ™é‡å‘½å
            current_folder_name = folder_path.name
            if new_folder_name != folder_name:
                new_folder_path = self.output_dir / new_folder_name
                try:
                    # å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
                    if new_folder_path.exists():
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        new_folder_name = f"{new_folder_name}_{timestamp}"
                        new_folder_path = self.output_dir / new_folder_name
                    
                    folder_path.rename(new_folder_path)
                    folder_path = new_folder_path  # æ›´æ–°å¼•ç”¨
                    md_path = folder_path / md_filename
                    logger.info(f"âœ… æ–‡ä»¶å¤¹å·²é‡å‘½åä¸º: {folder_path.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹é‡å‘½åå¤±è´¥: {e}ï¼Œä¿æŒåŸåç§°: {current_folder_name}")
            
            return {
                "success": True,
                "url": url,
                "platform": platform_adapter.name,
                "output_path": str(folder_path),
                "markdown_path": str(md_path),
                "title": page_title,
                "content_length": len(markdown_content),
                "images_downloaded": len(url_mapping) if image_urls else 0,
                "images_total": len(image_urls) if image_urls else 0,
                "ocr_enabled": with_ocr,
                "ocr_text_length": len(ocr_text) if with_ocr else 0
            }
            
        except Exception as e:
            logger.error(f"å½’æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": url
            }
        finally:
            # ä»»åŠ¡ç»“æŸï¼Œå…³é—­æ ‡ç­¾é¡µï¼ˆæµè§ˆå™¨ä¿æŒè¿è¡Œï¼‰
            self._close_tab()
            # æ³¨æ„ï¼šæµè§ˆå™¨ä¼šåœ¨ç¨‹åºé€€å‡ºæ—¶é€šè¿‡ atexit è‡ªåŠ¨å…³é—­
    
    def _extract_content(self, platform_adapter: PlatformAdapter, mode: str = "default") -> str:
        """
        æå–é¡µé¢å†…å®¹
        
        Args:
            platform_adapter: å¹³å°é€‚é…å™¨
            mode: å½’æ¡£æ¨¡å¼ ('default' æˆ– 'full')
                  default: ä»…ä¿ç•™æ­£æ–‡å’Œå›¾ç‰‡ï¼Œç§»é™¤è¯„è®ºã€ä¾§è¾¹æ ç­‰æ— å…³å…ƒç´ 
                  full: ä¿ç•™é€‰å®šå®¹å™¨å†…çš„æ‰€æœ‰å†…å®¹
        """
        selector = platform_adapter.content_selector
        exclude_selector = platform_adapter.exclude_selector if hasattr(platform_adapter, 'exclude_selector') else ""
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦ç™»å½•ï¼ˆå¸¸è§ç™»å½•æç¤ºï¼‰
        login_indicators = [
            "ç™»å½•åæ¨è",
            "é©¬ä¸Šç™»å½•",
            "è¯·å…ˆç™»å½•",
            "Sign in",
            "Log in",
            "ç™»å…¥"
        ]
        
        page_text = self.current_tab.html
        for indicator in login_indicators:
            if indicator in page_text:
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹ï¼ˆç™»å½•æç¤ºé€šå¸¸æ–‡æœ¬å¾ˆçŸ­ï¼‰
                if len(self.current_tab.ele('body', timeout=1).text.strip()) < 500:
                    logger.warning(f"âš ï¸  æ£€æµ‹åˆ°ç™»å½•æç¤º: {indicator}")
                    logger.warning("   å»ºè®®æ“ä½œï¼š")
                    logger.warning("   1. è¿è¡Œ 'make login' ç™»å½•å¹¶ä¿å­˜ç™»å½•æ€")
                    logger.warning("   2. æˆ–è¿è¡Œ 'make config-drission-cookie' æ‰‹åŠ¨é…ç½®Cookie")
                    break
        
        # å°è¯•ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹
        if platform_adapter.name == 'twitter' and mode == 'default':
            try:
                logger.info("Twitter: å°è¯•æ„å»ºçº¯å‡€å†…å®¹ (Text + Photos)...")
                
                # Manual finding of article to avoid selector issues
                articles = self.current_tab.eles('tag:article')
                article = None
                for a in articles:
                    if a.attrs.get('data-testid') == 'tweet':
                        article = a
                        break
                
                if article:
                    logger.info("  - æ‰¾åˆ°ä¸»æ¨æ–‡å®¹å™¨ article[data-testid='tweet']")
                    parts = []
                    parts = []
                    # 1. æå–æ­£æ–‡ - Try CSS first, then XPath
                    text_div = article.ele("[data-testid='tweetText']")
                    if not text_div:
                        logger.warning("  - CSSæ‰¾ä¸tweetText, å°è¯•XPath...")
                        text_div = article.ele("xpath:.//*[@data-testid='tweetText']")
                    
                    if text_div:
                        parts.append(text_div.html)
                        logger.info(f"  - æ‰¾åˆ°æ¨æ–‡æ­£æ–‡ (é•¿åº¦: {len(text_div.text)})")
                    else:
                        logger.warning("  - âŒ æœªæ‰¾åˆ°æ¨æ–‡æ­£æ–‡ [data-testid='tweetText']")
                    
                    # 2. æå–å›¾ç‰‡å®¹å™¨
                    # Try CSS first, then XPath, then manual scan
                    photos = article.eles("[data-testid='tweetPhoto']")
                    if not photos:
                        logger.info("  - CSSæœªæ‰¾åˆ°å›¾ç‰‡, å°è¯•XPath...")
                        photos = article.eles("xpath:.//*[@data-testid='tweetPhoto']")
                    
                    if photos:
                        logger.info(f"  - æ‰¾åˆ° {len(photos)} ä¸ªå›¾ç‰‡å®¹å™¨")
                        for p in photos:
                            html_part = p.html
                            # Ensure high res images in HTML to match downloader logic
                            if 'name=' in html_part:
                                import re
                                html_part = re.sub(r'name=(small|medium|360x360|900x900)', 'name=large', html_part)
                            parts.append(html_part)
                    else:
                        logger.info("  - âŒ æœªæ‰¾åˆ°å›¾ç‰‡å®¹å™¨ (tweetPhoto)")
                        # Fallback: Find all images in article and filter avatars
                        imgs = article.eles("tag:img")
                        valid_imgs = []
                        for img in imgs:
                            src = img.attrs.get('src', '')
                            if 'profile_images' in src or 'emoji' in src:
                                continue
                            if src:
                                # Wrap in simple img tag if container not found
                                valid_imgs.append(f'<img src="{src}" />')
                        
                        if valid_imgs:
                             logger.info(f"  -ç”±äºæœªæ‰¾åˆ°å®¹å™¨ï¼Œç›´æ¥æå–äº† {len(valid_imgs)} å¼ æ­£æ–‡å›¾ç‰‡")
                             parts.extend(valid_imgs)
                            
                    if parts:
                        combined_html = "\n<br>\n".join(parts)
                        return combined_html
                else:
                    logger.warning("  - âŒ æœªæ‰¾åˆ°ä¸»æ¨æ–‡å®¹å™¨ article[data-testid='tweet']")
                    # DEBUG: Check what articles actually exist
                    arts = self.current_tab.eles('tag:article')
                    logger.info(f"DEBUG: Found {len(arts)} generic articles in Crawler Session")
                    for i, a in enumerate(arts[:3]):
                        logger.info(f"DEBUG Art {i} Attrs: {a.attrs}")
                    
                    # DEBUG: Check title again
                    logger.info(f"DEBUG Page Title: {self.current_tab.title}")

            except Exception as e:
                logger.warning(f"Twitter çº¯å‡€æå–å¤±è´¥: {e}, å°†å°è¯•é€šç”¨é€‰æ‹©å™¨")
                import traceback
                logger.warning(traceback.format_exc())

        if selector:
            for sel in selector.split(','):
                sel = sel.strip()
                element = self.current_tab.ele(sel, timeout=2)
                if element:
                    # å¦‚æœä¸æ˜¯å…¨é‡æ¨¡å¼ï¼Œä¸”å®šä¹‰äº†æ’é™¤é€‰æ‹©å™¨ï¼Œå°è¯•ç§»é™¤æ— å…³å…ƒç´ 
                    # æ³¨æ„ï¼šDrissionPage çš„å…ƒç´ æ“ä½œé€šå¸¸æ˜¯å³æ—¶çš„ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥æ“ä½œé¡µé¢ä¸Šçš„å…ƒç´ 
                    # ä½†ä¸ºäº†ä¸ç ´åé¡µé¢ç»“æ„å½±å“åç»­ï¼ˆè™½ç„¶æˆ‘ä»¬å¾ˆå¿«å°±å…³é—­ï¼‰ï¼Œæˆ–è€…ä¸ºäº†å¤„ç†æ–¹ä¾¿
                    # æˆ‘ä»¬ä¸»è¦é€šè¿‡ BeautifulSoup åå¤„ç†ï¼Œæˆ–è€…åœ¨è¿™é‡Œå°è¯•ç§»é™¤
                    
                    if mode == "default" and exclude_selector:
                        logger.info(f"æ¸…ç†æ¨¡å¼: ç§»é™¤æ— å…³å…ƒç´ ")
                        
                        # 1. ç§»é™¤é…ç½®ä¸­å®šä¹‰çš„å…ƒç´ 
                        for exclude in exclude_selector.split(','):
                            exclude = exclude.strip()
                            if not exclude:
                                continue
                            
                            try:
                                unwanted_elements = element.eles(exclude)
                                removed_count = 0
                                for unwanted in unwanted_elements:
                                    self.current_tab.run_js("arguments[0].remove()", unwanted)
                                    removed_count += 1
                                if removed_count > 0:
                                    logger.info(f"  - å·²ç§»é™¤ {removed_count} ä¸ª {exclude} å…ƒç´ ")
                            except Exception as e:
                                logger.debug(f"  - ç§»é™¤ {exclude} å¤±è´¥: {e}")
                        
                        # 2. å¯¹äºå°çº¢ä¹¦ï¼Œé¢å¤–ç§»é™¤ä½œè€…ä¿¡æ¯å’Œå…³æ³¨æŒ‰é’®
                        if platform_adapter.name == "xiaohongshu":
                            # ç§»é™¤ç”¨æˆ·profileé“¾æ¥ï¼ˆä½œè€…å¤´åƒå’Œåå­—ï¼‰
                            try:
                                profile_links = element.eles('a[href*="/user/profile"]')
                                if profile_links:
                                    logger.info(f"  - å·²ç§»é™¤ {len(profile_links)} ä¸ªç”¨æˆ·profileé“¾æ¥")
                                    for link in profile_links:
                                        self.current_tab.run_js("arguments[0].remove()", link)
                            except Exception:
                                pass
                            
                            # ç§»é™¤"å…³æ³¨"æŒ‰é’® - é€šè¿‡æ–‡å­—å†…å®¹åŒ¹é…
                            try:
                                all_elements = element.eles('tag:div') + element.eles('tag:button')
                                follow_count = 0
                                for elem in all_elements:
                                    if elem.text and elem.text.strip() == 'å…³æ³¨':
                                        self.current_tab.run_js("arguments[0].remove()", elem)
                                        follow_count += 1
                                if follow_count > 0:
                                    logger.info(f"  - å·²ç§»é™¤ {follow_count} ä¸ªå…³æ³¨æŒ‰é’®")
                            except Exception:
                                pass
                    
                    # é‡æ–°è·å– HTML (ç§»é™¤å…ƒç´ å)
                    html = element.html
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
                    if html and len(html) > 1000:
                        logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹: {sel}")
                        return html
        
        # å›é€€ï¼šä½¿ç”¨é€šç”¨é€‰æ‹©å™¨
        for fallback in ['article', 'main', 'body']:
            element = self.current_tab.ele(fallback, timeout=2)
            if element:
                logger.info(f"ä½¿ç”¨å›é€€é€‰æ‹©å™¨: {fallback}")
                return element.html
        
        # æœ€åçš„å›é€€ï¼šæ•´ä¸ªé¡µé¢
        logger.warning("ä½¿ç”¨æ•´ä¸ªé¡µé¢ä½œä¸ºå†…å®¹")
        return self.current_tab.html
    
    def _convert_to_markdown(
        self,
        html: str,
        title: str,
        url: str,
        platform: str,
        mode: str = "default"
    ) -> str:
        """å°† HTML è½¬æ¢ä¸º Markdown"""
        # æ·»åŠ å…ƒæ•°æ®å¤´éƒ¨
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        metadata = f"""---
title: {title}
url: {url}
platform: {platform}
archived_at: {timestamp}
---

"""
        
        # è½¬æ¢ HTML
        markdown_content = self.converter.handle(html)
        
        # å¦‚æœæ˜¯é»˜è®¤æ¨¡å¼ï¼Œåšé¢å¤–çš„ Markdown æ¸…æ´—
        if mode == "default":
            import re
            # å°çº¢ä¹¦ï¼šç§»é™¤ç”¨æˆ·profileé“¾æ¥
            if platform == "xiaohongshu":
                # ç§»é™¤ç”¨æˆ·profileé“¾æ¥ (æ ¼å¼: [![...](images/...jpg)](/user/profile/...)æ–‡å­—)
                markdown_content = re.sub(
                    r'\[!\[.*?\]\(images/.*?\)\]\(/user/profile/[^\)]+\)\[.*?\]\(/user/profile/[^\)]+\)\s*',
                    '',
                    markdown_content
                )
                # ç§»é™¤å•ç‹¬çš„ç”¨æˆ·é“¾æ¥ (æ ¼å¼: [ç”¨æˆ·å](/user/profile/...))
                markdown_content = re.sub(
                    r'\[.*?\]\(/user/profile/[^\)]+\)\s*',
                    '',
                    markdown_content
                )
                # ç§»é™¤å•ç‹¬çš„"å…³æ³¨"æ–‡å­—
                markdown_content = re.sub(r'^\s*å…³æ³¨\s*$', '', markdown_content, flags=re.MULTILINE)
            
            # æ¨ç‰¹ï¼šç§»é™¤ç”¨æˆ·profileé“¾æ¥å’Œäº’åŠ¨æŒ‰é’®
            elif platform == "twitter":
                # ç§»é™¤ç”¨æˆ·profileé“¾æ¥ (/@username)
                markdown_content = re.sub(
                    r'\[@[^\]]+\]\(/[^\)]+\)\s*',
                    '',
                    markdown_content
                )
                # ç§»é™¤äº’åŠ¨æ•°å­—ï¼ˆè½¬æ¨ã€ç‚¹èµç­‰ï¼‰
                markdown_content = re.sub(
                    r'^\s*\d+\s*(Retweets?|Likes?|Replies?|Views?)\s*$',
                    '',
                    markdown_content,
                    flags=re.MULTILINE
                )
            
            # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        
        return metadata + markdown_content
    
    def _generate_folder_name(self, title: str, platform: str) -> str:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼ˆä»…ä½¿ç”¨æ ‡é¢˜ï¼Œä¸åŒ…å«æ¥æºï¼‰"""
        import re
        
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤éæ³•å­—ç¬¦
        clean_title = re.sub(r'[<>:"/\\|?*]', '', title)
        clean_title = clean_title.strip()
        
        # ç§»é™¤æœ«å°¾çš„æ¥æºæ ‡è¯†ï¼ˆå¦‚"- å°çº¢ä¹¦"ã€"- çŸ¥ä¹"ç­‰ï¼‰
        # åŒ¹é…æ¨¡å¼ï¼š" - å¹³å°åç§°" æˆ– " - æ¥æº"
        clean_title = re.sub(r'\s*-\s*(å°çº¢ä¹¦|çŸ¥ä¹|Bç«™|å“”å“©å“”å“©|Reddit|wordpress|ç½‘ç«™|ç¤¾åŒº).*$', '', clean_title)
        clean_title = clean_title.strip()
        
        # é™åˆ¶é•¿åº¦
        if len(clean_title) > 60:
            clean_title = clean_title[:60]
        
        # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨æ—¶é—´æˆ³
        if not clean_title or clean_title == "Untitled":
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            return f"{platform}_{timestamp}"
        
        return clean_title
    
    def _generate_folder_name_with_llm(self, markdown_content: str, title: str, platform: str, url: str) -> str:
        """
        ä½¿ç”¨ openai/gpt-oss-20b æ¨¡å‹æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´çš„æ–‡ä»¶å¤¹åç§°
        
        Args:
            markdown_content: ä¿å­˜çš„ Markdown å†…å®¹
            title: åŸå§‹é¡µé¢æ ‡é¢˜
            platform: å¹³å°åç§°
            url: åŸå§‹ URL
        
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶å¤¹åç§°ï¼ˆé•¿åº¦é™åˆ¶åœ¨50ä¸ªå­—ç¬¦ä»¥å†…ï¼‰
        """
        if not GROQ_AVAILABLE:
            logger.warning("Groq SDK æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
            return self._generate_folder_name(title, platform)
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            logger.warning("GROQ_API_KEY æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
            return self._generate_folder_name(title, platform)
        
        try:
            client = Groq(api_key=api_key)
            
            # æå–å†…å®¹æ‘˜è¦
            content_lines = markdown_content.split('\n')
            content_start = 0
            
            # è·³è¿‡ YAML frontmatter
            if content_lines and content_lines[0].strip() == '---':
                for i, line in enumerate(content_lines[1:], 1):
                    if line.strip() == '---':
                        content_start = i + 1
                        break
            
            # è·å–å®é™…å†…å®¹
            actual_content = '\n'.join(content_lines[content_start:])
            # ç§»é™¤å›¾ç‰‡é“¾æ¥
            import re
            actual_content = re.sub(r'!\[.*?\]\(.*?\)', '', actual_content)
            # é™åˆ¶é•¿åº¦åˆ°å‰800å­—ç¬¦
            content_summary = actual_content[:800].strip()
            
            if not content_summary or len(content_summary) < 20:
                logger.warning("å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
                return self._generate_folder_name(title, platform)
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶å‘½åä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´ã€è¯­ä¹‰åŒ–çš„æ–‡ä»¶å¤¹åç§°ã€‚

## è¾“å…¥ä¿¡æ¯

**ç½‘é¡µæ ‡é¢˜**: {title}  
**å¹³å°**: {platform}  
**URL**: {url}

**å†…å®¹æ‘˜è¦**:
{content_summary}

## å‘½åè¦æ±‚

### å†…å®¹è¦æ±‚
1. **æ ¸å¿ƒä¸»é¢˜æç‚¼**: å‡†ç¡®æ•æ‰å†…å®¹çš„æ ¸å¿ƒæ¦‚å¿µæˆ–å…³é”®ä¸»é¢˜
2. **ä¿¡æ¯å¯†åº¦**: åœ¨æœ‰é™å­—ç¬¦å†…ä¼ è¾¾æœ€å¤§ä¿¡æ¯é‡
3. **è¯­ä¹‰æ¸…æ™°**: è®©ç”¨æˆ·ä¸€çœ¼å°±èƒ½ç†è§£æ–‡ä»¶å¤¹å†…å®¹
4. **åŒºåˆ†åº¦é«˜**: é¿å…ä½¿ç”¨è¿‡äºé€šç”¨çš„è¯æ±‡ï¼ˆå¦‚"ç¬”è®°"ã€"æ–‡ç« "ç­‰ï¼‰

### æ ¼å¼è¦æ±‚
1. **åˆ†éš”ç¬¦**: ä½¿ç”¨ä¸‹åˆ’çº¿ `_` åˆ†éš”å•è¯ï¼Œç¦æ­¢ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
2. **é•¿åº¦é™åˆ¶**: ä¸­æ–‡ä¸è¶…è¿‡15ä¸ªå­—ï¼Œè‹±æ–‡ä¸è¶…è¿‡30ä¸ªå­—ç¬¦
3. **å­—ç¬¦é™åˆ¶**: ä»…ä½¿ç”¨ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œç¦æ­¢ `<>:"/\\|?*`
4. **å¤§å°å†™**: è‹±æ–‡å•è¯é¦–å­—æ¯å¤§å†™ï¼ˆå¦‚ Python_Best_Practicesï¼‰
5. **å¹³å°å**: ä¸éœ€è¦åŒ…å«å¹³å°åç§°ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ ï¼‰

### ä¼˜å…ˆçº§è§„åˆ™
- å¦‚æœæ˜¯æ•™ç¨‹/æŒ‡å—ï¼šçªå‡ºä¸»é¢˜+ç›®æ ‡ï¼ˆå¦‚ "PyTorchå›¾åƒåˆ†ç±»å®æˆ˜"ï¼‰
- å¦‚æœæ˜¯é—®ç­”/è®¨è®ºï¼šæç‚¼æ ¸å¿ƒé—®é¢˜ï¼ˆå¦‚ "å¦‚ä½•ä¼˜åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦"ï¼‰
- å¦‚æœæ˜¯å·¥å…·/èµ„æºï¼šå¼ºè°ƒå·¥å…·å+ç”¨é€”ï¼ˆå¦‚ "Crawl4AIç½‘é¡µæŠ“å–"ï¼‰
- å¦‚æœæ˜¯è§‚ç‚¹/åˆ†æï¼šæç‚¼æ ¸å¿ƒè®ºç‚¹ï¼ˆå¦‚ "Agentè®¾è®¡äº”å¤§æ¨¡å¼"ï¼‰

## ä¼˜ç§€ç¤ºä¾‹

- âœ… `PyTorchå›¾åƒåˆ†ç±»å®æˆ˜`ï¼ˆæ¸…æ™°ã€å…·ä½“ã€æœ‰æ“ä½œæ€§ï¼‰
- âœ… `Agentè®¾è®¡äº”å¤§æ¨¡å¼`ï¼ˆæç‚¼æ ¸å¿ƒæ•°å­—+ä¸»é¢˜ï¼‰
- âœ… `LLM_Token_ä¼˜åŒ–æŠ€å·§`ï¼ˆæŠ€æœ¯æœ¯è¯­+å®ç”¨æ€§ï¼‰
- âŒ `æœºå™¨å­¦ä¹ ç¬”è®°`ï¼ˆè¿‡äºç¬¼ç»Ÿï¼‰
- âŒ `Pythonæ•™ç¨‹ç¬¬ä¸€ç« `ï¼ˆç¼ºä¹è¯­ä¹‰ï¼‰
- âŒ `ä»Šå¤©çœ‹åˆ°çš„æ–‡ç« `ï¼ˆæ— ä»»ä½•ä¿¡æ¯ä»·å€¼ï¼‰

## è¾“å‡ºæ ¼å¼

**ä»…è¿”å›æ–‡ä»¶å¤¹åç§°æœ¬èº«**ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€å¼•å·ã€åºå·æˆ–å…¶ä»–æ–‡å­—ã€‚

æ–‡ä»¶å¤¹åç§°ï¼š"""

            response = client.chat.completions.create(
                model="openai/gpt-oss-20b",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶å‘½ååŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´ã€æè¿°æ€§çš„æ–‡ä»¶å¤¹åç§°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=50,
                temperature=0.3,
            )
            
            folder_name = response.choices[0].message.content.strip()
            
            # æ¸…ç†æ–‡ä»¶å¤¹åç§°
            folder_name = re.sub(r'["\'\'\n\r\t]', '', folder_name)
            folder_name = re.sub(r'[/\\\\]', '_', folder_name)
            folder_name = re.sub(r'[<>:"|?*]', '', folder_name)
            
            # é™åˆ¶é•¿åº¦
            if len(folder_name) > 50:
                folder_name = folder_name[:50]
            
            # å¦‚æœç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜
            if not folder_name or len(folder_name) < 3:
                logger.warning("LLM ç”Ÿæˆçš„æ–‡ä»¶å¤¹åæ— æ•ˆï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜")
                return self._generate_folder_name(title, platform)
            
            logger.info(f"âœ… LLM ç”Ÿæˆçš„æ–‡ä»¶å¤¹å: {folder_name}")
            return folder_name
            
        except Exception as e:
            logger.warning(f"LLM æ–‡ä»¶å¤¹å‘½åå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åç§°")
            return self._generate_folder_name(title, platform)
    
    def _generate_report_with_llm(
        self,
        archive_content: str,
        title: str,
        url: str,
        platform: str
    ) -> Optional[str]:
        """
        ä½¿ç”¨ LLM å°†ç½‘é¡µå½’æ¡£å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–æŠ¥å‘Š
        
        Args:
            archive_content: ç½‘é¡µå½’æ¡£åŸå§‹å†…å®¹ï¼ˆæ¥è‡ª archive_raw.mdï¼‰
            title: ç½‘é¡µæ ‡é¢˜
            url: åŸå§‹ URL
            platform: å¹³å°åç§°
        
        Returns:
            ç”Ÿæˆçš„ Markdown æŠ¥å‘Šå†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        if not GROQ_AVAILABLE:
            logger.warning("Groq SDK æœªå®‰è£…ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return None
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            logger.warning("GROQ_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return None
        
        try:
            client = Groq(api_key=api_key)
            
            # é™åˆ¶è¾“å…¥é•¿åº¦
            content_limit = 3000
            if len(archive_content) > content_limit:
                archive_content = archive_content[:content_limit] + "\n\n...(å†…å®¹å·²æˆªæ–­)"
            
            prompt = f"""è¯·å°†ä»¥ä¸‹ç½‘é¡µçš„å½’æ¡£å†…å®¹æ•´ç†æˆä¸€ä»½**ç»“æ„åŒ– Markdown çŸ¥è¯†æ¡£æ¡ˆ**ã€‚

## è¾“å…¥ä¿¡æ¯

**æ ‡é¢˜**: {title}
**æ¥æº**: {url}
**å¹³å°**: {platform}

## âš ï¸ é‡è¦ï¼šè¯†åˆ«é”™è¯¯ä¿®æ­£

OCR æ–‡æœ¬å¯èƒ½å­˜åœ¨è¯†åˆ«é”™è¯¯ï¼Œä½ å¿…é¡»æ ¹æ®ä¸Šä¸‹æ–‡**ä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£**ï¼š
- **åŒéŸ³å­—/è¯é”™è¯¯**: "æœºå™¨å­¦ä¹ "â†’"é¸¡å™¨å­¦ä¹ "ã€"Python"â†’"æ´¾æ£®"
- **ä¸“ä¸šæœ¯è¯­**: æŠ€æœ¯åè¯ã€å­¦æœ¯æ¦‚å¿µè¦ä½¿ç”¨è§„èŒƒè¡¨è¾¾
- **äººååœ°å**: ç¡®ä¿æ‹¼å†™å‡†ç¡®
- **æ ‡ç‚¹ç¬¦å·**: è¡¥å……ç¼ºå¤±çš„é€—å·ã€å¥å·ã€é—®å·
- **æ®µè½åˆ†éš”**: ä¿®æ­£ä¸åˆç†çš„æ¢è¡Œå’Œåˆ†æ®µ

## ä½ éœ€è¦å®Œæˆï¼š

1. **å†…å®¹æ¸…æ´—ä¸ä¿®æ­£**
   - ä¿®æ­£ OCR è¯†åˆ«é”™è¯¯ï¼ˆåŒéŸ³å­—ã€é”™åˆ«å­—ã€æ–­å¥é—®é¢˜ï¼‰
   - åˆ é™¤æ— å…³çš„å¹¿å‘Šã€æ¨å¹¿ã€å¯¼èˆªã€è¯„è®º
   - è¡¥å……ç¼ºå¤±çš„æ ‡ç‚¹ç¬¦å·å’Œæ®µè½åˆ†éš”
   - ç¡®ä¿ä½¿ç”¨ä¸“ä¸šã€å‡†ç¡®çš„æœ¯è¯­

2. **ç»“æ„åŒ–ç»„ç»‡**
   - ç”Ÿæˆæ¸…æ™°çš„å±‚çº§æ ‡é¢˜ï¼ˆ##ã€###ï¼‰
   - ä½¿ç”¨ Markdown åˆ—è¡¨æ ¼å¼ï¼ˆæœ‰åº/æ— åºï¼‰
   - ä»£ç å—ä½¿ç”¨ ``` æ ‡è®°ï¼Œæ ‡æ³¨è¯­è¨€
   - é‡è¦å†…å®¹ä½¿ç”¨ **åŠ ç²—** æˆ– `è¡Œå†…ä»£ç `
   - å¼•ç”¨ä½¿ç”¨ > æ ¼å¼

3. **ä¿¡æ¯æç‚¼**
   - ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼ˆæ•°æ®ã€ç»“è®ºã€æ–¹æ³•ã€æ­¥éª¤ï¼‰
   - åˆ é™¤å†—ä½™çš„å£è¯­åŒ–è¡¨è¾¾
   - å¦‚æœæœ‰ä½œè€…è§‚ç‚¹ï¼Œç”¨ã€Œã€æˆ– > å¼•ç”¨æ ¼å¼æ ‡æ³¨
   - æå–æ ¸å¿ƒè¦ç‚¹ï¼Œç”Ÿæˆå°ç»“

4. **å®Œæ•´æ€§è¦æ±‚**
   - å›¾ç‰‡ä¿ç•™åŸæœ‰é“¾æ¥ï¼ˆ`![](images/xxx.jpg)`ï¼‰
   - é“¾æ¥ä¿ç•™åŸæœ‰æ ¼å¼ï¼ˆ`[æ–‡æœ¬](URL)`ï¼‰
   - è¡¨æ ¼ä¿ç•™åŸæœ‰ç»“æ„
   - ä¸è¦çœç•¥é‡è¦ç»†èŠ‚

## æ¨èè¾“å‡ºç»“æ„

# [æ–‡ç« æ ‡é¢˜]

## æ‘˜è¦
ï¼ˆ50å­—ä»¥å†…çš„æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬ï¼‰

## ä¸»è¦å†…å®¹

### [ç« èŠ‚1æ ‡é¢˜]
ï¼ˆè¯¦ç»†å†…å®¹...ï¼‰

### [ç« èŠ‚2æ ‡é¢˜]
ï¼ˆè¯¦ç»†å†…å®¹...ï¼‰

## å…³é”®ä¿¡æ¯
- é‡è¦æ•°æ®ï¼š...
- æ ¸å¿ƒè§‚ç‚¹ï¼š...
- æ“ä½œæ­¥éª¤ï¼š...

## æ ‡ç­¾
æ ‡ç­¾: æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3

---

## åŸå§‹å½’æ¡£å†…å®¹

```
{archive_content}
```

---

**è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦ä»»ä½•åŒ…è£¹æˆ–é¢å¤–è¯´æ˜ã€‚**"""

            response = client.chat.completions.create(
                model="openai/gpt-oss-20b",
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ•´ç†ä¸“å®¶ï¼Œå…·å¤‡æ™ºèƒ½çº é”™èƒ½åŠ›ã€‚

ä½ çš„èŒè´£æ˜¯ï¼š
- å°†æ··ä¹±çš„ OCR æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„æ¸…æ™°çš„ Markdown çŸ¥è¯†æ¡£æ¡ˆ
- **æ™ºèƒ½çº é”™**ï¼šä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£OCRè¯†åˆ«é”™è¯¯ï¼ˆåŒéŸ³å­—/è¯ã€ä¸“ä¸šæœ¯è¯­ã€æ ‡ç‚¹ç¬¦å·ï¼‰
- æ¨æ–­å¹¶è¡¥å…¨ä¸å®Œæ•´çš„å¥å­å’Œæ®µè½
- æå–æ ¸å¿ƒä¿¡æ¯å¹¶ç»“æ„åŒ–ç»„ç»‡
- ç¡®ä¿è¾“å‡ºä½¿ç”¨å‡†ç¡®ã€ä¸“ä¸šçš„æœ¯è¯­è¡¨è¾¾
- ç”Ÿæˆæ¸…æ™°ã€å¯é•¿æœŸä¿å­˜ã€é€‚åˆæ£€ç´¢çš„çŸ¥è¯†æ–‡æ¡£"""
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=8192,
                temperature=0.7,
            )
            
            report_content = response.choices[0].message.content.strip()
            
            if not report_content or len(report_content) < 50:
                logger.warning("LLM ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡ä¿å­˜")
                return None
            
            logger.info(f"âœ… LLM ç”ŸæˆæŠ¥å‘ŠæˆåŠŸï¼ˆ{len(report_content)} å­—ç¬¦ï¼‰")
            return report_content
            
        except Exception as e:
            logger.warning(f"LLM æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _perform_ocr_on_images(self, images_dir: Path) -> str:
        """
        å¯¹æŒ‡å®šç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
        
        Args:
            images_dir: å›¾ç‰‡ç›®å½•è·¯å¾„
        
        Returns:
            åˆå¹¶åçš„OCRæ–‡æœ¬
        """
        if not images_dir.exists() or not images_dir.is_dir():
            logger.warning(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
            return ""
        
        try:
            # å°è¯•å¯¼å…¥OCRæ¨¡å—
            try:
                from ocr.ocr_vision import init_vision_ocr, ocr_image_vision
                ocr_engine = "vision"
            except ImportError:
                logger.warning("Vision OCR ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ PaddleOCR")
                try:
                    from ocr.ocr_paddle import init_paddleocr, ocr_image_paddle
                    ocr_engine = "paddle"
                except ImportError:
                    logger.error("æœªæ‰¾åˆ°å¯ç”¨çš„OCRå¼•æ“")
                    return ""
            
            # åˆå§‹åŒ–OCR
            if ocr_engine == "vision":
                ocr = init_vision_ocr(lang="ch", recognition_level="accurate")
                logger.info("ä½¿ç”¨ Apple Vision OCR")
            else:
                ocr = init_paddleocr(lang="ch", use_gpu=False)
                logger.info("ä½¿ç”¨ PaddleOCR")
            
            # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
            image_files = [
                f for f in images_dir.iterdir() 
                if f.is_file() and f.suffix.lower() in image_extensions
            ]
            
            if not image_files:
                logger.warning(f"åœ¨ {images_dir} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
                return ""
            
            logger.info(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹è¯†åˆ«...")
            
            # å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡ŒOCR
            all_text = []
            for i, image_file in enumerate(image_files, 1):
                try:
                    if ocr_engine == "vision":
                        result = ocr_image_vision(ocr, str(image_file))
                    else:
                        result = ocr_image_paddle(ocr, str(image_file))
                    
                    if result and result.strip():
                        all_text.append(f"## å›¾ç‰‡ {i}: {image_file.name}\n\n{result}\n")
                        logger.debug(f"  [{i}/{len(image_files)}] {image_file.name}: {len(result)} å­—ç¬¦")
                    else:
                        logger.debug(f"  [{i}/{len(image_files)}] {image_file.name}: æœªè¯†åˆ«åˆ°æ–‡å­—")
                        
                except Exception as e:
                    logger.warning(f"  [{i}/{len(image_files)}] {image_file.name}: OCRå¤±è´¥ - {e}")
                    continue
            
            if all_text:
                combined_text = "\n\n".join(all_text)
                logger.info(f"âœ… OCRå®Œæˆï¼šå…±è¯†åˆ« {len(combined_text)} å­—ç¬¦")
                return combined_text
            else:
                logger.warning("æ‰€æœ‰å›¾ç‰‡å‡æœªè¯†åˆ«åˆ°æ–‡å­—")
                return ""
                
        except Exception as e:
            logger.error(f"OCRå¤„ç†å¤±è´¥: {e}")
            return ""
    
    def close(self):
        """
        å…³é—­å½’æ¡£å™¨
        
        æ³¨æ„ï¼šä¸ä¼šå…³é—­æµè§ˆå™¨è¿›ç¨‹ï¼Œåªå…³é—­å½“å‰æ ‡ç­¾é¡µï¼ˆå¦‚æœæœ‰ï¼‰
        æµè§ˆå™¨ä¼šåœ¨ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨å…³é—­
        """
        self._close_tab()
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.close()
