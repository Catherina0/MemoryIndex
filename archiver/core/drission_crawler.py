"""
åŸºäº DrissionPage çš„ç½‘é¡µå½’æ¡£å™¨
ä½¿ç”¨çœŸå®æµè§ˆå™¨ç¯å¢ƒï¼Œæ”¯æŒå¤æ‚çš„ JS æ¸²æŸ“å’Œç™»å½•æ€
"""

import time
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime

try:
    from DrissionPage import Chromium
    DRISSIONPAGE_AVAILABLE = True
except ImportError:
    DRISSIONPAGE_AVAILABLE = False
    logging.warning("DrissionPage not installed. Run: pip install DrissionPage")

try:
    import html2text
    HTML2TEXT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False
    logging.warning("html2text not installed. Run: pip install html2text")

from archiver.platforms.base import PlatformAdapter
from archiver.utils.url_parser import detect_platform
from archiver.utils.image_downloader import ImageDownloader
from archiver.utils.browser_manager import get_browser_manager


logger = logging.getLogger(__name__)


class DrissionArchiver:
    """åŸºäº DrissionPage çš„ç½‘é¡µå½’æ¡£å™¨"""
    
    def __init__(
        self,
        output_dir: str = "archived",
        browser_data_dir: str = "./browser_data",
        headless: bool = True,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–å½’æ¡£å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            browser_data_dir: æµè§ˆå™¨æ•°æ®ç›®å½•ï¼ˆå­˜å‚¨ Cookies å’Œç™»å½•æ€ï¼‰
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        if not DRISSIONPAGE_AVAILABLE:
            raise ImportError("Please install DrissionPage: pip install DrissionPage")
        
        if not HTML2TEXT_AVAILABLE:
            raise ImportError("Please install html2text: pip install html2text")
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.browser_data_dir = Path(browser_data_dir)
        self.browser_data_dir.mkdir(exist_ok=True)
        
        self.headless = headless
        self.verbose = verbose
        
        # è·å–æµè§ˆå™¨ç®¡ç†å™¨ï¼ˆå…¨å±€å•ä¾‹ï¼‰
        self.browser_manager = get_browser_manager()
        
        # é…ç½® HTML2Text
        self.converter = html2text.HTML2Text()
        self.converter.ignore_links = False
        self.converter.ignore_images = False
        self.converter.body_width = 0
        
        # å½“å‰ä»»åŠ¡çš„æ ‡ç­¾é¡µï¼ˆæ¯ä¸ªä»»åŠ¡ä¸€ä¸ª tabï¼‰
        self.current_tab = None
        
        # é…ç½®æ—¥å¿—
        if verbose:
            logging.basicConfig(level=logging.INFO)
    
    def _init_tab(self):
        """ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºæ–°æ ‡ç­¾é¡µ"""
        # è·å–å…¨å±€æµè§ˆå™¨å®ä¾‹
        browser = self.browser_manager.get_browser(
            browser_data_dir=str(self.browser_data_dir),
            headless=self.headless
        )
        
        # åˆ›å»ºæ–°æ ‡ç­¾é¡µ
        tab = self.browser_manager.new_tab()
        logger.info("âœ“ æ–°æ ‡ç­¾é¡µå·²åˆ›å»º")
        return tab
    
    def _close_tab(self):
        """å…³é—­å½“å‰ä»»åŠ¡çš„æ ‡ç­¾é¡µ"""
        if self.current_tab is not None:
            self.browser_manager.close_tab(self.current_tab)
            self.current_tab = None
    
    def _deduplicate_twitter_images(self, image_urls: list) -> list:
        """
        Twitter å›¾ç‰‡å»é‡ï¼šç§»é™¤åŒä¸€å›¾ç‰‡çš„ä¸åŒå°ºå¯¸ç‰ˆæœ¬
        
        Twitter å›¾ç‰‡ URL æ ¼å¼ï¼š
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=small
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=medium
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=large
        https://pbs.twimg.com/media/xxxxx?format=jpg&name=orig
        
        ç­–ç•¥ï¼šåªä¿ç•™æ¯å¼ å›¾ç‰‡çš„æœ€å¤§å°ºå¯¸ç‰ˆæœ¬ï¼ˆä¼˜å…ˆçº§ï¼šorig > large > medium > smallï¼‰
        """
        if not image_urls:
            return image_urls
        
        # æŒ‰å›¾ç‰‡IDåˆ†ç»„
        image_groups = {}
        size_priority = {'orig': 4, 'large': 3, '4096x4096': 3, 'medium': 2, 'small': 1, '900x900': 1, '360x360': 0}
        
        for url in image_urls:
            if 'twimg.com/media/' in url:
                # æå–å›¾ç‰‡IDï¼ˆå»é™¤å‚æ•°ï¼‰
                base_url = url.split('?')[0]
                
                # æå–å°ºå¯¸å‚æ•°
                size = 'medium'  # é»˜è®¤
                if 'name=' in url:
                    import re
                    match = re.search(r'name=(\w+)', url)
                    if match:
                        size = match.group(1)
                
                # è®°å½•æˆ–æ›´æ–°æœ€å¤§å°ºå¯¸ç‰ˆæœ¬
                if base_url not in image_groups:
                    image_groups[base_url] = {'url': url, 'size': size, 'priority': size_priority.get(size, 0)}
                else:
                    current_priority = size_priority.get(size, 0)
                    if current_priority > image_groups[base_url]['priority']:
                        image_groups[base_url] = {'url': url, 'size': size, 'priority': current_priority}
            else:
                # é Twitter å›¾ç‰‡ï¼Œç›´æ¥ä¿ç•™
                image_groups[url] = {'url': url, 'size': 'unknown', 'priority': 999}
        
        # è¿”å›å»é‡åçš„ URL åˆ—è¡¨
        result = [item['url'] for item in image_groups.values()]
        
        if len(result) < len(image_urls):
            logger.info(f"Twitter å›¾ç‰‡å»é‡: {len(image_urls)} -> {len(result)} å¼ ï¼ˆç§»é™¤äº†é‡å¤å°ºå¯¸ï¼‰")
        
        return result
    
    def _load_manual_cookies(self, platform_name: str, url: str):
        """
        åŠ è½½æ‰‹åŠ¨é…ç½®çš„ Cookieï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
        Args:
            platform_name: å¹³å°åç§°ï¼ˆzhihu, xiaohongshu, bilibiliï¼‰
            url: ç›®æ ‡URL
        """
        # é…ç½®æ–‡ä»¶è·¯å¾„
        config_dir = Path(__file__).parent.parent / "config"
        cookie_file = config_dir / f"{platform_name}_drission_cookie.txt"
        
        if not cookie_file.exists():
            logger.debug(f"æœªæ‰¾åˆ°æ‰‹åŠ¨é…ç½®çš„ Cookie: {cookie_file}")
            return False
        
        try:
            # è¯»å– Cookie
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookie_string = f.read().strip()
            
            if not cookie_string:
                logger.warning(f"Cookie æ–‡ä»¶ä¸ºç©º: {cookie_file}")
                return False
            
            logger.info(f"åŠ è½½æ‰‹åŠ¨é…ç½®çš„ Cookie: {platform_name}")
            
            # ç¡®ä¿å·²è®¿é—®é¡µé¢ï¼ˆCookie éœ€è¦åŸŸåï¼‰
            if not self.current_tab.url or self.current_tab.url == 'about:blank':
                logger.info(f"é¦–æ¬¡è®¿é—®é¡µé¢ä»¥è®¾ç½® Cookie...")
                self.current_tab.get(url)
                time.sleep(1)
            
            # è§£æå¹¶è®¾ç½® Cookie
            # æ ¼å¼ï¼šname1=value1; name2=value2; ...
            cookie_pairs = [pair.strip() for pair in cookie_string.split(';') if pair.strip()]
            
            for pair in cookie_pairs:
                if '=' not in pair:
                    continue
                
                name, value = pair.split('=', 1)
                name = name.strip()
                value = value.strip()
                
                try:
                    # è®¾ç½® Cookie
                    self.current_tab.set.cookies({
                        'name': name,
                        'value': value,
                        'domain': self._get_cookie_domain(url),
                        'path': '/'
                    })
                    logger.debug(f"âœ“ è®¾ç½® Cookie: {name}")
                except Exception as e:
                    logger.warning(f"âœ— è®¾ç½® Cookie å¤±è´¥ {name}: {e}")
            
            logger.info(f"âœ“ æˆåŠŸåŠ è½½ {len(cookie_pairs)} ä¸ª Cookie")
            
            # åˆ·æ–°é¡µé¢ä½¿ Cookie ç”Ÿæ•ˆ
            logger.info("åˆ·æ–°é¡µé¢...")
            self.current_tab.refresh()
            time.sleep(1)
            
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ‰‹åŠ¨ Cookie å¤±è´¥: {e}")
            return False
    
    def _get_cookie_domain(self, url: str) -> str:
        """ä» URL æå– Cookie åŸŸå"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        # è¿”å› .domain.com æ ¼å¼
        domain_parts = parsed.netloc.split('.')
        if len(domain_parts) >= 2:
            return f".{'.'.join(domain_parts[-2:])}"
        return parsed.netloc
    

    
    def archive(
        self,
        url: str,
        platform_adapter: Optional[PlatformAdapter] = None,
        mode: str = "default"
    ) -> Dict[str, Any]:
        """
        å½’æ¡£æŒ‡å®šURLçš„ç½‘é¡µå†…å®¹
        
        Args:
            url: ç›®æ ‡URLï¼ˆæ”¯æŒåˆ†äº«æ–‡æœ¬æ ¼å¼ï¼Œä¼šè‡ªåŠ¨æå–URLï¼‰
            platform_adapter: å¹³å°é€‚é…å™¨ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
            mode: å½’æ¡£æ¨¡å¼ (default/full)
        
        Returns:
            åŒ…å«å½’æ¡£ç»“æœçš„å­—å…¸
        """
        # ä»è¾“å…¥æ–‡æœ¬ä¸­æå– URLï¼ˆæ”¯æŒåˆ†äº«æ–‡æœ¬æ ¼å¼ï¼‰
        from archiver.utils.url_parser import extract_url_from_text
        original_input = url
        url = extract_url_from_text(url)
        
        if not url:
            return {
                'success': False,
                'error': 'æ— æ³•ä»è¾“å…¥ä¸­æå–æœ‰æ•ˆçš„URL',
                'input': original_input
            }
        
        # å¦‚æœæå–çš„URLä¸è¾“å…¥ä¸åŒï¼Œè®°å½•æ—¥å¿—
        if url != original_input:
            logger.info(f"ä»åˆ†äº«æ–‡æœ¬ä¸­æå–URL: {url}")
        
        logger.info(f"å¼€å§‹å½’æ¡£: {url}")
        
        # ä¸ºæ­¤ä»»åŠ¡åˆ›å»ºæ–°æ ‡ç­¾é¡µ
        self.current_tab = self._init_tab()
        
        try:
            # è‡ªåŠ¨æ£€æµ‹å¹³å°
            if platform_adapter is None:
                platform_name = detect_platform(url)
                logger.info(f"æ£€æµ‹å¹³å°: {platform_name} (æ¨¡å¼: {mode})")
                from archiver.platforms import (
                    ZhihuAdapter, XiaohongshuAdapter, BilibiliAdapter,
                    RedditAdapter, WordPressAdapter, TwitterAdapter
                )
                
                adapters = {
                    "zhihu": ZhihuAdapter(),
                    "xiaohongshu": XiaohongshuAdapter(),
                    "bilibili": BilibiliAdapter(),
                    "reddit": RedditAdapter(),
                    "twitter": TwitterAdapter(),
                    "wordpress": WordPressAdapter(),
                }
                platform_adapter = adapters.get(platform_name, WordPressAdapter())
            
            # è®¿é—®é¡µé¢
            logger.info(f"æ­£åœ¨è®¿é—®: {url}")
            
            # å°è¯•åŠ è½½æ‰‹åŠ¨é…ç½®çš„ Cookie
            if platform_adapter.name in ['zhihu', 'xiaohongshu', 'bilibili', 'twitter']:
                self._load_manual_cookies(platform_adapter.name, url)
            
            self.current_tab.get(url)
            
            # æ™ºèƒ½ç­‰å¾…é¡µé¢åŠ è½½
            self.current_tab.wait.load_start()
            time.sleep(2)  # ç­‰å¾… JS æ‰§è¡Œ
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•ï¼ˆæ¨ç‰¹ç‰¹æ®Šå¤„ç†ï¼‰
            if platform_adapter.name == 'twitter':
                current_url = self.current_tab.url
                if 'login' in current_url or 'i/flow/login' in current_url:
                    logger.warning("âš ï¸  æ¨ç‰¹éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹å†…å®¹")
                    logger.info("ğŸ’¡ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ç™»å½•æ¨ç‰¹ï¼š")
                    logger.info("   make login-twitter")
                    logger.info("   æˆ–è€…è®¿é—® https://twitter.com æ‰‹åŠ¨ç™»å½•")
                    return {
                        "success": False,
                        "error": "æ¨ç‰¹éœ€è¦ç™»å½•ã€‚è¯·è¿è¡Œ 'make login-twitter' ç™»å½•è´¦å·",
                        "url": url
                    }
            
            # æ»šåŠ¨é¡µé¢ç¡®ä¿æ‡’åŠ è½½å†…å®¹åŠ è½½å®Œæˆ
            logger.info("æ»šåŠ¨é¡µé¢åŠ è½½æ‡’åŠ è½½å†…å®¹...")
            self.current_tab.scroll.to_bottom()
            time.sleep(1)
            self.current_tab.scroll.to_top()
            time.sleep(1)
            
            # è·å–é¡µé¢æ ‡é¢˜
            page_title = self.current_tab.title
            if not page_title:
                page_title = "Untitled"
            
            # ğŸ†• æå‰æå–å›¾ç‰‡URLï¼ˆä»å®Œæ•´é¡µé¢ï¼‰
            full_page_html = self.current_tab.html
            logger.info("ä»å®Œæ•´é¡µé¢æå–å›¾ç‰‡URL...")
            
            # æå–å†…å®¹
            content_html = self._extract_content(platform_adapter, mode=mode)
            
            if not content_html:
                return {
                    "success": False,
                    "error": "æ— æ³•æå–é¡µé¢å†…å®¹",
                    "url": url
                }
            
            # è½¬æ¢ä¸º Markdown
            markdown_content = self._convert_to_markdown(
                html=content_html,
                title=page_title,
                url=url,
                platform=platform_adapter.name,
                mode=mode
            )
            
            # åˆ›å»ºæ–‡ä»¶å¤¹
            folder_name = self._generate_folder_name(page_title, platform_adapter.name)
            folder_path = self.output_dir / folder_name
            folder_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½å›¾ç‰‡
            logger.info("å¼€å§‹ä¸‹è½½å›¾ç‰‡...")
            image_downloader = ImageDownloader(
                output_dir=str(folder_path / "images"),
                format="jpg"
            )
            
            # æå–å›¾ç‰‡URL
            # é»˜è®¤åªä»å†…å®¹æå–ï¼Œç‰¹æ®Šæƒ…å†µï¼ˆå¦‚æ— æ³•æå–åˆ°å†…å®¹å›¾ç‰‡ï¼‰æ‰ä»å…¨é¡µæå–
            image_urls = image_downloader.extract_image_urls(content_html, url)
            
            # æ¨ç‰¹ç‰¹æ®Šå¤„ç†ï¼šå®Œæ•´æ¨¡å¼ä¸‹ï¼Œæˆ–è€…å†…å®¹æå–ä¸åˆ°å›¾ç‰‡æ—¶ï¼Œå°è¯•ä»å®Œæ•´é¡µé¢æå–
            if platform_adapter.name == 'twitter':
                if mode == 'full' or not image_urls:
                    logger.info("æ¨ç‰¹ï¼šå°è¯•ä»å®Œæ•´é¡µé¢æå–å›¾ç‰‡...")
                    more_urls = image_downloader.extract_image_urls(full_page_html, url)
                    image_urls = list(set(image_urls + more_urls))
                
                # è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹æå–çš„å›¾ç‰‡ URL
                logger.debug(f"åŸå§‹æå–çš„å›¾ç‰‡ URLs: {image_urls}")
            
            # è¿‡æ»¤å›¾ç‰‡ï¼ˆé’ˆå¯¹é»˜è®¤æ¨¡å¼ï¼‰
            if image_urls and mode == 'default':
                # æ¨ç‰¹ï¼šç§»é™¤å¤´åƒå’Œè¡¨æƒ…åŒ…ï¼Œåªä¿ç•™åª’ä½“å›¾ç‰‡
                if platform_adapter.name == 'twitter':
                    filtered_urls = []
                    for img_url in image_urls:
                        # æ’é™¤å¤´åƒ (profile_images)
                        if 'profile_images' in img_url:
                            continue
                        # æ’é™¤å°å›¾æ ‡/è¡¨æƒ… (emoji)
                        if 'emoji' in img_url:
                            continue
                        filtered_urls.append(img_url)
                    
                    if len(filtered_urls) < len(image_urls):
                        logger.info(f"è¿‡æ»¤äº† {len(image_urls) - len(filtered_urls)} å¼ æ— å…³å›¾ç‰‡ï¼ˆå¤´åƒ/è¡¨æƒ…ï¼‰")
                    image_urls = filtered_urls
                
                # Twitter å›¾ç‰‡å»é‡ï¼ˆç§»é™¤åŒä¸€å›¾ç‰‡çš„ä¸åŒå°ºå¯¸ç‰ˆæœ¬ï¼‰
                if platform_adapter.name == 'twitter':
                    image_urls = self._deduplicate_twitter_images(image_urls)
            
            url_mapping = {}
            if image_urls:
                logger.info(f"å‘ç° {len(image_urls)} å¼ å›¾ç‰‡")
                url_mapping = image_downloader.download_all(image_urls, referer=url)
                
                # æ›´æ–°markdownä¸­çš„å›¾ç‰‡é“¾æ¥
                if url_mapping:
                    for orig_url, local_path in url_mapping.items():
                        rel_path = f"images/{local_path}"
                        markdown_content = markdown_content.replace(orig_url, rel_path)
                    logger.info(f"å·²æ›´æ–° {len(url_mapping)} ä¸ªå›¾ç‰‡é“¾æ¥")
            
            # ä¿å­˜ Markdown æ–‡ä»¶
            md_filename = "README.md"
            md_path = folder_path / md_filename
            
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            
            logger.info(f"å½’æ¡£æˆåŠŸ: {folder_path}")
            logger.info(f"  - Markdown: {md_path.name}")
            if image_urls:
                logger.info(f"  - å›¾ç‰‡: {len(url_mapping)}/{len(image_urls)} å¼ ")
            
            return {
                "success": True,
                "url": url,
                "platform": platform_adapter.name,
                "output_path": str(folder_path),
                "markdown_path": str(md_path),
                "title": page_title,
                "content_length": len(markdown_content),
                "images_downloaded": len(url_mapping) if image_urls else 0,
                "images_total": len(image_urls) if image_urls else 0
            }
            
        except Exception as e:
            logger.error(f"å½’æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": url
            }
        finally:
            # ä»»åŠ¡ç»“æŸï¼Œå…³é—­æ ‡ç­¾é¡µï¼ˆæµè§ˆå™¨ä¿æŒè¿è¡Œï¼‰
            self._close_tab()
            # æ³¨æ„ï¼šæµè§ˆå™¨ä¼šåœ¨ç¨‹åºé€€å‡ºæ—¶é€šè¿‡ atexit è‡ªåŠ¨å…³é—­
    
    def _extract_content(self, platform_adapter: PlatformAdapter, mode: str = "default") -> str:
        """
        æå–é¡µé¢å†…å®¹
        
        Args:
            platform_adapter: å¹³å°é€‚é…å™¨
            mode: å½’æ¡£æ¨¡å¼ ('default' æˆ– 'full')
                  default: ä»…ä¿ç•™æ­£æ–‡å’Œå›¾ç‰‡ï¼Œç§»é™¤è¯„è®ºã€ä¾§è¾¹æ ç­‰æ— å…³å…ƒç´ 
                  full: ä¿ç•™é€‰å®šå®¹å™¨å†…çš„æ‰€æœ‰å†…å®¹
        """
        selector = platform_adapter.content_selector
        exclude_selector = platform_adapter.exclude_selector if hasattr(platform_adapter, 'exclude_selector') else ""
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦ç™»å½•ï¼ˆå¸¸è§ç™»å½•æç¤ºï¼‰
        login_indicators = [
            "ç™»å½•åæ¨è",
            "é©¬ä¸Šç™»å½•",
            "è¯·å…ˆç™»å½•",
            "Sign in",
            "Log in",
            "ç™»å…¥"
        ]
        
        page_text = self.current_tab.html
        for indicator in login_indicators:
            if indicator in page_text:
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹ï¼ˆç™»å½•æç¤ºé€šå¸¸æ–‡æœ¬å¾ˆçŸ­ï¼‰
                if len(self.current_tab.ele('body', timeout=1).text.strip()) < 500:
                    logger.warning(f"âš ï¸  æ£€æµ‹åˆ°ç™»å½•æç¤º: {indicator}")
                    logger.warning("   å»ºè®®æ“ä½œï¼š")
                    logger.warning("   1. è¿è¡Œ 'make login' ç™»å½•å¹¶ä¿å­˜ç™»å½•æ€")
                    logger.warning("   2. æˆ–è¿è¡Œ 'make config-drission-cookie' æ‰‹åŠ¨é…ç½®Cookie")
                    break
        
        # å°è¯•ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹
        if platform_adapter.name == 'twitter' and mode == 'default':
            try:
                logger.info("Twitter: å°è¯•æ„å»ºçº¯å‡€å†…å®¹ (Text + Photos)...")
                
                # Manual finding of article to avoid selector issues
                articles = self.current_tab.eles('tag:article')
                article = None
                for a in articles:
                    if a.attrs.get('data-testid') == 'tweet':
                        article = a
                        break
                
                if article:
                    logger.info("  - æ‰¾åˆ°ä¸»æ¨æ–‡å®¹å™¨ article[data-testid='tweet']")
                    parts = []
                    parts = []
                    # 1. æå–æ­£æ–‡ - Try CSS first, then XPath
                    text_div = article.ele("[data-testid='tweetText']")
                    if not text_div:
                        logger.warning("  - CSSæ‰¾ä¸tweetText, å°è¯•XPath...")
                        text_div = article.ele("xpath:.//*[@data-testid='tweetText']")
                    
                    if text_div:
                        parts.append(text_div.html)
                        logger.info(f"  - æ‰¾åˆ°æ¨æ–‡æ­£æ–‡ (é•¿åº¦: {len(text_div.text)})")
                    else:
                        logger.warning("  - âŒ æœªæ‰¾åˆ°æ¨æ–‡æ­£æ–‡ [data-testid='tweetText']")
                    
                    # 2. æå–å›¾ç‰‡å®¹å™¨
                    # Try CSS first, then XPath, then manual scan
                    photos = article.eles("[data-testid='tweetPhoto']")
                    if not photos:
                        logger.info("  - CSSæœªæ‰¾åˆ°å›¾ç‰‡, å°è¯•XPath...")
                        photos = article.eles("xpath:.//*[@data-testid='tweetPhoto']")
                    
                    if photos:
                        logger.info(f"  - æ‰¾åˆ° {len(photos)} ä¸ªå›¾ç‰‡å®¹å™¨")
                        for p in photos:
                            html_part = p.html
                            # Ensure high res images in HTML to match downloader logic
                            if 'name=' in html_part:
                                import re
                                html_part = re.sub(r'name=(small|medium|360x360|900x900)', 'name=large', html_part)
                            parts.append(html_part)
                    else:
                        logger.info("  - âŒ æœªæ‰¾åˆ°å›¾ç‰‡å®¹å™¨ (tweetPhoto)")
                        # Fallback: Find all images in article and filter avatars
                        imgs = article.eles("tag:img")
                        valid_imgs = []
                        for img in imgs:
                            src = img.attrs.get('src', '')
                            if 'profile_images' in src or 'emoji' in src:
                                continue
                            if src:
                                # Wrap in simple img tag if container not found
                                valid_imgs.append(f'<img src="{src}" />')
                        
                        if valid_imgs:
                             logger.info(f"  -ç”±äºæœªæ‰¾åˆ°å®¹å™¨ï¼Œç›´æ¥æå–äº† {len(valid_imgs)} å¼ æ­£æ–‡å›¾ç‰‡")
                             parts.extend(valid_imgs)
                            
                    if parts:
                        combined_html = "\n<br>\n".join(parts)
                        return combined_html
                else:
                    logger.warning("  - âŒ æœªæ‰¾åˆ°ä¸»æ¨æ–‡å®¹å™¨ article[data-testid='tweet']")
                    # DEBUG: Check what articles actually exist
                    arts = self.current_tab.eles('tag:article')
                    logger.info(f"DEBUG: Found {len(arts)} generic articles in Crawler Session")
                    for i, a in enumerate(arts[:3]):
                        logger.info(f"DEBUG Art {i} Attrs: {a.attrs}")
                    
                    # DEBUG: Check title again
                    logger.info(f"DEBUG Page Title: {self.current_tab.title}")

            except Exception as e:
                logger.warning(f"Twitter çº¯å‡€æå–å¤±è´¥: {e}, å°†å°è¯•é€šç”¨é€‰æ‹©å™¨")
                import traceback
                logger.warning(traceback.format_exc())

        if selector:
            for sel in selector.split(','):
                sel = sel.strip()
                element = self.current_tab.ele(sel, timeout=2)
                if element:
                    # å¦‚æœä¸æ˜¯å…¨é‡æ¨¡å¼ï¼Œä¸”å®šä¹‰äº†æ’é™¤é€‰æ‹©å™¨ï¼Œå°è¯•ç§»é™¤æ— å…³å…ƒç´ 
                    # æ³¨æ„ï¼šDrissionPage çš„å…ƒç´ æ“ä½œé€šå¸¸æ˜¯å³æ—¶çš„ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥æ“ä½œé¡µé¢ä¸Šçš„å…ƒç´ 
                    # ä½†ä¸ºäº†ä¸ç ´åé¡µé¢ç»“æ„å½±å“åç»­ï¼ˆè™½ç„¶æˆ‘ä»¬å¾ˆå¿«å°±å…³é—­ï¼‰ï¼Œæˆ–è€…ä¸ºäº†å¤„ç†æ–¹ä¾¿
                    # æˆ‘ä»¬ä¸»è¦é€šè¿‡ BeautifulSoup åå¤„ç†ï¼Œæˆ–è€…åœ¨è¿™é‡Œå°è¯•ç§»é™¤
                    
                    if mode == "default" and exclude_selector:
                        logger.info(f"æ¸…ç†æ¨¡å¼: ç§»é™¤æ— å…³å…ƒç´ ")
                        
                        # 1. ç§»é™¤é…ç½®ä¸­å®šä¹‰çš„å…ƒç´ 
                        for exclude in exclude_selector.split(','):
                            exclude = exclude.strip()
                            if not exclude:
                                continue
                            
                            try:
                                unwanted_elements = element.eles(exclude)
                                removed_count = 0
                                for unwanted in unwanted_elements:
                                    self.current_tab.run_js("arguments[0].remove()", unwanted)
                                    removed_count += 1
                                if removed_count > 0:
                                    logger.info(f"  - å·²ç§»é™¤ {removed_count} ä¸ª {exclude} å…ƒç´ ")
                            except Exception as e:
                                logger.debug(f"  - ç§»é™¤ {exclude} å¤±è´¥: {e}")
                        
                        # 2. å¯¹äºå°çº¢ä¹¦ï¼Œé¢å¤–ç§»é™¤ä½œè€…ä¿¡æ¯å’Œå…³æ³¨æŒ‰é’®
                        if platform_adapter.name == "xiaohongshu":
                            # ç§»é™¤ç”¨æˆ·profileé“¾æ¥ï¼ˆä½œè€…å¤´åƒå’Œåå­—ï¼‰
                            try:
                                profile_links = element.eles('a[href*="/user/profile"]')
                                if profile_links:
                                    logger.info(f"  - å·²ç§»é™¤ {len(profile_links)} ä¸ªç”¨æˆ·profileé“¾æ¥")
                                    for link in profile_links:
                                        self.current_tab.run_js("arguments[0].remove()", link)
                            except:
                                pass
                            
                            # ç§»é™¤"å…³æ³¨"æŒ‰é’® - é€šè¿‡æ–‡å­—å†…å®¹åŒ¹é…
                            try:
                                all_elements = element.eles('tag:div') + element.eles('tag:button')
                                follow_count = 0
                                for elem in all_elements:
                                    if elem.text and elem.text.strip() == 'å…³æ³¨':
                                        self.current_tab.run_js("arguments[0].remove()", elem)
                                        follow_count += 1
                                if follow_count > 0:
                                    logger.info(f"  - å·²ç§»é™¤ {follow_count} ä¸ªå…³æ³¨æŒ‰é’®")
                            except:
                                pass
                    
                    # é‡æ–°è·å– HTML (ç§»é™¤å…ƒç´ å)
                    html = element.html
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
                    if html and len(html) > 1000:
                        logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹: {sel}")
                        return html
        
        # å›é€€ï¼šä½¿ç”¨é€šç”¨é€‰æ‹©å™¨
        for fallback in ['article', 'main', 'body']:
            element = self.current_tab.ele(fallback, timeout=2)
            if element:
                logger.info(f"ä½¿ç”¨å›é€€é€‰æ‹©å™¨: {fallback}")
                return element.html
        
        # æœ€åçš„å›é€€ï¼šæ•´ä¸ªé¡µé¢
        logger.warning("ä½¿ç”¨æ•´ä¸ªé¡µé¢ä½œä¸ºå†…å®¹")
        return self.current_tab.html
    
    def _convert_to_markdown(
        self,
        html: str,
        title: str,
        url: str,
        platform: str,
        mode: str = "default"
    ) -> str:
        """å°† HTML è½¬æ¢ä¸º Markdown"""
        # æ·»åŠ å…ƒæ•°æ®å¤´éƒ¨
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        metadata = f"""---
title: {title}
url: {url}
platform: {platform}
archived_at: {timestamp}
---

"""
        
        # è½¬æ¢ HTML
        markdown_content = self.converter.handle(html)
        
        # å¦‚æœæ˜¯é»˜è®¤æ¨¡å¼ï¼Œåšé¢å¤–çš„ Markdown æ¸…æ´—
        if mode == "default":
            import re
            # å°çº¢ä¹¦ï¼šç§»é™¤ç”¨æˆ·profileé“¾æ¥
            if platform == "xiaohongshu":
                # ç§»é™¤ç”¨æˆ·profileé“¾æ¥ (æ ¼å¼: [![...](images/...jpg)](/user/profile/...)æ–‡å­—)
                markdown_content = re.sub(
                    r'\[!\[.*?\]\(images/.*?\)\]\(/user/profile/[^\)]+\)\[.*?\]\(/user/profile/[^\)]+\)\s*',
                    '',
                    markdown_content
                )
                # ç§»é™¤å•ç‹¬çš„ç”¨æˆ·é“¾æ¥ (æ ¼å¼: [ç”¨æˆ·å](/user/profile/...))
                markdown_content = re.sub(
                    r'\[.*?\]\(/user/profile/[^\)]+\)\s*',
                    '',
                    markdown_content
                )
                # ç§»é™¤å•ç‹¬çš„"å…³æ³¨"æ–‡å­—
                markdown_content = re.sub(r'^\s*å…³æ³¨\s*$', '', markdown_content, flags=re.MULTILINE)
            
            # æ¨ç‰¹ï¼šç§»é™¤ç”¨æˆ·profileé“¾æ¥å’Œäº’åŠ¨æŒ‰é’®
            elif platform == "twitter":
                # ç§»é™¤ç”¨æˆ·profileé“¾æ¥ (/@username)
                markdown_content = re.sub(
                    r'\[@[^\]]+\]\(/[^\)]+\)\s*',
                    '',
                    markdown_content
                )
                # ç§»é™¤äº’åŠ¨æ•°å­—ï¼ˆè½¬æ¨ã€ç‚¹èµç­‰ï¼‰
                markdown_content = re.sub(
                    r'^\s*\d+\s*(Retweets?|Likes?|Replies?|Views?)\s*$',
                    '',
                    markdown_content,
                    flags=re.MULTILINE
                )
            
            # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        
        return metadata + markdown_content
    
    def _generate_folder_name(self, title: str, platform: str) -> str:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼ˆä»…ä½¿ç”¨æ ‡é¢˜ï¼Œä¸åŒ…å«æ¥æºï¼‰"""
        import re
        
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤éæ³•å­—ç¬¦
        clean_title = re.sub(r'[<>:"/\\|?*]', '', title)
        clean_title = clean_title.strip()
        
        # ç§»é™¤æœ«å°¾çš„æ¥æºæ ‡è¯†ï¼ˆå¦‚"- å°çº¢ä¹¦"ã€"- çŸ¥ä¹"ç­‰ï¼‰
        # åŒ¹é…æ¨¡å¼ï¼š" - å¹³å°åç§°" æˆ– " - æ¥æº"
        clean_title = re.sub(r'\s*-\s*(å°çº¢ä¹¦|çŸ¥ä¹|Bç«™|å“”å“©å“”å“©|Reddit|wordpress|ç½‘ç«™|ç¤¾åŒº).*$', '', clean_title)
        clean_title = clean_title.strip()
        
        # é™åˆ¶é•¿åº¦
        if len(clean_title) > 60:
            clean_title = clean_title[:60]
        
        # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨æ—¶é—´æˆ³
        if not clean_title or clean_title == "Untitled":
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            return f"{platform}_{timestamp}"
        
        return clean_title
    
    def close(self):
        """
        å…³é—­å½’æ¡£å™¨
        
        æ³¨æ„ï¼šä¸ä¼šå…³é—­æµè§ˆå™¨è¿›ç¨‹ï¼Œåªå…³é—­å½“å‰æ ‡ç­¾é¡µï¼ˆå¦‚æœæœ‰ï¼‰
        æµè§ˆå™¨ä¼šåœ¨ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨å…³é—­
        """
        self._close_tab()
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.close()
