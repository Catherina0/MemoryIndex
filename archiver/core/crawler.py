"""
é€šç”¨ç½‘é¡µå½’æ¡£å™¨
ä½¿ç”¨ Crawl4AI å®ç°è·¨å¹³å°ç½‘é¡µå†…å®¹æå–
"""

import os
import asyncio
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    logging.warning("Crawl4AI not installed. Run: pip install crawl4ai")

from archiver.platforms.base import PlatformAdapter
from archiver.utils.url_parser import detect_platform
from archiver.core.markdown_converter import MarkdownConverter
from archiver.utils.image_downloader import ImageDownloader


logger = logging.getLogger(__name__)


class UniversalArchiver:
    """é€šç”¨ç½‘é¡µå½’æ¡£å™¨"""
    
    def __init__(
        self,
        output_dir: str = "archived",
        headless: bool = True,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–å½’æ¡£å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        if not CRAWL4AI_AVAILABLE:
            raise ImportError("Please install crawl4ai: pip install crawl4ai")
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.headless = headless
        self.verbose = verbose
        self.markdown_converter = MarkdownConverter()
        
        # é…ç½®æ—¥å¿—
        if verbose:
            logging.basicConfig(level=logging.INFO)
    
    async def archive(
        self,
        url: str,
        platform_adapter: Optional[PlatformAdapter] = None,
        cookies: Optional[Dict[str, str]] = None,
        mode: str = "default",
        generate_report: bool = False
    ) -> Dict[str, Any]:
        """
        å½’æ¡£æŒ‡å®šURLçš„ç½‘é¡µå†…å®¹
        
        Args:
            url: ç›®æ ‡URL
            platform_adapter: å¹³å°é€‚é…å™¨ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
            cookies: Cookieå­—å…¸
            mode: å½’æ¡£æ¨¡å¼ (default=åªæå–æ­£æ–‡/full=å®Œæ•´å†…å®¹å«è¯„è®º)
            generate_report: æ˜¯å¦ç”Ÿæˆ LLM ç»“æ„åŒ–æŠ¥å‘Š
        
        Returns:
            åŒ…å«å½’æ¡£ç»“æœçš„å­—å…¸
        """
        logger.info(f"å¼€å§‹å½’æ¡£: {url}")
        
        # è‡ªåŠ¨æ£€æµ‹å¹³å°
        if platform_adapter is None:
            platform_name = detect_platform(url)
            
            # éœ€è¦ç™»å½•çš„å¹³å°å¼ºåˆ¶ä½¿ç”¨ DrissionPage
            if platform_name in ['xiaohongshu', 'twitter']:
                logger.info(f"æ£€æµ‹åˆ°éœ€è¦ç™»å½•çš„å¹³å° ({platform_name})ï¼Œå¼ºåˆ¶ä½¿ç”¨ DrissionPage")
                try:
                    from archiver.core.drission_crawler import DrissionArchiver
                    from archiver.platforms import (
                        ZhihuAdapter, XiaohongshuAdapter, BilibiliAdapter,
                        RedditAdapter, TwitterAdapter, WordPressAdapter
                    )
                    adapters = {
                        "zhihu": ZhihuAdapter(),
                        "xiaohongshu": XiaohongshuAdapter(),
                        "bilibili": BilibiliAdapter(),
                        "reddit": RedditAdapter(),
                        "twitter": TwitterAdapter(),
                        "wordpress": WordPressAdapter(),
                    }
                    platform_adapter_instance = adapters.get(platform_name, WordPressAdapter())
                    
                    drission = DrissionArchiver(
                        output_dir=self.output_dir,
                        browser_data_dir="./browser_data",
                        headless=self.headless,
                        verbose=self.verbose
                    )
                    return drission.archive(url, platform_adapter_instance, mode=mode)
                except Exception as e:
                    logger.error(f"DrissionPage å½’æ¡£å¤±è´¥: {e}")
                    return {
                        "success": False,
                        "error": str(e),
                        "url": url
                    }
            from archiver.platforms import (
                ZhihuAdapter, XiaohongshuAdapter, BilibiliAdapter,
                RedditAdapter, TwitterAdapter, WordPressAdapter
            )
            
            adapters = {
                "zhihu": ZhihuAdapter(),
                "xiaohongshu": XiaohongshuAdapter(),
                "bilibili": BilibiliAdapter(),
                "reddit": RedditAdapter(),
                "twitter": TwitterAdapter(),
                "wordpress": WordPressAdapter(),
            }
            platform_adapter = adapters.get(platform_name, WordPressAdapter())
        
        # è‡ªåŠ¨è·å–å°çº¢ä¹¦Cookieï¼ˆå¦‚æœéœ€è¦ï¼‰
        if platform_adapter.name == "xiaohongshu" and cookies is None:
            logger.info("æ£€æµ‹åˆ°å°çº¢ä¹¦å¹³å°ï¼Œå°è¯•è‡ªåŠ¨åŠ è½½Cookie...")
            from archiver.utils.cookie_manager import get_xiaohongshu_cookies
            cookies = get_xiaohongshu_cookies()
            if cookies:
                logger.info("âœ“ æˆåŠŸåŠ è½½å°çº¢ä¹¦Cookie")
            else:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°å°çº¢ä¹¦Cookieï¼Œå¯èƒ½æ— æ³•è®¿é—®éœ€è¦ç™»å½•çš„å†…å®¹")
                logger.info("æç¤ºï¼šè¿è¡Œ 'make config-xhs-cookie' é…ç½®Cookie")
        
        # è‡ªåŠ¨è·å–çŸ¥ä¹Cookieï¼ˆå¦‚æœéœ€è¦ï¼‰
        if platform_adapter.name == "zhihu" and cookies is None:
            logger.info("æ£€æµ‹åˆ°çŸ¥ä¹å¹³å°ï¼Œå°è¯•è‡ªåŠ¨åŠ è½½Cookie...")
            from archiver.utils.cookie_manager import get_zhihu_cookies
            cookies = get_zhihu_cookies()
            if cookies:
                logger.info("âœ“ æˆåŠŸåŠ è½½çŸ¥ä¹Cookie")
            else:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°çŸ¥ä¹Cookieï¼Œå¯èƒ½æ— æ³•è®¿é—®éœ€è¦ç™»å½•çš„å†…å®¹")
                logger.info("æç¤ºï¼šè¯·å…ˆåœ¨Chromeæµè§ˆå™¨ç™»å½•çŸ¥ä¹ï¼Œç¨‹åºå°†è‡ªåŠ¨è¯»å–Cookie")
        
        # å°†cookieså­—å…¸è½¬æ¢ä¸ºPlaywrightæ ¼å¼ï¼ˆåˆ—è¡¨ï¼‰
        playwright_cookies = None
        if cookies:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            
            playwright_cookies = []
            for name, value in cookies.items():
                cookie = {
                    'name': name,
                    'value': value,
                    'domain': domain if not domain.startswith('www.') else domain[4:],
                    'path': '/',
                }
                playwright_cookies.append(cookie)
            
            logger.info(f"è½¬æ¢äº† {len(playwright_cookies)} ä¸ª Cookie")
            
        # ä¼˜å…ˆä½¿ç”¨ DrissionArchiver (å› ä¸º Crawl4AI å¯¹ JS åŠ¨æ€æ¸²æŸ“æ”¯æŒè¾ƒå¼±)
        try:
            from archiver.core.drission_crawler import DrissionArchiver
            drission = DrissionArchiver(
                output_dir=self.output_dir,
                browser_data_dir="./browser_data",
                headless=self.headless,
                verbose=self.verbose
            )
            # æ³¨æ„: DrissionCrawler æ˜¯åŒæ­¥çš„ï¼Œè¿™é‡Œç›´æ¥è°ƒç”¨
            result = drission.archive(url, platform_adapter, mode=mode)
            if result.get('success'):
                return result
            else:
                logger.warning(f"DrissionPage å½’æ¡£å¤±è´¥ï¼Œå°è¯•å›é€€åˆ° Crawl4AI: {result.get('error')}")
        except Exception as e:
            logger.warning(f"DrissionCrawler å¼‚å¸¸: {e}")
        
        # é…ç½®æµè§ˆå™¨
        browser_config = BrowserConfig(
            headless=self.headless,
            verbose=self.verbose,
            cookies=playwright_cookies,
            extra_args=["--disable-blink-features=AutomationControlled"]
        )
        
        # é…ç½®çˆ¬è™«è¿è¡Œå‚æ•°
        run_config = CrawlerRunConfig(
            wait_until="networkidle",
            page_timeout=30000,
        )
        
        # æ‰§è¡Œçˆ¬å–
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url=url,
                config=run_config
            )
            
            if not result.success:
                logger.error(f"çˆ¬å–å¤±è´¥: {result.error_message}")
                return {
                    "success": False,
                    "error": result.error_message,
                    "url": url
                }
            
            # è½¬æ¢ä¸ºMarkdown
            page_title = result.metadata.get("title", "Untitled")
            markdown_content = self.markdown_converter.convert(
                html=result.html,
                title=page_title,
                url=url,
                platform=platform_adapter.name,
                content_selector=platform_adapter.content_selector,
                exclude_selector=platform_adapter.exclude_selector
            )
            
            # æå–çº¯å†…å®¹ï¼ˆå»æ‰ YAML frontmatterï¼‰
            content_lines = markdown_content.split('\n')
            content_start = 0
            
            # è·³è¿‡ YAML frontmatter
            if content_lines and content_lines[0].strip() == '---':
                for i, line in enumerate(content_lines[1:], 1):
                    if line.strip() == '---':
                        content_start = i + 1
                        break
            
            # çº¯OCRå†…å®¹ï¼ˆä¸å«å…ƒä¿¡æ¯ï¼‰
            pure_ocr_content = '\n'.join(content_lines[content_start:]).strip()
            
            # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¹³å°_æ ‡é¢˜ï¼‰
            folder_name = self._generate_folder_name(page_title, platform_adapter.name)
            folder_path = self.output_dir / folder_name
            folder_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½å›¾ç‰‡
            logger.info("å¼€å§‹ä¸‹è½½å›¾ç‰‡...")
            image_downloader = ImageDownloader(
                output_dir=str(folder_path / "images"),
                format="jpg"
            )
            
            # æå–å¹¶ä¸‹è½½å›¾ç‰‡
            image_urls = image_downloader.extract_image_urls(result.html, url)
            if image_urls:
                logger.info(f"å‘ç° {len(image_urls)} å¼ å›¾ç‰‡")
                url_mapping = image_downloader.download_all(image_urls, referer=url)
                
                # æ›´æ–°markdownä¸­çš„å›¾ç‰‡é“¾æ¥
                if url_mapping:
                    for orig_url, local_path in url_mapping.items():
                        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ images/xxx.jpg
                        rel_path = f"images/{local_path}"
                        markdown_content = markdown_content.replace(orig_url, rel_path)
                    logger.info(f"å·²æ›´æ–° {len(url_mapping)} ä¸ªå›¾ç‰‡é“¾æ¥")
            
            # 1. ä¿å­˜ ocr_raw.mdï¼ˆçº¯OCRæ•°æ®ï¼Œä¸å«å…ƒä¿¡æ¯ï¼‰
            ocr_raw_path = folder_path / "ocr_raw.md"
            with open(ocr_raw_path, "w", encoding="utf-8") as f:
                # æ›´æ–°å›¾ç‰‡é“¾æ¥åçš„çº¯å†…å®¹
                updated_pure_content = pure_ocr_content
                if url_mapping:
                    for orig_url, local_path in url_mapping.items():
                        rel_path = f"images/{local_path}"
                        updated_pure_content = updated_pure_content.replace(orig_url, rel_path)
                f.write(updated_pure_content)
            logger.info(f"âœ… ä¿å­˜OCRåŸå§‹æ•°æ®: {ocr_raw_path.name}")
            
            # 2. ä¿å­˜ README.mdï¼ˆå…ƒä¿¡æ¯ + å¼•ç”¨ï¼‰
            readme_content = f"""---
title: {page_title}
url: {url}
platform: {platform_adapter.name}
archived_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
---

# {page_title}

**æ¥æº**: [{url}]({url})  
**å¹³å°**: {platform_adapter.name}  
**å½’æ¡£æ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## ğŸ“„ åŸå§‹å†…å®¹

è¯¦è§ [ocr_raw.md](ocr_raw.md)

---

> ğŸ’¡ **æç¤º**: ä½¿ç”¨ `report.md` æŸ¥çœ‹ LLM å¤„ç†åçš„ç»“æ„åŒ–å†…å®¹
"""
            
            md_filename = "README.md"
            md_path = folder_path / md_filename
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(readme_content)
            
            logger.info(f"å½’æ¡£æˆåŠŸ: {folder_path}")
            logger.info(f"  - README: {md_path.name}")
            logger.info(f"  - OCRåŸå§‹: {ocr_raw_path.name}")
            if image_urls:
                logger.info(f"  - å›¾ç‰‡: {len(url_mapping)}/{len(image_urls)} å¼ ")
            
            # 3. ç”Ÿæˆ report.mdï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
            if generate_report:
                logger.info(">> ä½¿ç”¨ LLM ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š...")
                report_content = self._generate_report_with_llm(
                    ocr_content=updated_pure_content,
                    title=page_title,
                    url=url,
                    platform=platform_adapter.name
                )
                
                if report_content:
                    report_path = folder_path / "report.md"
                    with open(report_path, "w", encoding="utf-8") as f:
                        f.write(report_content)
                    logger.info(f"âœ… ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š: {report_path.name}")
            else:
                logger.info("â„¹ï¸  è·³è¿‡ report.md ç”Ÿæˆï¼ˆä½¿ç”¨ --generate-report å¯ç”¨ï¼‰")
            
            # ä½¿ç”¨ LLM ç”Ÿæˆè¯­ä¹‰åŒ–çš„æ–‡ä»¶å¤¹åç§°å¹¶é‡å‘½å
            logger.info(">> ä½¿ç”¨ LLM ç”Ÿæˆè¯­ä¹‰åŒ–æ–‡ä»¶å¤¹å...")
            new_folder_name = self._generate_folder_name_with_llm(
                markdown_content=markdown_content,
                title=page_title,
                platform=platform_adapter.name,
                url=url
            )
            
            # å¦‚æœç”Ÿæˆçš„åç§°ä¸å½“å‰æ–‡ä»¶å¤¹åä¸åŒï¼Œåˆ™é‡å‘½å
            current_folder_name = folder_path.name
            if new_folder_name != folder_name:
                new_folder_path = self.output_dir / new_folder_name
                try:
                    # å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
                    if new_folder_path.exists():
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        new_folder_name = f"{new_folder_name}_{timestamp}"
                        new_folder_path = self.output_dir / new_folder_name
                    
                    folder_path.rename(new_folder_path)
                    folder_path = new_folder_path  # æ›´æ–°å¼•ç”¨
                    md_path = folder_path / md_filename
                    logger.info(f"âœ… æ–‡ä»¶å¤¹å·²é‡å‘½åä¸º: {folder_path.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹é‡å‘½åå¤±è´¥: {e}ï¼Œä¿æŒåŸåç§°: {current_folder_name}")
            
            return {
                "success": True,
                "url": url,
                "platform": platform_adapter.name,
                "output_path": str(folder_path),
                "markdown_path": str(md_path),
                "title": page_title,
                "content_length": len(markdown_content),
                "images_downloaded": len(url_mapping) if image_urls else 0,
                "images_total": len(image_urls) if image_urls else 0
            }
    
    def _generate_filename(self, url: str, platform: str) -> str:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆæ—§ç‰ˆæ–¹æ³•ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ä»URLä¸­æå–ç®€çŸ­æ ‡è¯†
        url_hash = abs(hash(url)) % 10000
        return f"{platform}_{url_hash}_{timestamp}.md"
    
    def _generate_folder_name(self, title: str, platform: str) -> str:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼ˆå¹³å°_æ ‡é¢˜ï¼‰"""
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤éæ³•å­—ç¬¦
        import re
        clean_title = re.sub(r'[<>:"/\\|?*]', '', title)
        clean_title = clean_title.strip()
        
        # é™åˆ¶é•¿åº¦
        if len(clean_title) > 50:
            clean_title = clean_title[:50]
        
        # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨æ—¶é—´æˆ³
        if not clean_title or clean_title == "Untitled":
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            return f"{platform}_{timestamp}"
        
        return f"{platform}_{clean_title}"
    
    def _generate_folder_name_with_llm(self, markdown_content: str, title: str, platform: str, url: str) -> str:
        """
        ä½¿ç”¨ llama-3.1-8b-instant æ¨¡å‹æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´çš„æ–‡ä»¶å¤¹åç§°
        
        Args:
            markdown_content: ä¿å­˜çš„ Markdown å†…å®¹
            title: åŸå§‹é¡µé¢æ ‡é¢˜
            platform: å¹³å°åç§°
            url: åŸå§‹ URL
        
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶å¤¹åç§°ï¼ˆé•¿åº¦é™åˆ¶åœ¨50ä¸ªå­—ç¬¦ä»¥å†…ï¼‰
        """
        if not GROQ_AVAILABLE:
            logger.warning("Groq SDK æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
            return self._generate_folder_name(title, platform)
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            logger.warning("GROQ_API_KEY æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
            return self._generate_folder_name(title, platform)
        
        try:
            client = Groq(api_key=api_key)
            
            # æå–å†…å®¹æ‘˜è¦ï¼ˆå»æ‰å‰é¢çš„ metadata å’Œå›¾ç‰‡é“¾æ¥ï¼Œé™åˆ¶é•¿åº¦ï¼‰
            content_lines = markdown_content.split('\n')
            content_start = 0
            
            # è·³è¿‡ YAML frontmatter
            if content_lines and content_lines[0].strip() == '---':
                for i, line in enumerate(content_lines[1:], 1):
                    if line.strip() == '---':
                        content_start = i + 1
                        break
            
            # è·å–å®é™…å†…å®¹
            actual_content = '\n'.join(content_lines[content_start:])
            # ç§»é™¤å›¾ç‰‡é“¾æ¥
            import re
            actual_content = re.sub(r'!\[.*?\]\(.*?\)', '', actual_content)
            # é™åˆ¶é•¿åº¦åˆ°å‰800å­—ç¬¦
            content_summary = actual_content[:800].strip()
            
            if not content_summary or len(content_summary) < 20:
                logger.warning("å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹å")
                return self._generate_folder_name(title, platform)
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶å‘½åä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´ã€è¯­ä¹‰åŒ–çš„æ–‡ä»¶å¤¹åç§°ã€‚

## è¾“å…¥ä¿¡æ¯

**ç½‘é¡µæ ‡é¢˜**: {title}  
**å¹³å°**: {platform}  
**URL**: {url}

**å†…å®¹æ‘˜è¦**:
{content_summary}

## å‘½åè¦æ±‚

### å†…å®¹è¦æ±‚
1. **æ ¸å¿ƒä¸»é¢˜æç‚¼**: å‡†ç¡®æ•æ‰å†…å®¹çš„æ ¸å¿ƒæ¦‚å¿µæˆ–å…³é”®ä¸»é¢˜
2. **ä¿¡æ¯å¯†åº¦**: åœ¨æœ‰é™å­—ç¬¦å†…ä¼ è¾¾æœ€å¤§ä¿¡æ¯é‡
3. **è¯­ä¹‰æ¸…æ™°**: è®©ç”¨æˆ·ä¸€çœ¼å°±èƒ½ç†è§£æ–‡ä»¶å¤¹å†…å®¹
4. **åŒºåˆ†åº¦é«˜**: é¿å…ä½¿ç”¨è¿‡äºé€šç”¨çš„è¯æ±‡ï¼ˆå¦‚"ç¬”è®°"ã€"æ–‡ç« "ç­‰ï¼‰

### æ ¼å¼è¦æ±‚
1. **åˆ†éš”ç¬¦**: ä½¿ç”¨ä¸‹åˆ’çº¿ `_` åˆ†éš”å•è¯ï¼Œç¦æ­¢ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
2. **é•¿åº¦é™åˆ¶**: ä¸­æ–‡ä¸è¶…è¿‡15ä¸ªå­—ï¼Œè‹±æ–‡ä¸è¶…è¿‡30ä¸ªå­—ç¬¦
3. **å­—ç¬¦é™åˆ¶**: ä»…ä½¿ç”¨ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œç¦æ­¢ `<>:"/\\|?*`
4. **å¤§å°å†™**: è‹±æ–‡å•è¯é¦–å­—æ¯å¤§å†™ï¼ˆå¦‚ Python_Best_Practicesï¼‰
5. **å¹³å°å**: ä¸éœ€è¦åŒ…å«å¹³å°åç§°ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ ï¼‰

### ä¼˜å…ˆçº§è§„åˆ™
- å¦‚æœæ˜¯æ•™ç¨‹/æŒ‡å—ï¼šçªå‡ºä¸»é¢˜+ç›®æ ‡ï¼ˆå¦‚ "PyTorchå›¾åƒåˆ†ç±»å®æˆ˜"ï¼‰
- å¦‚æœæ˜¯é—®ç­”/è®¨è®ºï¼šæç‚¼æ ¸å¿ƒé—®é¢˜ï¼ˆå¦‚ "å¦‚ä½•ä¼˜åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦"ï¼‰
- å¦‚æœæ˜¯å·¥å…·/èµ„æºï¼šå¼ºè°ƒå·¥å…·å+ç”¨é€”ï¼ˆå¦‚ "Crawl4AIç½‘é¡µæŠ“å–"ï¼‰
- å¦‚æœæ˜¯è§‚ç‚¹/åˆ†æï¼šæç‚¼æ ¸å¿ƒè®ºç‚¹ï¼ˆå¦‚ "Agentè®¾è®¡äº”å¤§æ¨¡å¼"ï¼‰

## ä¼˜ç§€ç¤ºä¾‹

- âœ… `PyTorchå›¾åƒåˆ†ç±»å®æˆ˜`ï¼ˆæ¸…æ™°ã€å…·ä½“ã€æœ‰æ“ä½œæ€§ï¼‰
- âœ… `Agentè®¾è®¡äº”å¤§æ¨¡å¼`ï¼ˆæç‚¼æ ¸å¿ƒæ•°å­—+ä¸»é¢˜ï¼‰
- âœ… `LLM_Token_ä¼˜åŒ–æŠ€å·§`ï¼ˆæŠ€æœ¯æœ¯è¯­+å®ç”¨æ€§ï¼‰
- âŒ `æœºå™¨å­¦ä¹ ç¬”è®°`ï¼ˆè¿‡äºç¬¼ç»Ÿï¼‰
- âŒ `Pythonæ•™ç¨‹ç¬¬ä¸€ç« `ï¼ˆç¼ºä¹è¯­ä¹‰ï¼‰
- âŒ `ä»Šå¤©çœ‹åˆ°çš„æ–‡ç« `ï¼ˆæ— ä»»ä½•ä¿¡æ¯ä»·å€¼ï¼‰

## è¾“å‡ºæ ¼å¼

**ä»…è¿”å›æ–‡ä»¶å¤¹åç§°æœ¬èº«**ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€å¼•å·ã€åºå·æˆ–å…¶ä»–æ–‡å­—ã€‚

æ–‡ä»¶å¤¹åç§°ï¼š"""

            response = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶å‘½ååŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´ã€æè¿°æ€§çš„æ–‡ä»¶å¤¹åç§°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=50,
                temperature=0.3,
            )
            
            folder_name = response.choices[0].message.content.strip()
            
            # æ¸…ç†æ–‡ä»¶å¤¹åç§°
            folder_name = re.sub(r'["\'\'\n\r\t]', '', folder_name)
            folder_name = re.sub(r'[/\\\\]', '_', folder_name)
            folder_name = re.sub(r'[<>:"|?*]', '', folder_name)
            
            # é™åˆ¶é•¿åº¦
            if len(folder_name) > 50:
                folder_name = folder_name[:50]
            
            # å¦‚æœç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜
            if not folder_name or len(folder_name) < 3:
                logger.warning("LLM ç”Ÿæˆçš„æ–‡ä»¶å¤¹åæ— æ•ˆï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜")
                return self._generate_folder_name(title, platform)
            
            logger.info(f"âœ… LLM ç”Ÿæˆçš„æ–‡ä»¶å¤¹å: {folder_name}")
            return folder_name
            
        except Exception as e:
            logger.warning(f"LLM æ–‡ä»¶å¤¹å‘½åå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åç§°")
            return self._generate_folder_name(title, platform)
    
    def _generate_report_with_llm(
        self,
        ocr_content: str,
        title: str,
        url: str,
        platform: str
    ) -> Optional[str]:
        """
        ä½¿ç”¨ LLM å°† OCR åŸå§‹æ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ–æŠ¥å‘Š
        
        Args:
            ocr_content: OCR åŸå§‹å†…å®¹ï¼ˆæ¥è‡ª ocr_raw.mdï¼‰
            title: ç½‘é¡µæ ‡é¢˜
            url: åŸå§‹ URL
            platform: å¹³å°åç§°
        
        Returns:
            ç”Ÿæˆçš„ Markdown æŠ¥å‘Šå†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        if not GROQ_AVAILABLE:
            logger.warning("Groq SDK æœªå®‰è£…ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return None
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            logger.warning("GROQ_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return None
        
        try:
            client = Groq(api_key=api_key)
            
            # é™åˆ¶è¾“å…¥é•¿åº¦
            content_limit = 3000
            if len(ocr_content) > content_limit:
                ocr_content = ocr_content[:content_limit] + "\n\n...(å†…å®¹å·²æˆªæ–­)"
            
            prompt = f"""è¯·å°†ä»¥ä¸‹ç½‘é¡µçš„ OCR æå–å†…å®¹æ•´ç†æˆä¸€ä»½**ç»“æ„åŒ– Markdown çŸ¥è¯†æ¡£æ¡ˆ**ã€‚

## è¾“å…¥ä¿¡æ¯

**æ ‡é¢˜**: {title}
**æ¥æº**: {url}
**å¹³å°**: {platform}

## âš ï¸ é‡è¦ï¼šè¯†åˆ«é”™è¯¯ä¿®æ­£

OCR æ–‡æœ¬å¯èƒ½å­˜åœ¨è¯†åˆ«é”™è¯¯ï¼Œä½ å¿…é¡»æ ¹æ®ä¸Šä¸‹æ–‡**ä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£**ï¼š
- **åŒéŸ³å­—/è¯é”™è¯¯**: "æœºå™¨å­¦ä¹ "â†’"é¸¡å™¨å­¦ä¹ "ã€"Python"â†’"æ´¾æ£®"
- **ä¸“ä¸šæœ¯è¯­**: æŠ€æœ¯åè¯ã€å­¦æœ¯æ¦‚å¿µè¦ä½¿ç”¨è§„èŒƒè¡¨è¾¾
- **äººååœ°å**: ç¡®ä¿æ‹¼å†™å‡†ç¡®
- **æ ‡ç‚¹ç¬¦å·**: è¡¥å……ç¼ºå¤±çš„é€—å·ã€å¥å·ã€é—®å·
- **æ®µè½åˆ†éš”**: ä¿®æ­£ä¸åˆç†çš„æ¢è¡Œå’Œåˆ†æ®µ

## ä½ éœ€è¦å®Œæˆï¼š

1. **å†…å®¹æ¸…æ´—ä¸ä¿®æ­£**
   - ä¿®æ­£ OCR è¯†åˆ«é”™è¯¯ï¼ˆåŒéŸ³å­—ã€é”™åˆ«å­—ã€æ–­å¥é—®é¢˜ï¼‰
   - åˆ é™¤æ— å…³çš„å¹¿å‘Šã€æ¨å¹¿ã€å¯¼èˆªã€è¯„è®º
   - è¡¥å……ç¼ºå¤±çš„æ ‡ç‚¹ç¬¦å·å’Œæ®µè½åˆ†éš”
   - ç¡®ä¿ä½¿ç”¨ä¸“ä¸šã€å‡†ç¡®çš„æœ¯è¯­

2. **ç»“æ„åŒ–ç»„ç»‡**
   - ç”Ÿæˆæ¸…æ™°çš„å±‚çº§æ ‡é¢˜ï¼ˆ##ã€###ï¼‰
   - ä½¿ç”¨ Markdown åˆ—è¡¨æ ¼å¼ï¼ˆæœ‰åº/æ— åºï¼‰
   - ä»£ç å—ä½¿ç”¨ ``` æ ‡è®°ï¼Œæ ‡æ³¨è¯­è¨€
   - é‡è¦å†…å®¹ä½¿ç”¨ **åŠ ç²—** æˆ– `è¡Œå†…ä»£ç `
   - å¼•ç”¨ä½¿ç”¨ > æ ¼å¼

3. **ä¿¡æ¯æç‚¼**
   - ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼ˆæ•°æ®ã€ç»“è®ºã€æ–¹æ³•ã€æ­¥éª¤ï¼‰
   - åˆ é™¤å†—ä½™çš„å£è¯­åŒ–è¡¨è¾¾
   - å¦‚æœæœ‰ä½œè€…è§‚ç‚¹ï¼Œç”¨ã€Œã€æˆ– > å¼•ç”¨æ ¼å¼æ ‡æ³¨
   - æå–æ ¸å¿ƒè¦ç‚¹ï¼Œç”Ÿæˆå°ç»“

4. **å®Œæ•´æ€§è¦æ±‚**
   - å›¾ç‰‡ä¿ç•™åŸæœ‰é“¾æ¥ï¼ˆ`![](images/xxx.jpg)`ï¼‰
   - é“¾æ¥ä¿ç•™åŸæœ‰æ ¼å¼ï¼ˆ`[æ–‡æœ¬](URL)`ï¼‰
   - è¡¨æ ¼ä¿ç•™åŸæœ‰ç»“æ„
   - ä¸è¦çœç•¥é‡è¦ç»†èŠ‚

## æ¨èè¾“å‡ºç»“æ„

# [æ–‡ç« æ ‡é¢˜]

## æ‘˜è¦
ï¼ˆ50å­—ä»¥å†…çš„æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬ï¼‰

## ä¸»è¦å†…å®¹

### [ç« èŠ‚1æ ‡é¢˜]
ï¼ˆè¯¦ç»†å†…å®¹...ï¼‰

### [ç« èŠ‚2æ ‡é¢˜]
ï¼ˆè¯¦ç»†å†…å®¹...ï¼‰

## å…³é”®ä¿¡æ¯
- é‡è¦æ•°æ®ï¼š...
- æ ¸å¿ƒè§‚ç‚¹ï¼š...
- æ“ä½œæ­¥éª¤ï¼š...

## æ ‡ç­¾
æ ‡ç­¾: æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3

---

## OCR åŸå§‹å†…å®¹

```
{ocr_content}
```

---

**è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦ä»»ä½•åŒ…è£¹æˆ–é¢å¤–è¯´æ˜ã€‚**"""

            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ•´ç†ä¸“å®¶ï¼Œå…·å¤‡æ™ºèƒ½çº é”™èƒ½åŠ›ã€‚

ä½ çš„èŒè´£æ˜¯ï¼š
- å°†æ··ä¹±çš„ OCR æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„æ¸…æ™°çš„ Markdown çŸ¥è¯†æ¡£æ¡ˆ
- **æ™ºèƒ½çº é”™**ï¼šä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£OCRè¯†åˆ«é”™è¯¯ï¼ˆåŒéŸ³å­—/è¯ã€ä¸“ä¸šæœ¯è¯­ã€æ ‡ç‚¹ç¬¦å·ï¼‰
- æ¨æ–­å¹¶è¡¥å…¨ä¸å®Œæ•´çš„å¥å­å’Œæ®µè½
- æå–æ ¸å¿ƒä¿¡æ¯å¹¶ç»“æ„åŒ–ç»„ç»‡
- ç¡®ä¿è¾“å‡ºä½¿ç”¨å‡†ç¡®ã€ä¸“ä¸šçš„æœ¯è¯­è¡¨è¾¾
- ç”Ÿæˆæ¸…æ™°ã€å¯é•¿æœŸä¿å­˜ã€é€‚åˆæ£€ç´¢çš„çŸ¥è¯†æ–‡æ¡£"""
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=8192,
                temperature=0.7,
            )
            
            report_content = response.choices[0].message.content.strip()
            
            if not report_content or len(report_content) < 50:
                logger.warning("LLM ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡ä¿å­˜")
                return None
            
            logger.info(f"âœ… LLM ç”ŸæˆæŠ¥å‘ŠæˆåŠŸï¼ˆ{len(report_content)} å­—ç¬¦ï¼‰")
            return report_content
            
        except Exception as e:
            logger.warning(f"LLM æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    async def archive_batch(
        self,
        urls: list,
        max_concurrent: int = 3
    ) -> list:
        """
        æ‰¹é‡å½’æ¡£å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        
        Returns:
            å½’æ¡£ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def archive_with_semaphore(url):
            async with semaphore:
                return await self.archive(url)
        
        tasks = [archive_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results


async def archive_url(url: str, output_dir: str = "archived") -> Dict[str, Any]:
    """
    ç®€å•çš„å•URLå½’æ¡£å‡½æ•°
    
    Args:
        url: ç›®æ ‡URL
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        å½’æ¡£ç»“æœ
    """
    archiver = UniversalArchiver(output_dir=output_dir)
    return await archiver.archive(url)
